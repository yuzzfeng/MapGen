{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Map Generalization for Polygons using Pix2Pix strucutures - 15k\n",
    "## \n",
    "## Author: Yu Feng, yuzz.feng@gmail.com\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def lambda_output(input_shape):\n",
    "    return input_shape[:2]\n",
    "\n",
    "def minb_disc(x):\n",
    "    diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
    "    abs_diffs = K.sum(K.abs(diffs), 2)\n",
    "    x = K.sum(K.exp(-abs_diffs), 2)\n",
    "\n",
    "    return x\n",
    "\n",
    "def generate_patch_gan_loss(last_disc_conv_layer, patch_dim, input_layer, nb_patches):\n",
    "\n",
    "    # generate a list of inputs for the different patches to the network\n",
    "    list_input = [Input(shape=patch_dim, name=\"patch_gan_input_%s\" % i) for i in range(nb_patches)]\n",
    "\n",
    "    # get an activation\n",
    "    x_flat = Flatten()(last_disc_conv_layer)\n",
    "    x = Dense(2, activation='softmax', name=\"disc_dense\")(x_flat)\n",
    "\n",
    "    patch_gan = Model(inputs=[input_layer], outputs=[x, x_flat], name=\"patch_gan\")\n",
    "\n",
    "    # generate individual losses for each patch\n",
    "    x = [patch_gan(patch)[0] for patch in list_input]\n",
    "    x_mbd = [patch_gan(patch)[1] for patch in list_input]\n",
    "\n",
    "    # merge layers if have multiple patches (aka perceptual loss)\n",
    "    if len(x) > 1:\n",
    "        #x = merge(x, mode=\"concat\", name=\"merged_features\")\n",
    "        x = Concatenate(name=\"merged_features\")(x)\n",
    "    else:\n",
    "        x = x[0]\n",
    "\n",
    "    # merge mbd if needed\n",
    "    # mbd = mini batch discrimination\n",
    "    # https://arxiv.org/pdf/1606.03498.pdf\n",
    "    if len(x_mbd) > 1:\n",
    "        #x_mbd = merge(x_mbd, mode=\"concat\", name=\"merged_feature_mbd\")\n",
    "        x_mbd = Concatenate(name=\"merged_feature_mbd\")(x_mbd)\n",
    "    else:\n",
    "        x_mbd = x_mbd[0]\n",
    "\n",
    "    num_kernels = 100\n",
    "    dim_per_kernel = 5\n",
    "\n",
    "    M = Dense(num_kernels * dim_per_kernel, use_bias=False, activation=None)\n",
    "    MBD = Lambda(minb_disc, output_shape=lambda_output)\n",
    "\n",
    "    x_mbd = M(x_mbd)\n",
    "    x_mbd = Reshape((num_kernels, dim_per_kernel))(x_mbd)\n",
    "    x_mbd = MBD(x_mbd)\n",
    "    \n",
    "    #x = merge([x, x_mbd], mode='concat')\n",
    "    x = Concatenate()([x, x_mbd])\n",
    "\n",
    "    x_out = Dense(2, activation=\"softmax\", name=\"disc_output\")(x)\n",
    "\n",
    "    discriminator = Model(inputs=list_input, outputs=[x_out], name='discriminator_nn')\n",
    "    return discriminator\n",
    "\n",
    "def res_block(x, nb_filters, strides, increase = False):\n",
    "    # This implementation used the double 3x3 structure and followed the Identity Mappings\n",
    "    res_path = BatchNormalization()(x)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    \n",
    "    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same', strides=strides[0])(res_path)\n",
    "    res_path = BatchNormalization()(res_path)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same', strides=strides[1])(res_path)\n",
    "    \n",
    "    if increase:\n",
    "        shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1), strides=strides[0])(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    res_path = Add()([shortcut, res_path])\n",
    "    return res_path\n",
    "\n",
    "def decoder(x, from_encoder):\n",
    "    main_path = UpSampling2D(size=(2, 2))(x)\n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[2]])\n",
    "    main_path = res_block(main_path, [128, 128], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    main_path = UpSampling2D(size=(2, 2))(main_path) \n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[1]])\n",
    "    main_path = res_block(main_path, [64, 64], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    main_path = UpSampling2D(size=(2, 2))(main_path)\n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[0]])\n",
    "    main_path = res_block(main_path, [32, 32], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    return main_path\n",
    "\n",
    "def encoder(x):\n",
    "    to_decoder = []\n",
    "\n",
    "    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n",
    "    main_path = BatchNormalization()(main_path)\n",
    "    main_path = Activation(activation='relu')(main_path)\n",
    "    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n",
    "\n",
    "    shortcut = Conv2D(filters=32, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    main_path = Add()([shortcut, main_path])\n",
    "    # first branching to decoder\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    main_path = res_block(main_path, [64, 64], [(2, 2), (1, 1)], increase = True)\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    main_path = res_block(main_path, [128, 128], [(2, 2), (1, 1)], increase = True)\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    return to_decoder\n",
    "\n",
    "def build_res_unet(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    to_decoder = encoder(inputs)\n",
    "\n",
    "    path = res_block(to_decoder[2], [256, 256], [(2, 2), (1, 1)]) # 3x\n",
    "    \n",
    "    path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Yu.add - in 2018-12-02 16-09-04_15 only once\n",
    "\n",
    "    path = decoder(path, from_encoder=to_decoder)\n",
    "    \n",
    "    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path) \n",
    "\n",
    "    return Model(input=inputs, output=path)\n",
    "\n",
    "class EL_GAN(): # Based on pix2pix\n",
    "    def __init__(self):\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN) better version\n",
    "        self.patch_size = 32\n",
    "        self.nb_patches = int((self.img_rows / self.patch_size) * (self.img_cols / self.patch_size))\n",
    "        self.patch_gan_dim = (self.patch_size, self.patch_size, self.channels)\n",
    "        \n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5) # Original\n",
    "        #optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # An old version of Pix2pix\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator() # Old generator from \n",
    "        self.generator = self.build_res_unet_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        #img_A = Input(shape=self.img_shape) # Target\n",
    "        img_B = Input(shape=self.img_shape) # Input\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        #valid = self.discriminator([fake_A, img_B])\n",
    "        valid = self.discriminator([fake_A])\n",
    "\n",
    "        #self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined = Model(inputs= img_B, outputs=[valid, fake_A])\n",
    "        \n",
    "        # Original Pix2Pix - low weight for discriminator\n",
    "        self.combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100], # 20190117: original [1, 100]\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    def build_res_unet_generator(self):\n",
    "        \"\"\"Residual U-Net Generator\"\"\"\n",
    "        \n",
    "        inputs = Input(shape=self.img_shape)\n",
    "        to_decoder = encoder(inputs)\n",
    "        path = res_block(to_decoder[2], [256, 256], [(2, 2), (1, 1)], increase = True) # 3x\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Number of block of bottleneck = 1\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.14, achieved best result ever\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.15\n",
    "        path = decoder(path, from_encoder=to_decoder)\n",
    "        path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path) \n",
    "\n",
    "        return Model(input=inputs, output=path)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "    \n",
    "    def build_PatchGanDiscriminator(self):\n",
    "        \"\"\"\n",
    "        Creates the generator according to the specs in the paper below.\n",
    "        [https://arxiv.org/pdf/1611.07004v1.pdf][5. Appendix]\n",
    "\n",
    "        PatchGAN only penalizes structure at the scale of patches. This\n",
    "        discriminator tries to classify if each N x N patch in an\n",
    "        image is real or fake. We run this discriminator convolutationally\n",
    "        across the image, averaging all responses to provide\n",
    "        the ultimate output of D.\n",
    "\n",
    "        The discriminator has two parts. First part is the actual discriminator\n",
    "        seconds part we make it a PatchGAN by running each image patch through the model\n",
    "        and then we average the responses\n",
    "\n",
    "        Discriminator does the following:\n",
    "        1. Runs many pieces of the image through the network\n",
    "        2. Calculates the cost for each patch\n",
    "        3. Returns the avg of the costs as the output of the network\n",
    "\n",
    "        :param patch_dim: (channels, width, height) T\n",
    "        :param nb_patches:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # -------------------------------\n",
    "        # DISCRIMINATOR\n",
    "        # C64-C128-C256-C512-C512-C512 (for 256x256)\n",
    "        # otherwise, it scales from 64\n",
    "        # 1 layer block = Conv - BN - LeakyRelu\n",
    "        # -------------------------------\n",
    "        \n",
    "        output_img_dim = self.img_shape\n",
    "        patch_dim = self.patch_gan_dim\n",
    "        input_layer = Input(shape=patch_dim)\n",
    "        \n",
    "        # We have to build the discriminator dinamically because\n",
    "        # the size of the disc patches is dynamic\n",
    "        num_filters_start = self.gf\n",
    "        nb_conv = int(np.floor(np.log(output_img_dim[1]) / np.log(2)))\n",
    "        filters_list = [num_filters_start * min(8, (2 ** i)) for i in range(nb_conv)]\n",
    "        \n",
    "        # CONV 1\n",
    "        # Do first conv bc it is different from the rest\n",
    "        # paper skips batch norm for first layer\n",
    "        disc_out = Conv2D(filters=64, kernel_size=(4, 4), padding='same', strides=(2, 2), name='disc_conv_1')(input_layer)\n",
    "        disc_out = LeakyReLU(alpha=0.2)(disc_out)\n",
    "        \n",
    "        # CONV 2 - CONV N\n",
    "        # do the rest of the convs based on the sizes from the filters\n",
    "        for i, filter_size in enumerate(filters_list[1:]):\n",
    "            name = 'disc_conv_{}'.format(i+2)\n",
    "\n",
    "            disc_out = Conv2D(filters=filter_size, kernel_size=(4, 4), padding='same', strides=(2, 2), name=name)(disc_out)\n",
    "            disc_out = BatchNormalization(name=name + '_bn')(disc_out)\n",
    "            disc_out = LeakyReLU(alpha=0.2)(disc_out)\n",
    "        \n",
    "        # ------------------------\n",
    "        # BUILD PATCH GAN\n",
    "        # this is where we evaluate the loss over each sublayer of the input\n",
    "        # ------------------------\n",
    "        patch_gan_discriminator = generate_patch_gan_loss(last_disc_conv_layer=disc_out,\n",
    "                                                          patch_dim=patch_dim,\n",
    "                                                          input_layer=input_layer,\n",
    "                                                          nb_patches=nb_patches)\n",
    "        return patch_gan_discriminator\n",
    "    \n",
    "    def build_2head_discriminator(self):\n",
    "        \n",
    "        def d_layer(layer_input, filters, f_size=3, bn=True): # Chnaged here for the order of bn and activation\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            conv = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')\n",
    "            d = conv(layer_input)\n",
    "            e = conv(layer_input2)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Activation(activation='relu')(d)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "        \n",
    "        def d_layers(img_A):\n",
    "            d1 = d_layer(img_A, self.df, bn=False)\n",
    "            d2 = d_layer(d1, self.df*2)\n",
    "            d3 = d_layer(d2, self.df*4)\n",
    "            d4 = d_layer(d3, self.df*8)\n",
    "            d5 = Flatten()(d4)\n",
    "            d6 = Dense(128, activation='softmax')(d5)\n",
    "            \n",
    "            return Model(img_A, d6)\n",
    "        \n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        \n",
    "        encoded_a = d_layers(img_A)\n",
    "        encoded_b = d_layers(img_B)\n",
    "        \n",
    "        # We can then concatenate the two vectors:\n",
    "        #merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n",
    "        \n",
    "        return Model([img_A, img_B], validity)\n",
    "        \n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=3, bn=True): # Chnaged here for the order of bn and activation\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Activation(activation='relu')(d)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        #img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        ## Concatenate image and conditioning image by channels to produce input\n",
    "        #combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        #d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        \n",
    "        d1 = d_layer(img_A, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A], validity)\n",
    "    \n",
    "    def train_generator_only(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        data_gen_args = dict(rotation_range=180.)\n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        \n",
    "        seed = 1\n",
    "        BATCH_SIZE = 16\n",
    "        result_generator = zip(image_datagen.flow(x_train_sim, batch_size=BATCH_SIZE, seed=seed), \n",
    "                               mask_datagen.flow(y_train_sim, batch_size=BATCH_SIZE, seed=seed))\n",
    "        \n",
    "        History1 = History()\n",
    "        hist1 = self.generator.fit_generator( result_generator,\n",
    "                                              epochs = 100,\n",
    "                                              steps_per_epoch=2000,\n",
    "                                              verbose=1,\n",
    "                                              shuffle=True,\n",
    "                                              callbacks=[History1, \n",
    "                                                         EarlyStopping(patience=5), \n",
    "                                                         ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                                         ModelCheckpoint(outPath + \"weights.hdf5\", \n",
    "                                                                         save_best_only = True, \n",
    "                                                                         save_weights_only = False)],\n",
    "                                              validation_data=(x_test_sim, y_test_sim))\n",
    "        save_hist(History1, outPath)\n",
    "        \n",
    "    \n",
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                #d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                #d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs_A], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fake_A], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                self.discriminator.trainable = False\n",
    "                #g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "                g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "                self.discriminator.trainable = True\n",
    "                \n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    \n",
    "                    valid_test = np.ones((len(x_test_sim),) + self.disc_patch)\n",
    "                    #t_loss = self.combined.evaluate([y_test_sim, x_test_sim], [valid_test, y_test_sim], verbose=0)\n",
    "                    t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "                    \n",
    "                    print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] [Test loss&acc: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                100*t_loss[2], 100*t_loss[3], 100*t_loss[4],\n",
    "                                                                                elapsed_time))                 \n",
    "            if epoch > 10:\n",
    "                self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')\n",
    "                        \n",
    "                ## If at save interval => save generated image samples\n",
    "                #if batch_i % sample_interval == 0:\n",
    "                #    self.sample_images(outPath, epoch, batch_i)\n",
    "\n",
    "\n",
    "    def sample_images(self, outPath, epoch, batch_i, examples = [0, 77, 34]):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "        p_size_1 = 128\n",
    "        \n",
    "        imgs_A = y_test_sim[examples]\n",
    "        imgs_B = x_test_sim[examples]\n",
    "        \n",
    "        fake_A = gan.generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Generated', 'Target']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                \n",
    "                #axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(outPath + \"%d_%d.png\" % (epoch, batch_i),\n",
    "                   format='png', transparent=True, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the trains (32289, 128, 128, 1)\n",
      "Input Shape of the tests (3587, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "scale = 15\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the trains', x_train_sim.shape)\n",
    "print('Input Shape of the tests', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create\n",
    "\n",
    "from time import gmtime, strftime\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "\n",
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:218: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/26-0/1009] [D loss&acc: 1.511, 28.662%] [G loss&accA&accB: 27.499, 42.627%, 88.327%] [Test loss&acc: 32.002, 0.000%, 68.907%] time: 0:00:13.819794\n",
      "[Epoch 0/26-500/1009] [D loss&acc: 0.317, 6.934%] [G loss&accA&accB: 1.070, 7.129%, 98.935%] [Test loss&acc: 1.033, 0.000%, 98.979%] time: 0:03:24.027118\n",
      "[Epoch 0/26-1000/1009] [D loss&acc: 0.298, 12.012%] [G loss&accA&accB: 0.862, 5.420%, 99.139%] [Test loss&acc: 1.012, 0.000%, 98.991%] time: 0:06:33.291891\n",
      "[Epoch 1/26-0/1009] [D loss&acc: 0.298, 15.381%] [G loss&accA&accB: 1.279, 5.225%, 98.723%] [Test loss&acc: 1.011, 0.000%, 98.993%] time: 0:06:46.499162\n",
      "[Epoch 1/26-500/1009] [D loss&acc: 0.288, 14.697%] [G loss&accA&accB: 0.615, 6.250%, 99.391%] [Test loss&acc: 1.012, 0.000%, 98.993%] time: 0:09:54.743263\n",
      "[Epoch 1/26-1000/1009] [D loss&acc: 0.286, 11.401%] [G loss&accA&accB: 0.800, 4.590%, 99.205%] [Test loss&acc: 0.993, 0.000%, 99.008%] time: 0:13:02.627028\n",
      "[Epoch 2/26-0/1009] [D loss&acc: 0.281, 41.748%] [G loss&accA&accB: 0.574, 0.391%, 99.428%] [Test loss&acc: 0.987, 0.000%, 99.015%] time: 0:13:15.739932\n",
      "[Epoch 2/26-500/1009] [D loss&acc: 0.316, 3.149%] [G loss&accA&accB: 0.637, 1.367%, 99.365%] [Test loss&acc: 0.975, 0.000%, 99.026%] time: 0:16:24.184893\n",
      "[Epoch 2/26-1000/1009] [D loss&acc: 0.309, 9.106%] [G loss&accA&accB: 1.974, 8.105%, 98.027%] [Test loss&acc: 1.016, 1.251%, 98.990%] time: 0:19:32.566085\n",
      "[Epoch 3/26-0/1009] [D loss&acc: 0.295, 14.160%] [G loss&accA&accB: 0.683, 1.758%, 99.319%] [Test loss&acc: 0.958, 0.086%, 99.043%] time: 0:19:45.666881\n",
      "[Epoch 3/26-500/1009] [D loss&acc: 0.310, 8.252%] [G loss&accA&accB: 0.963, 5.615%, 99.042%] [Test loss&acc: 1.004, 3.327%, 99.009%] time: 0:22:53.716250\n",
      "[Epoch 3/26-1000/1009] [D loss&acc: 0.292, 24.609%] [G loss&accA&accB: 0.874, 25.488%, 99.128%] [Test loss&acc: 0.879, 19.754%, 99.126%] time: 0:26:01.508031\n",
      "[Epoch 4/26-0/1009] [D loss&acc: 0.284, 28.784%] [G loss&accA&accB: 0.678, 2.441%, 99.323%] [Test loss&acc: 0.846, 19.944%, 99.157%] time: 0:26:14.602821\n",
      "[Epoch 4/26-500/1009] [D loss&acc: 0.292, 8.496%] [G loss&accA&accB: 0.670, 13.574%, 99.333%] [Test loss&acc: 0.815, 1.186%, 99.186%] time: 0:29:22.005745\n",
      "[Epoch 4/26-1000/1009] [D loss&acc: 0.302, 5.981%] [G loss&accA&accB: 1.063, 6.836%, 98.938%] [Test loss&acc: 0.812, 0.790%, 99.191%] time: 0:32:29.412858\n",
      "[Epoch 5/26-0/1009] [D loss&acc: 0.297, 4.688%] [G loss&accA&accB: 0.623, 4.395%, 99.376%] [Test loss&acc: 0.776, 0.868%, 99.226%] time: 0:32:42.490927\n",
      "[Epoch 5/26-500/1009] [D loss&acc: 0.278, 25.098%] [G loss&accA&accB: 0.584, 1.953%, 99.417%] [Test loss&acc: 0.754, 8.949%, 99.247%] time: 0:35:50.010706\n",
      "[Epoch 5/26-1000/1009] [D loss&acc: 0.293, 7.788%] [G loss&accA&accB: 0.749, 6.689%, 99.249%] [Test loss&acc: 0.745, 12.546%, 99.256%] time: 0:38:57.324185\n",
      "[Epoch 6/26-0/1009] [D loss&acc: 0.292, 7.715%] [G loss&accA&accB: 0.564, 5.957%, 99.437%] [Test loss&acc: 0.749, 9.647%, 99.252%] time: 0:39:10.419559\n",
      "[Epoch 6/26-500/1009] [D loss&acc: 0.287, 9.058%] [G loss&accA&accB: 0.420, 12.842%, 99.581%] [Test loss&acc: 0.732, 0.263%, 99.268%] time: 0:42:18.752140\n",
      "[Epoch 6/26-1000/1009] [D loss&acc: 0.272, 41.748%] [G loss&accA&accB: 0.375, 1.660%, 99.626%] [Test loss&acc: 0.724, 1.950%, 99.277%] time: 0:45:27.021983\n",
      "[Epoch 7/26-0/1009] [D loss&acc: 0.278, 10.742%] [G loss&accA&accB: 0.563, 6.152%, 99.437%] [Test loss&acc: 0.744, 2.942%, 99.256%] time: 0:45:40.247834\n",
      "[Epoch 7/26-500/1009] [D loss&acc: 0.285, 8.423%] [G loss&accA&accB: 0.517, 5.127%, 99.485%] [Test loss&acc: 0.733, 0.037%, 99.268%] time: 0:48:49.023265\n",
      "[Epoch 7/26-1000/1009] [D loss&acc: 0.288, 10.059%] [G loss&accA&accB: 0.697, 5.664%, 99.304%] [Test loss&acc: 0.713, 9.329%, 99.287%] time: 0:51:57.647059\n",
      "[Epoch 8/26-0/1009] [D loss&acc: 0.290, 9.399%] [G loss&accA&accB: 0.765, 13.086%, 99.236%] [Test loss&acc: 0.727, 11.118%, 99.274%] time: 0:52:10.875529\n",
      "[Epoch 8/26-500/1009] [D loss&acc: 0.286, 20.776%] [G loss&accA&accB: 0.671, 32.666%, 99.327%] [Test loss&acc: 0.709, 1.350%, 99.292%] time: 0:55:19.306946\n",
      "[Epoch 8/26-1000/1009] [D loss&acc: 0.290, 11.255%] [G loss&accA&accB: 0.349, 6.543%, 99.651%] [Test loss&acc: 0.705, 0.000%, 99.296%] time: 0:58:27.683196\n",
      "[Epoch 9/26-0/1009] [D loss&acc: 0.288, 9.448%] [G loss&accA&accB: 0.768, 4.199%, 99.233%] [Test loss&acc: 0.818, 0.000%, 99.184%] time: 0:58:40.892083\n",
      "[Epoch 9/26-500/1009] [D loss&acc: 0.280, 9.082%] [G loss&accA&accB: 0.495, 6.396%, 99.507%] [Test loss&acc: 0.700, 11.969%, 99.300%] time: 1:01:49.268774\n",
      "[Epoch 9/26-1000/1009] [D loss&acc: 0.270, 20.679%] [G loss&accA&accB: 0.532, 11.328%, 99.469%] [Test loss&acc: 0.704, 3.607%, 99.297%] time: 1:04:57.238567\n",
      "[Epoch 10/26-0/1009] [D loss&acc: 0.272, 21.069%] [G loss&accA&accB: 0.500, 12.793%, 99.501%] [Test loss&acc: 0.697, 4.157%, 99.303%] time: 1:05:10.297386\n",
      "[Epoch 10/26-500/1009] [D loss&acc: 0.266, 25.220%] [G loss&accA&accB: 0.606, 10.938%, 99.393%] [Test loss&acc: 0.703, 1.094%, 99.298%] time: 1:08:17.575103\n",
      "[Epoch 10/26-1000/1009] [D loss&acc: 0.274, 23.120%] [G loss&accA&accB: 0.580, 42.139%, 99.421%] [Test loss&acc: 0.719, 0.336%, 99.281%] time: 1:11:24.634321\n",
      "[Epoch 11/26-0/1009] [D loss&acc: 0.260, 38.989%] [G loss&accA&accB: 0.363, 15.771%, 99.636%] [Test loss&acc: 0.696, 0.477%, 99.305%] time: 1:11:37.718602\n",
      "[Epoch 11/26-500/1009] [D loss&acc: 0.265, 32.544%] [G loss&accA&accB: 0.609, 33.936%, 99.391%] [Test loss&acc: 0.688, 0.000%, 99.313%] time: 1:14:43.560606\n",
      "[Epoch 11/26-1000/1009] [D loss&acc: 0.262, 29.858%] [G loss&accA&accB: 0.631, 17.383%, 99.368%] [Test loss&acc: 0.678, 0.000%, 99.322%] time: 1:17:49.203585\n",
      "[Epoch 12/26-0/1009] [D loss&acc: 0.264, 33.911%] [G loss&accA&accB: 0.673, 27.539%, 99.326%] [Test loss&acc: 0.690, 0.000%, 99.311%] time: 1:18:06.960065\n",
      "[Epoch 12/26-500/1009] [D loss&acc: 0.265, 34.937%] [G loss&accA&accB: 0.514, 13.428%, 99.486%] [Test loss&acc: 0.694, 0.000%, 99.306%] time: 1:21:12.499788\n",
      "[Epoch 12/26-1000/1009] [D loss&acc: 0.259, 38.379%] [G loss&accA&accB: 0.711, 29.443%, 99.291%] [Test loss&acc: 0.724, 0.000%, 99.276%] time: 1:24:17.676436\n",
      "[Epoch 13/26-0/1009] [D loss&acc: 0.261, 25.586%] [G loss&accA&accB: 0.672, 31.982%, 99.329%] [Test loss&acc: 0.720, 0.000%, 99.280%] time: 1:24:31.248380\n",
      "[Epoch 13/26-500/1009] [D loss&acc: 0.255, 41.016%] [G loss&accA&accB: 0.529, 33.203%, 99.472%] [Test loss&acc: 0.693, 0.000%, 99.307%] time: 1:27:36.592973\n",
      "[Epoch 13/26-1000/1009] [D loss&acc: 0.262, 26.538%] [G loss&accA&accB: 0.518, 53.223%, 99.482%] [Test loss&acc: 0.681, 0.000%, 99.319%] time: 1:30:42.286098\n",
      "[Epoch 14/26-0/1009] [D loss&acc: 0.259, 27.832%] [G loss&accA&accB: 0.521, 24.316%, 99.479%] [Test loss&acc: 0.678, 0.000%, 99.322%] time: 1:30:55.845544\n",
      "[Epoch 14/26-500/1009] [D loss&acc: 0.258, 38.086%] [G loss&accA&accB: 0.494, 26.270%, 99.507%] [Test loss&acc: 0.688, 0.000%, 99.312%] time: 1:34:01.510200\n",
      "[Epoch 14/26-1000/1009] [D loss&acc: 0.255, 46.631%] [G loss&accA&accB: 0.374, 20.117%, 99.626%] [Test loss&acc: 0.677, 0.000%, 99.323%] time: 1:37:06.876373\n",
      "[Epoch 15/26-0/1009] [D loss&acc: 0.254, 52.539%] [G loss&accA&accB: 0.402, 4.492%, 99.598%] [Test loss&acc: 0.686, 0.000%, 99.315%] time: 1:37:20.407864\n",
      "[Epoch 15/26-500/1009] [D loss&acc: 0.254, 45.068%] [G loss&accA&accB: 0.556, 12.744%, 99.446%] [Test loss&acc: 0.675, 0.000%, 99.325%] time: 1:40:25.753779\n",
      "[Epoch 15/26-1000/1009] [D loss&acc: 0.255, 44.312%] [G loss&accA&accB: 0.314, 50.195%, 99.686%] [Test loss&acc: 0.680, 0.000%, 99.320%] time: 1:43:31.058826\n",
      "[Epoch 16/26-0/1009] [D loss&acc: 0.250, 57.910%] [G loss&accA&accB: 0.536, 11.768%, 99.465%] [Test loss&acc: 0.679, 0.000%, 99.321%] time: 1:43:44.592612\n",
      "[Epoch 16/26-500/1009] [D loss&acc: 0.255, 40.942%] [G loss&accA&accB: 0.505, 60.254%, 99.495%] [Test loss&acc: 0.679, 0.000%, 99.322%] time: 1:46:49.715402\n",
      "[Epoch 16/26-1000/1009] [D loss&acc: 0.260, 33.057%] [G loss&accA&accB: 0.386, 44.531%, 99.614%] [Test loss&acc: 0.683, 0.000%, 99.318%] time: 1:49:54.785009\n",
      "[Epoch 17/26-0/1009] [D loss&acc: 0.252, 52.100%] [G loss&accA&accB: 0.707, 11.768%, 99.293%] [Test loss&acc: 0.685, 0.000%, 99.315%] time: 1:50:08.355991\n",
      "[Epoch 17/26-500/1009] [D loss&acc: 0.254, 44.897%] [G loss&accA&accB: 0.609, 76.025%, 99.391%] [Test loss&acc: 0.676, 0.000%, 99.325%] time: 1:53:13.470510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/26-1000/1009] [D loss&acc: 0.244, 73.682%] [G loss&accA&accB: 0.617, 35.742%, 99.384%] [Test loss&acc: 0.676, 0.001%, 99.324%] time: 1:56:18.553833\n",
      "[Epoch 18/26-0/1009] [D loss&acc: 0.267, 14.282%] [G loss&accA&accB: 0.666, 69.385%, 99.334%] [Test loss&acc: 0.677, 0.007%, 99.323%] time: 1:56:32.119264\n",
      "[Epoch 18/26-500/1009] [D loss&acc: 0.254, 37.231%] [G loss&accA&accB: 0.400, 31.006%, 99.601%] [Test loss&acc: 0.679, 0.018%, 99.321%] time: 1:59:37.255289\n",
      "[Epoch 18/26-1000/1009] [D loss&acc: 0.254, 41.211%] [G loss&accA&accB: 0.396, 37.988%, 99.605%] [Test loss&acc: 0.697, 0.027%, 99.303%] time: 2:02:42.543911\n",
      "[Epoch 19/26-0/1009] [D loss&acc: 0.252, 45.801%] [G loss&accA&accB: 0.436, 40.723%, 99.564%] [Test loss&acc: 0.690, 0.011%, 99.310%] time: 2:02:56.087806\n",
      "[Epoch 19/26-500/1009] [D loss&acc: 0.255, 41.650%] [G loss&accA&accB: 0.466, 16.260%, 99.534%] [Test loss&acc: 0.677, 0.000%, 99.323%] time: 2:06:01.056817\n",
      "[Epoch 19/26-1000/1009] [D loss&acc: 0.259, 35.840%] [G loss&accA&accB: 0.464, 61.377%, 99.535%] [Test loss&acc: 0.681, 0.060%, 99.319%] time: 2:09:05.840758\n",
      "[Epoch 20/26-0/1009] [D loss&acc: 0.253, 44.507%] [G loss&accA&accB: 0.422, 53.711%, 99.578%] [Test loss&acc: 0.682, 0.017%, 99.318%] time: 2:09:19.378957\n",
      "[Epoch 20/26-500/1009] [D loss&acc: 0.259, 38.232%] [G loss&accA&accB: 0.668, 80.908%, 99.332%] [Test loss&acc: 0.677, 0.044%, 99.324%] time: 2:12:24.388643\n",
      "[Epoch 20/26-1000/1009] [D loss&acc: 0.255, 43.970%] [G loss&accA&accB: 0.447, 22.656%, 99.554%] [Test loss&acc: 0.676, 0.094%, 99.324%] time: 2:15:29.340065\n",
      "[Epoch 21/26-0/1009] [D loss&acc: 0.255, 44.043%] [G loss&accA&accB: 0.648, 24.512%, 99.351%] [Test loss&acc: 0.687, 0.043%, 99.313%] time: 2:15:42.870578\n",
      "[Epoch 21/26-500/1009] [D loss&acc: 0.254, 37.671%] [G loss&accA&accB: 0.512, 16.064%, 99.488%] [Test loss&acc: 0.677, 0.041%, 99.323%] time: 2:18:47.634525\n",
      "[Epoch 21/26-1000/1009] [D loss&acc: 0.252, 47.412%] [G loss&accA&accB: 0.311, 33.789%, 99.688%] [Test loss&acc: 0.671, 0.002%, 99.329%] time: 2:21:52.512983\n",
      "[Epoch 22/26-0/1009] [D loss&acc: 0.253, 43.921%] [G loss&accA&accB: 0.394, 59.033%, 99.606%] [Test loss&acc: 0.673, 0.004%, 99.328%] time: 2:22:06.049899\n",
      "[Epoch 22/26-500/1009] [D loss&acc: 0.253, 46.802%] [G loss&accA&accB: 0.412, 31.982%, 99.587%] [Test loss&acc: 0.676, 0.002%, 99.324%] time: 2:25:10.800926\n",
      "[Epoch 22/26-1000/1009] [D loss&acc: 0.252, 45.923%] [G loss&accA&accB: 0.598, 71.387%, 99.403%] [Test loss&acc: 0.672, 0.021%, 99.328%] time: 2:28:15.529965\n",
      "[Epoch 23/26-0/1009] [D loss&acc: 0.254, 40.771%] [G loss&accA&accB: 0.437, 51.807%, 99.564%] [Test loss&acc: 0.681, 0.030%, 99.320%] time: 2:28:29.078269\n",
      "[Epoch 23/26-500/1009] [D loss&acc: 0.254, 34.570%] [G loss&accA&accB: 0.334, 34.424%, 99.665%] [Test loss&acc: 0.682, 0.032%, 99.318%] time: 2:31:33.521026\n",
      "[Epoch 23/26-1000/1009] [D loss&acc: 0.254, 39.673%] [G loss&accA&accB: 0.641, 55.029%, 99.360%] [Test loss&acc: 0.676, 0.098%, 99.324%] time: 2:34:38.261457\n",
      "[Epoch 24/26-0/1009] [D loss&acc: 0.247, 54.053%] [G loss&accA&accB: 0.443, 21.191%, 99.556%] [Test loss&acc: 0.675, 0.027%, 99.325%] time: 2:34:51.832505\n",
      "[Epoch 24/26-500/1009] [D loss&acc: 0.253, 37.158%] [G loss&accA&accB: 0.424, 15.186%, 99.576%] [Test loss&acc: 0.673, 0.031%, 99.327%] time: 2:37:56.588547\n",
      "[Epoch 24/26-1000/1009] [D loss&acc: 0.252, 48.169%] [G loss&accA&accB: 0.532, 17.285%, 99.468%] [Test loss&acc: 0.682, 0.067%, 99.318%] time: 2:41:01.444751\n",
      "[Epoch 25/26-0/1009] [D loss&acc: 0.256, 31.079%] [G loss&accA&accB: 0.322, 33.301%, 99.678%] [Test loss&acc: 0.664, 0.051%, 99.336%] time: 2:41:14.956919\n",
      "[Epoch 25/26-500/1009] [D loss&acc: 0.252, 45.483%] [G loss&accA&accB: 0.284, 33.301%, 99.716%] [Test loss&acc: 0.668, 0.001%, 99.333%] time: 2:44:19.801572\n",
      "[Epoch 25/26-1000/1009] [D loss&acc: 0.254, 36.768%] [G loss&accA&accB: 0.556, 56.006%, 99.444%] [Test loss&acc: 0.677, 0.134%, 99.323%] time: 2:47:24.404660\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = EL_GAN() # 26 Epochs, 32 \n",
    "    # size 32 - 26 Epochs\n",
    "    # size 8 - 20 Epoch\n",
    "    gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs=26, batch_size=32, sample_interval=500)\n",
    "    #gan.train_generator_only(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    def IoUcheck(img_input, img_output):\n",
    "\n",
    "        logic_and = np.sum(np.logical_and(img_output, img_input))\n",
    "        logic_or = np.sum(np.logical_or(img_output, img_input))\n",
    "\n",
    "        return logic_and/logic_or\n",
    "    \n",
    "    def rescaleImg(image_arr):\n",
    "    \n",
    "        if image_arr.shape[0] % 8 != 0:\n",
    "            n = image_arr.shape[0] % 8\n",
    "            new_x = image_arr.shape[0] - n\n",
    "        else:\n",
    "            new_x = image_arr.shape[0]\n",
    "\n",
    "        if image_arr.shape[1] % 8 != 0:\n",
    "            n = image_arr.shape[1] % 8\n",
    "            new_y = image_arr.shape[1] - n\n",
    "        else:\n",
    "            new_y = image_arr.shape[1]\n",
    "\n",
    "        image_arr = image_arr[:new_x, :new_y]\n",
    "\n",
    "        return image_arr\n",
    "    \n",
    "    def update_model_to_any_size(old_model):\n",
    "    \n",
    "        old_model.layers.pop(0)\n",
    "        \n",
    "        newInput = Input(shape=(None, None, 1)) # New image input\n",
    "        newOutputs = old_model(newInput)\n",
    "        newModel = Model(newInput, newOutputs)\n",
    "        #newModel.summary()\n",
    "\n",
    "        return newModel\n",
    "    \n",
    "    def evaluate(image_arrA, image_arrB):\n",
    "        \n",
    "        y_true = image_arrB.flatten().astype(bool) \n",
    "        y_pred = image_arrA.flatten().astype(bool)\n",
    "        \n",
    "        Accuracy = accuracy_score(image_arrB.flatten().astype(bool), \n",
    "                                  image_arrA.flatten().astype(bool))\n",
    "\n",
    "        IntOverUnion = IoUcheck(image_arrB.flatten().astype(bool), \n",
    "                                image_arrA.flatten().astype(bool))\n",
    "        \n",
    "        conf = confusion_matrix(image_arrB.flatten().astype(bool), \n",
    "                                image_arrA.flatten().astype(bool))\n",
    "        print('aAcc:', Accuracy)\n",
    "        print('Error:', 1 - Accuracy)\n",
    "        print('IoU:', IntOverUnion)\n",
    "        print('Confusion:', conf)\n",
    "        \n",
    "        target_names = ['0', '1']\n",
    "        print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "        \n",
    "        return Accuracy, IntOverUnion\n",
    "        \n",
    "    def model_predict(newModel, input_image, num_runs):\n",
    "        m,n = input_image.shape\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            input_image = np.reshape(input_image, (1, m, n, 1))\n",
    "            conc2 = newModel.predict([input_image])\n",
    "            input_image = np.reshape(conc2,(m, n))\n",
    "\n",
    "        return input_image\n",
    "    \n",
    "    def save_prediction(output_image, fn_input, subfix):\n",
    "        fig = plt.figure(figsize=(output_image.shape[1] / 1000, output_image.shape[0] / 1000), dpi=100, frameon=False)\n",
    "        ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "        ax.set_axis_off()\n",
    "        fig.add_axes(ax)\n",
    "\n",
    "        plt.imshow(output_image, cmap='gray')\n",
    "        #plt.imshow(output_image)\n",
    "        fig.savefig(outPath + fn_input[:-4] + subfix, dpi=1000)\n",
    "\n",
    "    def predict_15k_examples(testPath, fn_input, fn_target, nr = 1):\n",
    "    \n",
    "        image_arrA = readImg(testPath + fn_input)\n",
    "        image_arrB = readImg(testPath + fn_target)\n",
    "\n",
    "        print(\"15k\", 'Example: ')\n",
    "        acc_orig, iou_orig = evaluate(image_arrA, image_arrB)\n",
    "\n",
    "        image_arr = readImg(testPath + fn_input)\n",
    "        image_arr = rescaleImg(image_arr)\n",
    "\n",
    "        image_tar = readImg(testPath + fn_target)\n",
    "        image_tar = rescaleImg(image_tar)\n",
    "\n",
    "        newModel = update_model_to_any_size(gan.generator)\n",
    "        output_image = model_predict(newModel, image_arr, num_runs = nr)\n",
    "\n",
    "        print(\"- 15k\", 'Prediction: ')\n",
    "        acc_pred, iou_pred = evaluate(output_image > 0.5, image_tar)\n",
    "\n",
    "        save_prediction(output_image, fn_input, '_' + str(nr) + '_out.png')\n",
    "\n",
    "        return [[acc_orig, iou_orig], [acc_pred, iou_pred]]\n",
    "    \n",
    "    def predict_15k_only(testPath, fn_input, nr = 1):\n",
    "    \n",
    "        print(\"15k\", 'Example: ')\n",
    "        image_arr = readImg(testPath + fn_input)\n",
    "        image_arr = rescaleImg(image_arr)\n",
    "\n",
    "        newModel = update_model_to_any_size(gan.generator)\n",
    "        output_image = model_predict(newModel, image_arr, num_runs = nr)\n",
    "\n",
    "        print(\"- 15k\", 'Prediction: ')\n",
    "        acc_pred, iou_pred = evaluate(output_image > 0.5, image_tar)\n",
    "\n",
    "        save_prediction(output_image, fn_input, '_' + str(nr) + '_out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testPath = r\"../tmp_data/Data/Testing/\"\n",
    "all_records = []\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\", nr = 1)\n",
    "all_records.extend(records)\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\", nr = 1)\n",
    "all_records.extend(records)\n",
    "\n",
    "records = predict_15k_examples(testPath, r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\", nr = 10)\n",
    "all_records.extend(records)\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\", nr = 10)\n",
    "all_records.extend(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.transpose(all_records))\n",
    "df.columns = [\"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 1)\", \n",
    "              \"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 1)\",\n",
    "              \"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 10)\", \n",
    "              \"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 10)\"]\n",
    "\n",
    "df = df.rename({0: \"Accuracy\", 1: 'IoU'})\n",
    "df.index.name = 'Metrics'\n",
    "\n",
    "df[[\"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 1)\", \"Prediction vs Target (Test1 - 10)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[[\"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 1)\", \"Prediction vs Target (Test2 - 10)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gan.generator.summary()\n",
    "gan.discriminator.summary()\n",
    "gan.combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testPath = r\"../tmp_data/Data/Testing_large/4270/\"\n",
    "all_records = []\n",
    "\n",
    "records = predict_15k_examples(testPath, r\"geb_clip_4270.png\", r\"geb15_clip_4270.png\", nr = 1)\n",
    "all_records.extend(records)\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"geb_clip_4270.png\", r\"geb15_clip_4270.png\", nr = 10)\n",
    "all_records.extend(records)\n",
    "\n",
    "df = pd.DataFrame(np.transpose(all_records))\n",
    "df.columns = [\"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\", \n",
    "              \"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fn_input, fn_target = r\"geb_clip_4270.png\", r\"geb15_clip_4270.png\"\n",
    "image_arrA = readImg(testPath + fn_input)\n",
    "image_arrB = readImg(testPath + fn_target)\n",
    "\n",
    "print(\"15k\", 'Example: ')\n",
    "acc_orig, iou_orig = evaluate(image_arrA, image_arrB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_arr = readImg(testPath + fn_input)\n",
    "image_arr = rescaleImg(image_arr)[:2400, :2400]\n",
    "\n",
    "image_tar = readImg(testPath + fn_target)\n",
    "image_tar = rescaleImg(image_tar)[:2400, :2400]\n",
    "print(image_tar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newModel = update_model_to_any_size(gan.generator)\n",
    "output_image = model_predict(newModel, image_arr, num_runs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"- 15k\", 'Prediction: ')\n",
    "acc_pred, iou_pred = evaluate(output_image > 0.5, image_tar)\n",
    "nr = 1\n",
    "save_prediction(output_image, fn_input, '_' + str(nr) + '_out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newModel = update_model_to_any_size(gan.generator)\n",
    "newModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
