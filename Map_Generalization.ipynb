{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## Map Generalization for Polygons using Autoencode-like strucutures\n",
    "## Adatped based on Master Thesis of SERCAN CAKIR \"ROAD NETWORK EXTRACTION USING CNN\"\n",
    "## Author: Yu Feng, yuzz.feng@gmail.com\n",
    "## 1. Version Author: SERCAN CAKIR\n",
    "\n",
    "## Changes:\n",
    "## 1. Two conv layers were added before the first down convlusional layer\n",
    "## 2. Output can be any size during the evaluation\n",
    "## 3. Adapt the code to support more images as training examples\n",
    "## 4. Dropouot may make the sharpe corners vanishing, we delete half of them, but we should used some\n",
    "## 5. \n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg') # necessary for linux kernal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "np.random.seed(7)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History\n",
    "from keras.layers.core import Dropout\n",
    "#from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import MaxPooling2D, Conv2DTranspose, BatchNormalization, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dropout, UpSampling2D, Activation, Concatenate\n",
    "\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence\n",
    "\n",
    "from time import gmtime, strftime\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "\n",
    "def check_and_create(out_dir):\n",
    "    if os.path.isdir(out_dir) == False:\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "def prediction_independent(model_ex1, image_arr):\n",
    "    \n",
    "    conc2 = np.reshape(model_ex1.predict(np.reshape(image_arr, (1, image_arr.shape[0], image_arr.shape[1], 1))), \n",
    "                   (image_arr.shape[0], image_arr.shape[1]))\n",
    "    return conc2\n",
    "\n",
    "# cut the image to avoid shape error\n",
    "def cut_image(image_arr):\n",
    "    \n",
    "    print(\"Original:\", image_arr.shape)\n",
    "    \n",
    "    if image_arr.shape[0] % 4 != 0:\n",
    "        n = image_arr.shape[0] % 4\n",
    "        new_x = image_arr.shape[0] - n\n",
    "    else:\n",
    "        new_x = image_arr.shape[0]\n",
    "\n",
    "    if image_arr.shape[1] % 4 != 0:\n",
    "        n = image_arr.shape[1] % 4\n",
    "        new_y = image_arr.shape[1] - n\n",
    "    else:\n",
    "        new_y = image_arr.shape[1]\n",
    "    \n",
    "    image_arr = image_arr[:new_x, :new_y]\n",
    "    print(\"Clipped:\", image_arr.shape)\n",
    "\n",
    "\n",
    "############ Path Setting ##############\n",
    "\n",
    "trainPath = r\"Data/Training_Validation/\"\n",
    "testPath = r\"Data/Testing/\"\n",
    "  \n",
    "tmpPath = r\"../tmp_data/\"\n",
    "\n",
    "outPath = r\"Prediction/\"\n",
    "check_and_create(outPath + timestr)\n",
    "outPath = outPath + timestr + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load a saved model\n",
    "def LoadModel(model_json):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open(model_json)\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "##### function to calculate evaluation parameters (F1-Score, Precision, Recall) ######\n",
    "def evaluation(model, x_test, y_test, patch_size):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1Score = []\n",
    "    import math\n",
    "    for k in range(len(x_test_sim)):\n",
    "        y_pred = model.predict(x_test_sim[k:k + 1])\n",
    "        y_pred = np.reshape(y_pred, (32 * 32))\n",
    "\n",
    "        y_true = y_test_sim[k:k + 1]\n",
    "        y_true = np.reshape(y_true, (32 * 32))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        y_pred = np.round(y_pred)\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_true[i] == y_pred[i] == 1:\n",
    "                TP += 1\n",
    "            elif y_pred[i] == y_true[i] == 0:\n",
    "                TN += 1\n",
    "            elif y_pred[i] == 1 and y_true[i] != y_pred[i]:\n",
    "                FP += 1\n",
    "            elif y_pred[i] == 0 and y_true[i] != y_pred[i]:\n",
    "                FN += 1\n",
    "\n",
    "        precision.append(TP / (TP + FP + K.epsilon()))  # completeness\n",
    "        recall.append(TP / (TP + FN))  # correctness\n",
    "        beta = 1\n",
    "        f1Score.append((math.pow(beta, 2) + 1) * TP / ((math.pow(beta, 2) + 1) * TP + math.pow(beta, 2) * FN + FP))\n",
    "        # eval_list = [precision,  recall, f1Score]\n",
    "\n",
    "    avg_precision = sum(precision) / len(precision)\n",
    "    avg_recall = sum(recall) / len(precision)\n",
    "    avg_f1score = sum(f1Score) / len(precision)\n",
    "    avg_eval_param = [avg_precision, avg_recall, avg_f1score]\n",
    "    return avg_eval_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the CNN arch覺tecture with \"Sequential Model\" (model looks like autoencoder)\n",
    "## Version with batch normalozation - Do not benifit that much\n",
    "\n",
    "def create_model_batch(optimizer, input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    droprate = 0.3\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              input_shape=input_shape, kernel_initializer='random_uniform', name=\"flat_conv_a\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same', name=\"flat_conv_b\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    ## Encoding (down-sampling) ###   \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     name=\"down_conv_1\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_1\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_2\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    ## Encoding (down-sampling) ### \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     name=\"down_conv_2\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_3\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_4\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_5\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6a\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6b\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6c\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_7\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_8\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_1'))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"up_conv_1\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_9\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_10\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_2'))\n",
    "\n",
    "    model.add(Conv2D(filters=24, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"up_conv_2\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_11\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\"))\n",
    "    # model.add(Activation(our_activation))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the CNN arch覺tecture with \"Sequential Model\" \n",
    "##### (model looks like autoencoder)\n",
    "def create_model(optimizer, input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    droprate = 0.3\n",
    "\n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu', input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "              name=\"flat_conv_a\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu',name=\"flat_conv_b\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    \n",
    "#    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "#              strides=(1, 1), padding='same',\n",
    "#              activation='relu',name=\"flat_conv_c\"))\n",
    "#    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "#    model.add(Dropout(droprate))\n",
    "    \n",
    "    ## Encoding (down-sampling) ###   \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', #input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "                     name=\"down_conv_1\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_1\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_2\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    ##############################################################################\n",
    "    \n",
    "#    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "#              strides=(1, 1), padding='same',\n",
    "#              activation='relu',name=\"down_conv_2\"))\n",
    "#    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "#    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_2\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_3\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_4\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_5\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_6\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_7\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8\"))\n",
    "    model.add(Dropout(droprate))\n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_1'))\n",
    "    \n",
    "#    model.add(Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), \n",
    "#                              padding='same', activation='softmax'))\n",
    "    \n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_1\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_9\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_10\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_2'))\n",
    "    \n",
    "#    model.add(Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), # Lead the accuracy to 0.78\n",
    "#                              padding='same', activation='softmax'))\n",
    "\n",
    "    model.add(Conv2D(filters=24, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_2\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\"))\n",
    "    # model.add(Activation(our_activation))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "class LearningRateTracker(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr_list = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        # lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        lr = K.eval(\n",
    "            optimizer.lr * (1. / (1. + optimizer.decay * K.cast(optimizer.iterations, K.dtype(optimizer.decay)))))\n",
    "        print('\\n LR: {}\\n'.format(lr))\n",
    "        self.lr_list.append(lr)\n",
    "\n",
    "##################################################################################################################################\n",
    "class SaveWeights(keras.callbacks.Callback):  # Saves weights after each 25 epochs\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 49 == 0:\n",
    "            model_json = self.model.to_json()\n",
    "            with open(\"model_\" + str(epoch) + \".json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            self.model.save_weights(\"weights_model_\" + str(epoch) + \".h5\")\n",
    "            print(\"Saved model-weights to disk\")\n",
    "\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the CNN arch覺tecture with \"Model\" - skip connections were added\n",
    "def create_model_add_skips(optimizer, input_shape, drop_rate = 0.3):\n",
    "\n",
    "    \n",
    "    i = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu', input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "              name=\"flat_conv_a\")(i)\n",
    "    first_skip = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu',name=\"flat_conv_b\")(x)\n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu',\n",
    "                     name=\"down_conv_1\")(first_skip)\n",
    "    x = Dropout(drop_rate)(x) ################################################# First Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_1\")(x)\n",
    "    second_skip = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_2\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_2\")(second_skip)\n",
    "    x = Dropout(drop_rate)(x) ################################################# Second Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_3\")(x)\n",
    "    third_skip = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_4\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_3\")(third_skip)\n",
    "    x = Dropout(drop_rate)(x) ################################################# Third Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_5\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_6\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_7\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 4th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_0')(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_0\")(x)\n",
    "    concat0 = Concatenate()([third_skip, x])\n",
    "    x = Conv2D(filters=128, kernel_size=(1, 1),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8\")(concat0)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8b\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 5th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_1')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_1\")(x)\n",
    "    concat = Concatenate()([second_skip, x])\n",
    "    x = Conv2D(filters=64, kernel_size=(1, 1),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_9\")(concat)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_10\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 6th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_2')(x)\n",
    "    x = Conv2D(filters=24, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_2\")(x)\n",
    "    concat2 = Concatenate()([first_skip, x])\n",
    "    x = Conv2D(filters=12, kernel_size=(1, 1),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11\")(concat2)\n",
    "    x = Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11b\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 7th Drop\n",
    "    \n",
    "    \n",
    "    o = Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\")(x)\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the CNN arch覺tecture with \"Model\" - skip connections were added\n",
    "def create_model_add_skips_2(optimizer, input_shape, drop_rate = 0.3):\n",
    "\n",
    "    \n",
    "    i = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu', input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "              name=\"flat_conv_a\")(i)\n",
    "    first_skip = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu',name=\"flat_conv_b\")(x)\n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu',\n",
    "                     name=\"down_conv_1\")(first_skip)\n",
    "    #x = Dropout(drop_rate)(x) ################################################# First Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_1\")(x)\n",
    "    second_skip = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_2\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_2\")(second_skip)\n",
    "    #x = Dropout(drop_rate)(x) ################################################# Second Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_3\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_4\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"xx_conv_3\")(x)\n",
    "    #x = Dropout(drop_rate)(x) ################################################# Third Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_5\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_6\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_7\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 4th Drop\n",
    "    \n",
    "    \n",
    "\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"xx_conv_0\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8b\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 5th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_1')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_1\")(x)\n",
    "    concat = Concatenate()([second_skip, x])\n",
    "    x = Conv2D(filters=64, kernel_size=(1, 1),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_9\")(concat)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_10\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 6th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_2')(x)\n",
    "    x = Conv2D(filters=24, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_2\")(x)\n",
    "    concat2 = Concatenate()([first_skip, x])\n",
    "    x = Conv2D(filters=12, kernel_size=(1, 1),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11\")(concat2)\n",
    "    x = Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11b\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 7th Drop\n",
    "    \n",
    "    \n",
    "    o = Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\")(x)\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tiles:  16828\n",
      "15828 1000 15828 1000\n",
      "Input Shape of the models (15828, 128, 128, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff86980cef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff869804da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "# set the working directory\n",
    "#os.chdir(r'F:\\sercan\\input_images')\n",
    "PATH = os.getcwd()\n",
    "#plt.gray()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "\n",
    "def data_collector(fns_input, fns_output):\n",
    "    \n",
    "    sim_input = []\n",
    "    sim_output = []\n",
    "    \n",
    "    for fn_input, fn_output in zip(fns_input, fns_output):\n",
    "        \n",
    "        # load simulated heat map (TRAJECTORY SIMULATION) and target road for Hannover ####\n",
    "        sim_heatmap_hannover = readImg(fn_input)\n",
    "        sim_road_hannover = readImg(fn_output)\n",
    "        \n",
    "        sim_hm_patches_overlap = imagePatches(sim_heatmap_hannover, p_size_1, p_size_1, int(p_size_1))\n",
    "        sim_road_patches_overlap = imagePatches(sim_road_hannover, p_size_1, p_size_1, int(p_size_1))\n",
    "        sim_road_patches_overlap_new = removeCorrespondence(sim_road_patches_overlap, sim_hm_patches_overlap)\n",
    "        sim_hm_patches_overlap_new = removeCorrespondence(sim_hm_patches_overlap, sim_road_patches_overlap)\n",
    "        sim_road_patches_overlap_new_new = removeBlackImg(sim_road_patches_overlap)\n",
    "        \n",
    "        sim_input += sim_hm_patches_overlap_new\n",
    "        sim_output += sim_road_patches_overlap_new_new\n",
    "    \n",
    "    return sim_input, sim_output\n",
    "\n",
    "fns_input = [trainPath + r\"traininput_inv.png\"]\n",
    "fns_output = [trainPath + r\"trainoutput_inv.png\"]\n",
    "\n",
    "#fns_input = [r\"data/input2.tif\"]#, r\"data/geb1_inp_inv_cut.tif\"]\n",
    "#fns_output = [r\"data/output2.tif\"]#, r\"data/geb1_out_inv_cut.tif\"]\n",
    "\n",
    "sim_hm_patches_32_new, sim_road_patches_32_new_new = data_collector(fns_input, fns_output)\n",
    "print('Number of tiles: ', len(sim_hm_patches_32_new))\n",
    "\n",
    "#### experience 1 - simulated hm\n",
    "index_list_sim = list(range(len(sim_hm_patches_32_new)))\n",
    "random.shuffle(index_list_sim)\n",
    "\n",
    "idx_sim = 1000\n",
    "index_list_test_sim = index_list_sim[-idx_sim:]\n",
    "index_list_test_sim.sort()\n",
    "sim_hm_test = [sim_hm_patches_32_new[i] for i in index_list_test_sim]\n",
    "sim_road_test = [sim_road_patches_32_new_new[i] for i in index_list_test_sim]\n",
    "\n",
    "index_list_train_sim = index_list_sim[:-idx_sim]\n",
    "index_list_train_sim.sort()\n",
    "sim_hm_train = [sim_hm_patches_32_new[i] for i in index_list_train_sim]\n",
    "sim_road_train = [sim_road_patches_32_new_new[i] for i in index_list_train_sim]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sim_hm_train, sim_hm_test, sim_road_train, sim_road_test = train_test_split(sim_hm_patches_32_new, \n",
    "#                                                                            sim_road_patches_32_new_new,\n",
    "#                                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "print(len(sim_hm_train), len(sim_hm_test), len(sim_road_train), len(sim_road_test))\n",
    "\n",
    "x_train_sim = np.reshape(sim_hm_train, (len(sim_hm_train), p_size_1, p_size_1, 1))\n",
    "y_train_sim = np.reshape(sim_road_train, (len(sim_road_train), p_size_1, p_size_1, 1))\n",
    "x_test_sim = np.reshape(sim_hm_test, (len(sim_hm_test), p_size_1, p_size_1, 1))\n",
    "y_test_sim = np.reshape(sim_road_test, (len(sim_road_test), p_size_1, p_size_1, 1))\n",
    "\n",
    "# save image patch arrays\n",
    "np.save(tmpPath + \"x_train_sim.npy\", x_train_sim)\n",
    "np.save(tmpPath + \"y_train_sim.npy\", y_train_sim)\n",
    "np.save(tmpPath + \"x_test_sim.npy\", x_test_sim)\n",
    "np.save(tmpPath + \"y_test_sim.npy\", y_test_sim)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(x_test_sim[2], (p_size_1,p_size_1)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(y_test_sim[2], (p_size_1,p_size_1)))\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the models', x_train_sim.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_a (Conv2D)            (None, None, None, 2 240         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_b (Conv2D)            (None, None, None, 2 5208        flat_conv_a[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "down_conv_1 (Conv2D)            (None, None, None, 2 5208        flat_conv_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_1 (Conv2D)            (None, None, None, 6 13888       down_conv_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_2 (Conv2D)            (None, None, None, 6 36928       flat_conv_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "down_conv_2 (Conv2D)            (None, None, None, 6 36928       flat_conv_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_3 (Conv2D)            (None, None, None, 1 73856       down_conv_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_4 (Conv2D)            (None, None, None, 1 147584      flat_conv_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "xx_conv_3 (Conv2D)              (None, None, None, 1 147584      flat_conv_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_5 (Conv2D)            (None, None, None, 2 295168      xx_conv_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_6 (Conv2D)            (None, None, None, 2 590080      flat_conv_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_7 (Conv2D)            (None, None, None, 2 590080      flat_conv_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, None, 2 0           flat_conv_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "xx_conv_0 (Conv2D)              (None, None, None, 1 295040      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_8 (Conv2D)            (None, None, None, 1 147584      xx_conv_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_8b (Conv2D)           (None, None, None, 1 147584      flat_conv_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, None, None, 1 0           flat_conv_8b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_samp_1 (UpSampling2D)        (None, None, None, 1 0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_conv_1 (Conv2D)              (None, None, None, 6 131136      up_samp_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 1 0           flat_conv_2[0][0]                \n",
      "                                                                 up_conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_9 (Conv2D)            (None, None, None, 6 8256        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_10 (Conv2D)           (None, None, None, 6 36928       flat_conv_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, None, None, 6 0           flat_conv_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_samp_2 (UpSampling2D)        (None, None, None, 6 0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_conv_2 (Conv2D)              (None, None, None, 2 24600       up_samp_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           flat_conv_b[0][0]                \n",
      "                                                                 up_conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_11 (Conv2D)           (None, None, None, 1 588         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_11b (Conv2D)          (None, None, None, 1 1308        flat_conv_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, None, None, 1 0           flat_conv_11b[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_12 (Conv2D)           (None, None, None, 1 109         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,735,885\n",
      "Trainable params: 2,735,885\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt1 = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "\n",
    "#model_ex1 = create_model(opt1, input_shape1)\n",
    "\n",
    "#model_ex1 = create_model_batch(opt1, input_shape1)\n",
    "\n",
    "#model_ex1 = create_model_add_skips(opt1, input_shape1)\n",
    "\n",
    "model_ex1 = create_model_add_skips_2(opt1, input_shape1)\n",
    "\n",
    "model_ex1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15828 samples, validate on 1000 samples\n",
      "Epoch 1/80\n",
      "15828/15828 [==============================] - 84s 5ms/step - loss: 0.0849 - acc: 0.9821 - val_loss: 0.0426 - val_acc: 0.9885\n",
      "Epoch 2/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0459 - acc: 0.9880 - val_loss: 0.0394 - val_acc: 0.9893\n",
      "Epoch 3/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0438 - acc: 0.9887 - val_loss: 0.0378 - val_acc: 0.9896\n",
      "Epoch 4/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0423 - acc: 0.9890 - val_loss: 0.0381 - val_acc: 0.9897\n",
      "Epoch 5/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0412 - acc: 0.9892 - val_loss: 0.0361 - val_acc: 0.9898\n",
      "Epoch 6/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0403 - acc: 0.9893 - val_loss: 0.0351 - val_acc: 0.9898\n",
      "Epoch 7/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0394 - acc: 0.9893 - val_loss: 0.0377 - val_acc: 0.9898\n",
      "Epoch 8/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0389 - acc: 0.9894 - val_loss: 0.0339 - val_acc: 0.9899\n",
      "Epoch 9/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0384 - acc: 0.9894 - val_loss: 0.0341 - val_acc: 0.9899\n",
      "Epoch 10/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0378 - acc: 0.9895 - val_loss: 0.0328 - val_acc: 0.9900\n",
      "Epoch 11/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0373 - acc: 0.9895 - val_loss: 0.0321 - val_acc: 0.9901\n",
      "Epoch 12/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0368 - acc: 0.9896 - val_loss: 0.0318 - val_acc: 0.9902\n",
      "Epoch 13/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0363 - acc: 0.9897 - val_loss: 0.0320 - val_acc: 0.9902\n",
      "Epoch 14/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0357 - acc: 0.9898 - val_loss: 0.0307 - val_acc: 0.9903\n",
      "Epoch 15/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0350 - acc: 0.9899 - val_loss: 0.0302 - val_acc: 0.9905\n",
      "Epoch 16/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0343 - acc: 0.9899 - val_loss: 0.0312 - val_acc: 0.9901\n",
      "Epoch 17/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0340 - acc: 0.9900 - val_loss: 0.0294 - val_acc: 0.9906\n",
      "Epoch 18/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0334 - acc: 0.9901 - val_loss: 0.0288 - val_acc: 0.9907\n",
      "Epoch 19/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0331 - acc: 0.9903 - val_loss: 0.0309 - val_acc: 0.9895\n",
      "Epoch 20/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0326 - acc: 0.9904 - val_loss: 0.0277 - val_acc: 0.9910\n",
      "Epoch 21/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0321 - acc: 0.9905 - val_loss: 0.0280 - val_acc: 0.9907\n",
      "Epoch 22/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0314 - acc: 0.9907 - val_loss: 0.0274 - val_acc: 0.9913\n",
      "Epoch 23/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0310 - acc: 0.9908 - val_loss: 0.0268 - val_acc: 0.9915\n",
      "Epoch 24/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0307 - acc: 0.9909 - val_loss: 0.0267 - val_acc: 0.9913\n",
      "Epoch 25/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0301 - acc: 0.9911 - val_loss: 0.0346 - val_acc: 0.9876\n",
      "Epoch 26/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0298 - acc: 0.9912 - val_loss: 0.0258 - val_acc: 0.9919\n",
      "Epoch 27/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0295 - acc: 0.9913 - val_loss: 0.0257 - val_acc: 0.9919\n",
      "Epoch 28/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0293 - acc: 0.9914 - val_loss: 0.0251 - val_acc: 0.9920\n",
      "Epoch 29/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0289 - acc: 0.9915 - val_loss: 0.0252 - val_acc: 0.9920\n",
      "Epoch 30/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0285 - acc: 0.9916 - val_loss: 0.0252 - val_acc: 0.9921\n",
      "Epoch 31/80\n",
      "15828/15828 [==============================] - 80s 5ms/step - loss: 0.0284 - acc: 0.9917 - val_loss: 0.0253 - val_acc: 0.9920\n",
      "Epoch 32/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0278 - acc: 0.9918 - val_loss: 0.0247 - val_acc: 0.9922\n",
      "Epoch 33/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0280 - acc: 0.9919 - val_loss: 0.0244 - val_acc: 0.9923\n",
      "Epoch 34/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0275 - acc: 0.9920 - val_loss: 0.0246 - val_acc: 0.9921\n",
      "Epoch 35/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0273 - acc: 0.9920 - val_loss: 0.0240 - val_acc: 0.9924\n",
      "Epoch 36/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0271 - acc: 0.9921 - val_loss: 0.0243 - val_acc: 0.9924\n",
      "Epoch 37/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0267 - acc: 0.9922 - val_loss: 0.0247 - val_acc: 0.9924\n",
      "Epoch 38/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0266 - acc: 0.9922 - val_loss: 0.0280 - val_acc: 0.9909\n",
      "Epoch 39/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0265 - acc: 0.9923 - val_loss: 0.0253 - val_acc: 0.9917\n",
      "Epoch 40/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0264 - acc: 0.9923 - val_loss: 0.0254 - val_acc: 0.9922\n",
      "Epoch 41/80\n",
      "15828/15828 [==============================] - 79s 5ms/step - loss: 0.0259 - acc: 0.9924 - val_loss: 0.0246 - val_acc: 0.9924\n",
      "Epoch 42/80\n",
      " 2304/15828 [===>..........................] - ETA: 1:06 - loss: 0.0212 - acc: 0.9934"
     ]
    }
   ],
   "source": [
    "##### Train the model\n",
    "#covariance1 = Covariance()\n",
    "History1 = History()\n",
    "hist1 = model_ex1.fit(x_train_sim, y_train_sim,\n",
    "                      batch_size=16,\n",
    "                      epochs = 80,\n",
    "                      verbose=1,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[History1],\n",
    "                      validation_data=(x_test_sim, y_test_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save history\n",
    "History1_loss = History1.history['loss']\n",
    "History1_acc = History1.history['acc']\n",
    "History1_val_loss = History1.history['val_loss']\n",
    "History1_val_acc = History1.history['val_acc']\n",
    "\n",
    "\n",
    "thefile1 = open(outPath + 'History1_loss.txt', 'w')\n",
    "for item in History1_loss:\n",
    "    thefile1.write(\"%s\\n\" % item)\n",
    "thefile1.close()\n",
    "\n",
    "thefile2 = open(outPath + 'History1_acc.txt', 'w')\n",
    "for item in History1_acc:\n",
    "    thefile2.write(\"%s\\n\" % item)\n",
    "thefile2.close()\n",
    "\n",
    "thefile3 = open(outPath + 'History1_val_loss.txt', 'w')\n",
    "for item in History1_val_loss:\n",
    "    thefile3.write(\"%s\\n\" % item)\n",
    "thefile3.close()\n",
    "\n",
    "thefile4 = open(outPath + 'History1_val_acc.txt', 'w')\n",
    "for item in History1_val_acc:\n",
    "    thefile4.write(\"%s\\n\" % item)\n",
    "thefile4.close()\n",
    "\n",
    "### Save model\n",
    "model_json1 = model_ex1.to_json()\n",
    "with open(tmpPath + \"model_ex1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json1)\n",
    "model_ex1.save_weights(tmpPath + \"weights_model_ex1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot history of average covariance - accuracy and loss of the models\n",
    "plt.figure()\n",
    "plt.plot(History1.history['loss'])\n",
    "plt.plot(History1.history['val_loss'])\n",
    "plt.title('loss & val_loss')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.savefig(outPath + \"loss\", dpi=1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(History1.history['acc'])\n",
    "plt.plot(History1.history['val_acc'])\n",
    "plt.title('acc & val_acc')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.savefig(outPath + \"acc\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def testIndependet(fn, inpath, outpath):\n",
    "    \n",
    "    image_arr = readImg(inpath + fn)\n",
    "    print(image_arr.shape)\n",
    "    \n",
    "    if image_arr.shape[0] % 8 != 0:\n",
    "        n = image_arr.shape[0] % 8\n",
    "        new_x = image_arr.shape[0] - n\n",
    "    else:\n",
    "        new_x = image_arr.shape[0]\n",
    "\n",
    "\n",
    "    if image_arr.shape[1] % 8 != 0:\n",
    "        n = image_arr.shape[1] % 8\n",
    "        new_y = image_arr.shape[1] - n\n",
    "    else:\n",
    "        new_y = image_arr.shape[1]\n",
    "\n",
    "    image_arr = image_arr[:new_x, :new_y]\n",
    "    print(image_arr.shape)\n",
    "    \n",
    "    conc2 = np.reshape(model_ex1.predict(np.reshape(image_arr, (1, image_arr.shape[0], image_arr.shape[1], 1))), \n",
    "                       (image_arr.shape[0], image_arr.shape[1]))\n",
    "    \n",
    "    print(accuracy_score(image_arr.flatten().astype(bool), (conc2 > 0.5).flatten()))\n",
    "    \n",
    "    fig = plt.figure(figsize=(image_arr.shape[1] / 1000, image_arr.shape[0] / 1000), dpi=100, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    \n",
    "    plt.imshow(conc2)\n",
    "    fig.savefig(outpath + fn[:-4] + '_out.png', dpi=1000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(image_arr.shape[1] / 1000, image_arr.shape[0] / 1000), dpi=100, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    \n",
    "    conc2 = conc2 > 0.5\n",
    "    plt.imshow(conc2, cmap='gray')\n",
    "    fig.savefig(outpath + fn[:-4] + '_out_bw.png', dpi=1000)\n",
    "    \n",
    "\n",
    "testIndependet(r\"testexampleinput2.tif\", testPath, outPath)\n",
    "testIndependet(r\"FTest1_input_inv.png\", testPath, outPath)\n",
    "testIndependet(r\"FTest2_input_inv.png\", testPath, outPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrA = readImg(testPath + r\"FTest1_input_inv.png\")\n",
    "image_arrB = readImg(testPath + r\"FTest1_output_inv.png\")\n",
    "print(accuracy_score(image_arrB.flatten().astype(bool), \n",
    "                     image_arrA.flatten().astype(bool))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrA = readImg(testPath + r\"FTest2_input_inv.png\")\n",
    "image_arrB = readImg(testPath + r\"FTest2_output_inv.png\")\n",
    "print(accuracy_score(image_arrB.flatten().astype(bool), \n",
    "                     image_arrA.flatten().astype(bool))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
