{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## Map Generalization for Polygons using Autoencode-like strucutures\n",
    "## Adatped based on Master Thesis of SERCAN CAKIR \"ROAD NETWORK EXTRACTION USING CNN\"\n",
    "## Author: Yu Feng, yuzz.feng@gmail.com\n",
    "## 1. Version Author: SERCAN CAKIR\n",
    "\n",
    "## Changes:\n",
    "## 1. Two conv layers were added before the first down convlusional layer\n",
    "## 2. Output can be any size during the evaluation\n",
    "## 3. Adapt the code to support more images as training examples\n",
    "## 4. Dropouot may make the sharpe corners vanishing, we delete half of them, but we should used some\n",
    "## 5. \n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg') # necessary for linux kernal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "np.random.seed(7)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History\n",
    "from keras.layers.core import Dropout\n",
    "#from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import MaxPooling2D, Conv2DTranspose, BatchNormalization, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dropout, UpSampling2D, Activation, Concatenate\n",
    "\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence\n",
    "\n",
    "from time import gmtime, strftime\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "\n",
    "def check_and_create(out_dir):\n",
    "    if os.path.isdir(out_dir) == False:\n",
    "        os.mkdir(out_dir)\n",
    "\n",
    "def prediction_independent(model_ex1, image_arr):\n",
    "    \n",
    "    conc2 = np.reshape(model_ex1.predict(np.reshape(image_arr, (1, image_arr.shape[0], image_arr.shape[1], 1))), \n",
    "                   (image_arr.shape[0], image_arr.shape[1]))\n",
    "    return conc2\n",
    "\n",
    "# cut the image to avoid shape error\n",
    "def cut_image(image_arr):\n",
    "    \n",
    "    print(\"Original:\", image_arr.shape)\n",
    "    \n",
    "    if image_arr.shape[0] % 4 != 0:\n",
    "        n = image_arr.shape[0] % 4\n",
    "        new_x = image_arr.shape[0] - n\n",
    "    else:\n",
    "        new_x = image_arr.shape[0]\n",
    "\n",
    "    if image_arr.shape[1] % 4 != 0:\n",
    "        n = image_arr.shape[1] % 4\n",
    "        new_y = image_arr.shape[1] - n\n",
    "    else:\n",
    "        new_y = image_arr.shape[1]\n",
    "    \n",
    "    image_arr = image_arr[:new_x, :new_y]\n",
    "    print(\"Clipped:\", image_arr.shape)\n",
    "\n",
    "\n",
    "############ Path Setting ##############\n",
    "\n",
    "trainPath = r\"Data/Training_Validation/\"\n",
    "testPath = r\"Data/Testing/\"\n",
    "  \n",
    "tmpPath = r\"../tmp_data/\"\n",
    "\n",
    "outPath = r\"Prediction/\"\n",
    "check_and_create(outPath + timestr)\n",
    "outPath = outPath + timestr + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load a saved model\n",
    "def LoadModel(model_json):\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open(model_json)\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    return loaded_model\n",
    "\n",
    "\n",
    "##### function to calculate evaluation parameters (F1-Score, Precision, Recall) ######\n",
    "def evaluation(model, x_test, y_test, patch_size):\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1Score = []\n",
    "    import math\n",
    "    for k in range(len(x_test_sim)):\n",
    "        y_pred = model.predict(x_test_sim[k:k + 1])\n",
    "        y_pred = np.reshape(y_pred, (32 * 32))\n",
    "\n",
    "        y_true = y_test_sim[k:k + 1]\n",
    "        y_true = np.reshape(y_true, (32 * 32))\n",
    "\n",
    "        TP = 0\n",
    "        FP = 0\n",
    "        FN = 0\n",
    "        TN = 0\n",
    "\n",
    "        y_pred = np.round(y_pred)\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_true[i] == y_pred[i] == 1:\n",
    "                TP += 1\n",
    "            elif y_pred[i] == y_true[i] == 0:\n",
    "                TN += 1\n",
    "            elif y_pred[i] == 1 and y_true[i] != y_pred[i]:\n",
    "                FP += 1\n",
    "            elif y_pred[i] == 0 and y_true[i] != y_pred[i]:\n",
    "                FN += 1\n",
    "\n",
    "        precision.append(TP / (TP + FP + K.epsilon()))  # completeness\n",
    "        recall.append(TP / (TP + FN))  # correctness\n",
    "        beta = 1\n",
    "        f1Score.append((math.pow(beta, 2) + 1) * TP / ((math.pow(beta, 2) + 1) * TP + math.pow(beta, 2) * FN + FP))\n",
    "        # eval_list = [precision,  recall, f1Score]\n",
    "\n",
    "    avg_precision = sum(precision) / len(precision)\n",
    "    avg_recall = sum(recall) / len(precision)\n",
    "    avg_f1score = sum(f1Score) / len(precision)\n",
    "    avg_eval_param = [avg_precision, avg_recall, avg_f1score]\n",
    "    return avg_eval_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building the CNN archıtecture with \"Sequential Model\" (model looks like autoencoder)\n",
    "## Version with batch normalozation - Do not benifit that much\n",
    "\n",
    "def create_model_batch(optimizer, input_shape):\n",
    "    \n",
    "    model = Sequential()\n",
    "    droprate = 0.3\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              input_shape=input_shape, kernel_initializer='random_uniform', name=\"flat_conv_a\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same', name=\"flat_conv_b\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    ## Encoding (down-sampling) ###   \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     name=\"down_conv_1\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_1\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_2\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    ## Encoding (down-sampling) ### \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     name=\"down_conv_2\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_3\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_4\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_5\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6a\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6b\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=512, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_6c\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_7\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_8\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    \n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_1'))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"up_conv_1\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_9\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_10\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_2'))\n",
    "\n",
    "    model.add(Conv2D(filters=24, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"up_conv_2\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     name=\"flat_conv_11\"))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\"))\n",
    "    # model.add(Activation(our_activation))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the CNN archıtecture with \"Sequential Model\" \n",
    "##### (model looks like autoencoder)\n",
    "def create_model(optimizer, input_shape):\n",
    "    model = Sequential()\n",
    "    \n",
    "    droprate = 0.3\n",
    "\n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu', input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "              name=\"flat_conv_a\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu',name=\"flat_conv_b\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    \n",
    "#    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "#              strides=(1, 1), padding='same',\n",
    "#              activation='relu',name=\"flat_conv_c\"))\n",
    "#    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "#    model.add(Dropout(droprate))\n",
    "    \n",
    "    ## Encoding (down-sampling) ###   \n",
    "    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', #input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "                     name=\"down_conv_1\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_1\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_2\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    ##############################################################################\n",
    "    \n",
    "#    model.add(Conv2D(filters=24, kernel_size=(3, 3),\n",
    "#              strides=(1, 1), padding='same',\n",
    "#              activation='relu',name=\"down_conv_2\"))\n",
    "#    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
    "#    model.add(Dropout(droprate))\n",
    "    \n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_2\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_3\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_4\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_5\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_6\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_7\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8\"))\n",
    "    model.add(Dropout(droprate))\n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_1'))\n",
    "    \n",
    "#    model.add(Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), \n",
    "#                              padding='same', activation='softmax'))\n",
    "    \n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_1\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_9\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_10\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "    ###############################################################################\n",
    "    model.add(UpSampling2D(size=(2, 2), name='up_samp_2'))\n",
    "    \n",
    "#    model.add(Conv2DTranspose(filters=64, kernel_size=(3, 3), strides=(2, 2), # Lead the accuracy to 0.78\n",
    "#                              padding='same', activation='softmax'))\n",
    "\n",
    "    model.add(Conv2D(filters=24, kernel_size=(4, 4),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_2\"))\n",
    "    model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11\"))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    model.add(Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\"))\n",
    "    # model.add(Activation(our_activation))\n",
    "    #model.add(Dropout(droprate))\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "##################################################################################################################################\n",
    "class LearningRateTracker(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.lr_list = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        optimizer = self.model.optimizer\n",
    "        # lr = K.eval(optimizer.lr * (1. / (1. + optimizer.decay * optimizer.iterations)))\n",
    "        lr = K.eval(\n",
    "            optimizer.lr * (1. / (1. + optimizer.decay * K.cast(optimizer.iterations, K.dtype(optimizer.decay)))))\n",
    "        print('\\n LR: {}\\n'.format(lr))\n",
    "        self.lr_list.append(lr)\n",
    "\n",
    "##################################################################################################################################\n",
    "class SaveWeights(keras.callbacks.Callback):  # Saves weights after each 25 epochs\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % 49 == 0:\n",
    "            model_json = self.model.to_json()\n",
    "            with open(\"model_\" + str(epoch) + \".json\", \"w\") as json_file:\n",
    "                json_file.write(model_json)\n",
    "            self.model.save_weights(\"weights_model_\" + str(epoch) + \".h5\")\n",
    "            print(\"Saved model-weights to disk\")\n",
    "\n",
    "##################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Building the CNN archıtecture with \"Model\" - skip connections were added\n",
    "def create_model_add_skips(optimizer, input_shape, drop_rate = 0.3):\n",
    "\n",
    "    \n",
    "    i = Input(shape=input_shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu', input_shape=input_shape, kernel_initializer='random_uniform',\n",
    "              name=\"flat_conv_a\")(i)\n",
    "    first_skip = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "              strides=(1, 1), padding='same',\n",
    "              activation='relu',name=\"flat_conv_b\")(x)\n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu',\n",
    "                     name=\"down_conv_1\")(first_skip)\n",
    "    x = Dropout(drop_rate)(x) ################################################# First Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_1\")(x)\n",
    "    second_skip = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_2\")(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_2\")(second_skip)\n",
    "    x = Dropout(drop_rate)(x) ################################################# Second Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_3\")(x)\n",
    "    third_skip = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_4\")(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(2, 2), padding='same',\n",
    "                     activation='relu', name=\"down_conv_3\")(third_skip)\n",
    "    x = Dropout(drop_rate)(x) ################################################# Third Drop\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_5\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_6\")(x)\n",
    "    x = Conv2D(filters=256, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_7\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 4th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_0')(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_0\")(x)\n",
    "    concat0 = Concatenate()([third_skip, x])\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8\")(concat0)\n",
    "    x = Conv2D(filters=128, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_8b\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 5th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_1')(x)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_1\")(x)\n",
    "    concat = Concatenate()([second_skip, x])\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_9\")(concat)\n",
    "    x = Conv2D(filters=64, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_10\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 6th Drop\n",
    "    \n",
    "    \n",
    "    x = UpSampling2D(size=(2, 2), name='up_samp_2')(x)\n",
    "    x = Conv2D(filters=24, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"up_conv_2\")(x)\n",
    "    concat2 = Concatenate()([first_skip, x])\n",
    "    x = Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11\")(concat2)\n",
    "    x = Conv2D(filters=12, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='relu', name=\"flat_conv_11b\")(x)\n",
    "    x = Dropout(drop_rate)(x) ################################################# 7th Drop\n",
    "    \n",
    "    \n",
    "    o = Conv2D(filters=1, kernel_size=(3, 3),\n",
    "                     strides=(1, 1), padding='same',\n",
    "                     activation='sigmoid', name=\"flat_conv_12\")(x)\n",
    "    model = Model(inputs=i, outputs=o)\n",
    "\n",
    "    # Compile model with Adam optimizer and binary cross entropy loss function\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='mean_absolute_error',\n",
    "                  metrics=['acc'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "# set the working directory\n",
    "#os.chdir(r'F:\\sercan\\input_images')\n",
    "PATH = os.getcwd()\n",
    "#plt.gray()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "\n",
    "def data_collector(fns_input, fns_output):\n",
    "    \n",
    "    sim_input = []\n",
    "    sim_output = []\n",
    "    \n",
    "    for fn_input, fn_output in zip(fns_input, fns_output):\n",
    "        \n",
    "        # load simulated heat map (TRAJECTORY SIMULATION) and target road for Hannover ####\n",
    "        sim_heatmap_hannover = readImg(fn_input)\n",
    "        sim_road_hannover = readImg(fn_output)\n",
    "        \n",
    "        sim_hm_patches_overlap = imagePatches(sim_heatmap_hannover, p_size_1, p_size_1, int(p_size_1))\n",
    "        sim_road_patches_overlap = imagePatches(sim_road_hannover, p_size_1, p_size_1, int(p_size_1))\n",
    "        sim_road_patches_overlap_new = removeCorrespondence(sim_road_patches_overlap, sim_hm_patches_overlap)\n",
    "        sim_hm_patches_overlap_new = removeCorrespondence(sim_hm_patches_overlap, sim_road_patches_overlap)\n",
    "        sim_road_patches_overlap_new_new = removeBlackImg(sim_road_patches_overlap)\n",
    "        \n",
    "        sim_input += sim_hm_patches_overlap_new\n",
    "        sim_output += sim_road_patches_overlap_new_new\n",
    "    \n",
    "    return sim_input, sim_output\n",
    "\n",
    "fns_input = [trainPath + r\"traininput_inv.png\"]\n",
    "fns_output = [trainPath + r\"trainoutput_inv.png\"]\n",
    "\n",
    "#fns_input = [r\"data/input2.tif\"]#, r\"data/geb1_inp_inv_cut.tif\"]\n",
    "#fns_output = [r\"data/output2.tif\"]#, r\"data/geb1_out_inv_cut.tif\"]\n",
    "\n",
    "sim_hm_patches_32_new, sim_road_patches_32_new_new = data_collector(fns_input, fns_output)\n",
    "print('Number of tiles: ', len(sim_hm_patches_32_new))\n",
    "\n",
    "#### experience 1 - simulated hm\n",
    "index_list_sim = list(range(len(sim_hm_patches_32_new)))\n",
    "random.shuffle(index_list_sim)\n",
    "\n",
    "idx_sim = 1000\n",
    "index_list_test_sim = index_list_sim[-idx_sim:]\n",
    "index_list_test_sim.sort()\n",
    "sim_hm_test = [sim_hm_patches_32_new[i] for i in index_list_test_sim]\n",
    "sim_road_test = [sim_road_patches_32_new_new[i] for i in index_list_test_sim]\n",
    "\n",
    "index_list_train_sim = index_list_sim[:-idx_sim]\n",
    "index_list_train_sim.sort()\n",
    "sim_hm_train = [sim_hm_patches_32_new[i] for i in index_list_train_sim]\n",
    "sim_road_train = [sim_road_patches_32_new_new[i] for i in index_list_train_sim]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sim_hm_train, sim_hm_test, sim_road_train, sim_road_test = train_test_split(sim_hm_patches_32_new, \n",
    "#                                                                            sim_road_patches_32_new_new,\n",
    "#                                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "print(len(sim_hm_train), len(sim_hm_test), len(sim_road_train), len(sim_road_test))\n",
    "\n",
    "x_train_sim = np.reshape(sim_hm_train, (len(sim_hm_train), p_size_1, p_size_1, 1))\n",
    "y_train_sim = np.reshape(sim_road_train, (len(sim_road_train), p_size_1, p_size_1, 1))\n",
    "x_test_sim = np.reshape(sim_hm_test, (len(sim_hm_test), p_size_1, p_size_1, 1))\n",
    "y_test_sim = np.reshape(sim_road_test, (len(sim_road_test), p_size_1, p_size_1, 1))\n",
    "\n",
    "# save image patch arrays\n",
    "np.save(tmpPath + \"x_train_sim.npy\", x_train_sim)\n",
    "np.save(tmpPath + \"y_train_sim.npy\", y_train_sim)\n",
    "np.save(tmpPath + \"x_test_sim.npy\", x_test_sim)\n",
    "np.save(tmpPath + \"y_test_sim.npy\", y_test_sim)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(x_test_sim[2], (p_size_1,p_size_1)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(y_test_sim[2], (p_size_1,p_size_1)))\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the models', x_train_sim.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "\n",
    "model_ex1 = create_model(opt1, input_shape1)\n",
    "\n",
    "#model_ex1 = create_model_batch(opt1, input_shape1)\n",
    "\n",
    "#model_ex1 = create_model_add_skips(opt1, input_shape1)\n",
    "\n",
    "model_ex1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##### Train the model\n",
    "#covariance1 = Covariance()\n",
    "History1 = History()\n",
    "hist1 = model_ex1.fit(x_train_sim, y_train_sim,\n",
    "                      batch_size=16,\n",
    "                      epochs = 100,\n",
    "                      verbose=1,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[History1],\n",
    "                      validation_data=(x_test_sim, y_test_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Save history\n",
    "History1_loss = History1.history['loss']\n",
    "History1_acc = History1.history['acc']\n",
    "History1_val_loss = History1.history['val_loss']\n",
    "History1_val_acc = History1.history['val_acc']\n",
    "\n",
    "\n",
    "thefile1 = open(outPath + 'History1_loss.txt', 'w')\n",
    "for item in History1_loss:\n",
    "    thefile1.write(\"%s\\n\" % item)\n",
    "thefile1.close()\n",
    "\n",
    "thefile2 = open(outPath + 'History1_acc.txt', 'w')\n",
    "for item in History1_acc:\n",
    "    thefile2.write(\"%s\\n\" % item)\n",
    "thefile2.close()\n",
    "\n",
    "thefile3 = open(outPath + 'History1_val_loss.txt', 'w')\n",
    "for item in History1_val_loss:\n",
    "    thefile3.write(\"%s\\n\" % item)\n",
    "thefile3.close()\n",
    "\n",
    "thefile4 = open(outPath + 'History1_val_acc.txt', 'w')\n",
    "for item in History1_val_acc:\n",
    "    thefile4.write(\"%s\\n\" % item)\n",
    "thefile4.close()\n",
    "\n",
    "### Save model\n",
    "model_json1 = model_ex1.to_json()\n",
    "with open(tmpPath + \"model_ex1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json1)\n",
    "model_ex1.save_weights(tmpPath + \"weights_model_ex1.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot history of average covariance - accuracy and loss of the models\n",
    "plt.figure()\n",
    "plt.plot(History1.history['loss'])\n",
    "plt.plot(History1.history['val_loss'])\n",
    "plt.title('loss & val_loss')\n",
    "plt.legend(['train', 'test'], loc='upper right')\n",
    "plt.savefig(outPath + \"loss\", dpi=1000)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(History1.history['acc'])\n",
    "plt.plot(History1.history['val_acc'])\n",
    "plt.title('acc & val_acc')\n",
    "plt.legend(['train', 'test'], loc='lower right')\n",
    "plt.savefig(outPath + \"acc\", dpi=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def testIndependet(fn, inpath, outpath):\n",
    "    \n",
    "    image_arr = readImg(inpath + fn)\n",
    "    print(image_arr.shape)\n",
    "    \n",
    "    if image_arr.shape[0] % 4 != 0:\n",
    "        n = image_arr.shape[0] % 4\n",
    "        new_x = image_arr.shape[0] - n\n",
    "    else:\n",
    "        new_x = image_arr.shape[0]\n",
    "\n",
    "\n",
    "    if image_arr.shape[1] % 4 != 0:\n",
    "        n = image_arr.shape[1] % 4\n",
    "        new_y = image_arr.shape[1] - n\n",
    "    else:\n",
    "        new_y = image_arr.shape[1]\n",
    "\n",
    "    image_arr = image_arr[:new_x, :new_y]\n",
    "    print(image_arr.shape)\n",
    "    \n",
    "    conc2 = np.reshape(model_ex1.predict(np.reshape(image_arr, (1, image_arr.shape[0], image_arr.shape[1], 1))), \n",
    "                       (image_arr.shape[0], image_arr.shape[1]))\n",
    "    \n",
    "    print(accuracy_score(image_arr.flatten().astype(bool), (conc2 > 0.5).flatten()))\n",
    "    \n",
    "    fig = plt.figure(figsize=(image_arr.shape[1] / 1000, image_arr.shape[0] / 1000), dpi=100, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    \n",
    "    plt.imshow(conc2)\n",
    "    fig.savefig(outpath + fn[:-4] + '_out.png', dpi=1000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    fig = plt.figure(figsize=(image_arr.shape[1] / 1000, image_arr.shape[0] / 1000), dpi=100, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    \n",
    "    conc2 = conc2 > 0.5\n",
    "    plt.imshow(conc2, cmap='gray')\n",
    "    fig.savefig(outpath + fn[:-4] + '_out_bw.png', dpi=1000)\n",
    "    \n",
    "\n",
    "testIndependet(r\"testexampleinput2.tif\", testPath, outPath)\n",
    "testIndependet(r\"FTest1_input_inv.png\", testPath, outPath)\n",
    "testIndependet(r\"FTest2_input_inv.png\", testPath, outPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrA = readImg(testPath + r\"FTest1_input_inv.png\")\n",
    "image_arrB = readImg(testPath + r\"FTest1_output_inv.png\")\n",
    "print(accuracy_score(image_arrB.flatten().astype(bool), \n",
    "                     image_arrA.flatten().astype(bool))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrA = readImg(testPath + r\"FTest2_input_inv.png\")\n",
    "image_arrB = readImg(testPath + r\"FTest2_output_inv.png\")\n",
    "print(accuracy_score(image_arrB.flatten().astype(bool), \n",
    "                     image_arrA.flatten().astype(bool))\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
