{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import keras.backend as K\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class WGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        #self.latent_dim = 100\n",
    "\n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        self.clip_value = 0.01\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build and compile the critic\n",
    "        self.critic = self.build_critic_map()\n",
    "        #self.critic.compile(loss = self.wasserstein_loss,\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "        self.critic.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator_map()\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape = self.img_shape)\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.critic.trainable = False\n",
    "\n",
    "        # The critic takes generated images as input and determines validity\n",
    "        valid = self.critic(img)\n",
    "        \n",
    "        \n",
    "        # The combined model  (stacked generator and critic): \n",
    "        # In this case it just test if the input vector valid and not consider the target\n",
    "        #self.combined = Model(z, valid)\n",
    "        #self.combined.compile(loss=self.wasserstein_loss,\n",
    "        #    optimizer=optimizer,\n",
    "        #    metrics=['accuracy'])\n",
    "        \n",
    "\n",
    "        #loss=['mse', self.wasserstein_loss]\n",
    "        #loss_weights = [99, 1]\n",
    "        \n",
    "        loss=['mse', 'mae']\n",
    "        loss_weights=[100, 1]\n",
    "        \n",
    "        # The combined model  (stacked generator and critic)\n",
    "        self.combined = Model(z, [img, valid])\n",
    "        self.combined.compile(loss=loss, \n",
    "                              loss_weights=loss_weights,\n",
    "                              optimizer=optimizer,\n",
    "                              metrics=['accuracy'])\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator_map(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        # Encoder\n",
    "\n",
    "        # Down sample\n",
    "        model.add(Conv2D(48, kernel_size=(5, 5), strides=(2, 2), padding=\"same\", \n",
    "                         input_shape=self.img_shape))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        # Down sample\n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        # Down sample\n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), strides=(2, 2), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(1024, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(1024, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(1024, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(1024, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(512, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        # Decoder\n",
    "        \n",
    "        # Up sample\n",
    "        model.add(UpSampling2D(size=(2, 2)))\n",
    "        model.add(Conv2D(256, kernel_size=(4, 4), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(256, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        # Up sample\n",
    "        model.add(UpSampling2D(size=(2, 2)))\n",
    "        model.add(Conv2D(128, kernel_size=(4, 4), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(128, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(48, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        # Up sample\n",
    "        model.add(UpSampling2D(size=(2, 2)))\n",
    "        model.add(Conv2D(48, kernel_size=(4, 4), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(24, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Conv2D(1, kernel_size=(3, 3), strides=(1, 1), padding=\"same\"))\n",
    "        model.add(Activation('tanh')) # For range -1 - 1\n",
    "\n",
    "        model.summary()\n",
    "        \n",
    "        X = Input(shape=self.img_shape)\n",
    "        yhat = model(X)\n",
    "\n",
    "        return Model(X, yhat)\n",
    "    \n",
    "    \n",
    "    def build_critic_map(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=5, strides=2, input_shape=self.img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        #model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Dropout(0.25))\n",
    "        \n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        \n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='tanh'))\n",
    "        \n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, data, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        # # Load the dataset\n",
    "        # X_train, y_train, X_test, y_test = data\n",
    "        \n",
    "        # Rescale -1 to 1\n",
    "        X_train, y_train ,X_test, y_test = [tranform(ds) for ds in data]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake = np.ones((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            print(\"Epoch\", epoch)\n",
    "            for _ in range(self.n_critic):\n",
    "            \n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Select a random batch of images\n",
    "                idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "                \n",
    "                batch_X_train = X_train[idx]\n",
    "                batch_y_train = y_train[idx]\n",
    "\n",
    "                # Generate a batch of new images\n",
    "                gen_imgs = self.generator.predict(batch_X_train)\n",
    "\n",
    "                # Train the critic\n",
    "                d_loss_real = self.critic.train_on_batch(batch_y_train, valid)\n",
    "                d_loss_fake = self.critic.train_on_batch(gen_imgs, fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_fake, d_loss_real)\n",
    "\n",
    "                # Clip critic weights\n",
    "                for l in self.critic.layers:\n",
    "                    weights = l.get_weights()\n",
    "                    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                    l.set_weights(weights)\n",
    "\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            #g_loss = self.combined.train_on_batch(batch_X_train, valid) # The case only consider if input is valid\n",
    "            g_loss = self.combined.train_on_batch(batch_X_train, [batch_y_train, valid])      \n",
    "            \n",
    "            # Plot the progress\n",
    "            #print (\"%d [D loss: %f] [D acc: %f] [G loss: %f] [G acc: %f]\" % (epoch, \n",
    "            #                                                                 1 - d_loss[0], d_loss[1], \n",
    "            #                                                                 1 - g_loss[0], g_loss[1]))\n",
    "            \n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, \n",
    "                                                                           d_loss[0], 100*d_loss[1], \n",
    "                                                                           g_loss[0], g_loss[1]))\n",
    "\n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                \n",
    "                idx = [278, 451, 3263, 2654]\n",
    "                img_in, img_out = X_test[idx], y_test[idx]\n",
    "                self.sample_maps(epoch, img_in, img_out)\n",
    "                \n",
    "                \n",
    "    def predict(self, img_in):\n",
    "        \n",
    "        gen_missing = self.generator.predict(img_in)\n",
    "        \n",
    "        return gen_missing\n",
    "    \n",
    "    def sample_maps(self, epoch, img_in, img_out):\n",
    "\n",
    "        r, c = 4, 4\n",
    "        \n",
    "        gen_missing = self.generator.predict(img_in)\n",
    "\n",
    "        imgs = 0.5 * img_in + 0.5\n",
    "        masked_imgs = 0.5 * img_in + 0.5\n",
    "        gen_missing = 0.5 * gen_missing + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "        for i in range(c):\n",
    "            axs[0,i].imshow(np.reshape(img_in[i, :,:], (self.img_rows, self.img_cols)))\n",
    "            axs[0,i].axis('off')\n",
    "            axs[1,i].imshow(np.reshape(img_out[i, :,:], (self.img_rows, self.img_cols)))\n",
    "            axs[1,i].axis('off')\n",
    "            axs[2,i].imshow(np.reshape(gen_missing[i, :,:], (self.img_rows, self.img_cols)))\n",
    "            axs[2,i].axis('off')\n",
    "            axs[3,i].imshow(np.reshape(gen_missing[i, :,:] > 0.2, (self.img_rows, self.img_cols)))\n",
    "            axs[3,i].axis('off')\n",
    "            \n",
    "        fig.suptitle(str(epoch), fontsize=20)\n",
    "        fig.savefig(\"images2/%d.png\" % epoch)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform(data):\n",
    "    return (data - 0.5) * 2\n",
    "\n",
    "def load_dataset():\n",
    "    \n",
    "    scale = 25\n",
    "    p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "    trainPath = r\"../tmp_data/data_feng/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "    x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "    y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "    x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "    y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "    \n",
    "    return x_train_sim, y_train_sim, x_test_sim, y_test_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 16)        416       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 106,497\n",
      "Trainable params: 106,049\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 48)        1248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 64, 64, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 64, 64, 128)       55424     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 16, 16, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)   (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)   (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)   (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_16 (Conv2D)           (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)   (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_15 (Batc (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 16, 16, 512)       4719104   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)   (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_18 (Conv2D)           (None, 16, 16, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_19 (Conv2D)           (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_20 (Conv2D)           (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_21 (Conv2D)           (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 64, 64, 48)        55344     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64, 64, 48)        192       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2 (None, 128, 128, 48)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_25 (Conv2D)           (None, 128, 128, 48)      36912     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128, 128, 48)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 128, 128, 48)      192       \n",
      "_________________________________________________________________\n",
      "conv2d_26 (Conv2D)           (None, 128, 128, 24)      10392     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128, 128, 24)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 128, 128, 24)      96        \n",
      "_________________________________________________________________\n",
      "conv2d_27 (Conv2D)           (None, 128, 128, 1)       217       \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128, 128, 1)       0         \n",
      "=================================================================\n",
      "Total params: 44,859,601\n",
      "Trainable params: 44,844,673\n",
      "Non-trainable params: 14,928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    wgan = WGAN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    wgan.train(data, epochs=4000, batch_size=32, sample_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
