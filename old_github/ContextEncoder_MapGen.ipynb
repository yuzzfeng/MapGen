{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, GaussianNoise\n",
    "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras import losses\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from getModel import getDiscriminator, getGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    def custom_loss(yTrue, yPred):\n",
    "        return 100 * losses.mse(yTrue, yPred) + 0.0001 * sobelSELoss(yTrue, yPred)\n",
    "    \n",
    "    def expandedSobel(inputTensor):\n",
    "        #this contains both X and Y sobel filters in the format (3,3,1,2)\n",
    "        #size is 3 x 3, it considers 1 input channel and has two output channels: X and Y\n",
    "        sobelFilter = K.variable([[[[1.,  1.]], [[0.,  2.]],[[-1.,  1.]]],\n",
    "                              [[[2.,  0.]], [[0.,  0.]],[[-2.,  0.]]],\n",
    "                              [[[1., -1.]], [[0., -2.]],[[-1., -1.]]]])\n",
    "    \n",
    "        #this considers data_format = 'channels_last'\n",
    "        inputChannels = K.reshape(K.ones_like(inputTensor[0,0,0,:]),(1,1,-1,1))\n",
    "        #if you're using 'channels_first', use inputTensor[0,:,0,0] above\n",
    "\n",
    "        return sobelFilter * inputChannels\n",
    "\n",
    "    \n",
    "    def sobelSELoss(yTrue, yPred):\n",
    "\n",
    "        #get the sobel filter repeated for each input channel\n",
    "        filt = expandedSobel(yTrue)\n",
    "\n",
    "        #calculate the sobel filters for yTrue and yPred\n",
    "        #this generates twice the number of input channels \n",
    "        #a X and Y channel for each input channel\n",
    "        sobelTrue = K.depthwise_conv2d(yTrue,filt)\n",
    "        sobelPred = K.depthwise_conv2d(yPred,filt)\n",
    "\n",
    "        #now you just apply the mse:\n",
    "        return K.abs( K.sum(K.abs(sobelTrue)) - K.sum(K.abs(sobelPred)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tranform(data):\n",
    "    return (data - 0.5) * 2\n",
    "\n",
    "class ContextEncoder():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        \n",
    "        self.channels = 1\n",
    "        self.num_classes = 2\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        #optimizer = Adam(0.0002, 0.5) # Original one for minst\n",
    "        optimizer = Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "        #optimizer = RMSprop(lr=0.00005) # Trick from paper of WGAN\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        \n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise as input and generates the missing\n",
    "        # part of the image\n",
    "        masked_img = Input(shape=self.img_shape)\n",
    "        gen_missing = self.generator(masked_img)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated images as input and determines\n",
    "        # if it is generated or if it is a real image\n",
    "        valid = self.discriminator(gen_missing)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model(masked_img , [gen_missing, valid])\n",
    "        \n",
    "        self.combined.compile(loss=[custom_loss, 'binary_crossentropy'], # Original loss from ContextEncoder\n",
    "            loss_weights=[0.99, 0.01],\n",
    "            optimizer=optimizer)\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "        \n",
    "    def build_generator(self):\n",
    "        \n",
    "        model = getGenerator(self.img_shape, out_activation = 'tanh') # For range  -1 - 1\n",
    "        \n",
    "        X = Input(shape=self.img_shape)\n",
    "        yhat = model(X)\n",
    "\n",
    "        return Model(X, yhat)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        \n",
    "        model = getDiscriminator(self.img_shape, out_activation = 'sigmoid') # For range  0 - 1\n",
    "        \n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "    \n",
    "    \n",
    "    def train_map(self, data, epochs, batch_size=128, sample_interval=50):\n",
    "        \n",
    "        (masked_imgs, missing_parts), (X_test, y_test) = data\n",
    "        \n",
    "        masked_imgs, missing_parts,X_test, y_test = [tranform(ds) for ds in [masked_imgs, missing_parts,\n",
    "                                                                             X_test, y_test]]\n",
    "        \n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "        \n",
    "        y_test_bi = np.bool8(y_test > 0)  # -1 - 1\n",
    "        #y_test_bi = np.bool8(y_test > 0.5)  #  0 - 1\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            idx = np.random.randint(0, masked_imgs.shape[0], batch_size)\n",
    "            \n",
    "            batch_masked_imgs = masked_imgs[idx]\n",
    "            batch_missing_parts = missing_parts[idx]\n",
    "            \n",
    "            # Generate a batch of new images\n",
    "            batch_gen_missing = self.generator.predict(batch_masked_imgs)\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch(batch_missing_parts, valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch(batch_gen_missing, fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "            \n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(batch_masked_imgs, [batch_missing_parts, valid])          \n",
    "            \n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:  \n",
    "                \n",
    "                # Plot the progress\n",
    "                print (\"%d [D loss: %f, acc: %.2f%%] [G loss: %f, mse: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss[0], g_loss[1]))\n",
    "\n",
    "                idx = [278, 451, 3263]\n",
    "                #idx = np.random.randint(0, X_test.shape[0], 3)\n",
    "                img_in, img_out = X_test[idx], y_test[idx]\n",
    "                self.sample_maps(epoch, img_in, img_out)\n",
    "                \n",
    "                X_test_pred = self.predict(X_test)\n",
    "                \n",
    "                labels_pred = self.discriminator.predict(X_test_pred)\n",
    "                \n",
    "                print('Generated images who pass discriminator:', sum(labels_pred > 0.5), len(labels_pred))\n",
    "                #print(np.max(X_test_pred), np.min(X_test_pred), np.max(y_test), np.min(y_test))\n",
    "                \n",
    "                X_test_pred_bi = np.bool8(X_test_pred > 0)    # -1 - 1\n",
    "                #X_test_pred_bi = np.bool8(X_test_pred > 0.5) #  0 - 1\n",
    "                \n",
    "                accs = [accuracy_score(np.reshape(j, (128, 128)).flatten(), \n",
    "                                       np.reshape(i, (128, 128)).flatten()) for i, j in zip(X_test_pred_bi, y_test_bi)]\n",
    "                print(np.mean(accs))\n",
    "      \n",
    "    \n",
    "    def predict(self, img_in):\n",
    "        \n",
    "        gen_missing = self.generator.predict(img_in)\n",
    "        \n",
    "        return gen_missing\n",
    "    \n",
    "    def sample_maps(self, epoch, img_in, img_out):\n",
    "\n",
    "        r, c = 3, 3\n",
    "        \n",
    "        gen_missing = self.generator.predict(img_in)\n",
    "\n",
    "        imgs = 0.5 * img_in + 0.5\n",
    "        masked_imgs = 0.5 * img_in + 0.5\n",
    "        gen_missing = 0.5 * gen_missing + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c, figsize=(15,15))\n",
    "        for i in range(c):\n",
    "            axs[0,i].imshow(np.reshape(img_in[i, :,:], (self.img_rows, self.img_cols)))\n",
    "            axs[0,i].axis('off')\n",
    "            axs[1,i].imshow(np.reshape(img_out[i, :,:], (self.img_rows, self.img_cols)))\n",
    "            axs[1,i].axis('off')\n",
    "            axs[2,i].imshow(np.reshape(gen_missing[i, :,:], (self.img_rows, self.img_cols)))\n",
    "            axs[2,i].axis('off')\n",
    "        \n",
    "        fig.savefig(\"images/%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "    def save_model(self):\n",
    "\n",
    "        def save(model, model_name):\n",
    "            model_path = \"saved_model/%s.json\" % model_name\n",
    "            weights_path = \"saved_model/%s_weights.hdf5\" % model_name\n",
    "            options = {\"file_arch\": model_path,\n",
    "                        \"file_weight\": weights_path}\n",
    "            json_string = model.to_json()\n",
    "            open(options['file_arch'], 'w').write(json_string)\n",
    "            model.save_weights(options['file_weight'])\n",
    "\n",
    "        save(self.generator, \"generator\")\n",
    "        save(self.discriminator, \"discriminator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    scale = 25\n",
    "    p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "    trainPath = r\"../tmp_data/data_feng/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "    x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "    y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "    x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "    y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "    \n",
    "    return (x_train_sim, y_train_sim), (x_test_sim, y_test_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    data = load_dataset()\n",
    "    #(_, _), (X_test, y_test) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_171 (Conv2D)          (None, 64, 64, 16)        416       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_129 (LeakyReLU)  (None, 64, 64, 16)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_159 (Bat (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_172 (Conv2D)          (None, 32, 32, 32)        4640      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_130 (LeakyReLU)  (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_160 (Bat (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_173 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_131 (LeakyReLU)  (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_161 (Bat (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_174 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_132 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 8193      \n",
      "=================================================================\n",
      "Total params: 106,561\n",
      "Trainable params: 106,081\n",
      "Non-trainable params: 480\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_175 (Conv2D)          (None, 64, 64, 48)        1248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_133 (LeakyReLU)  (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 64, 64, 48)        192       \n",
      "_________________________________________________________________\n",
      "conv2d_176 (Conv2D)          (None, 64, 64, 128)       55424     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_134 (LeakyReLU)  (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_177 (Conv2D)          (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_135 (LeakyReLU)  (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_165 (Bat (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_178 (Conv2D)          (None, 32, 32, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_136 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_166 (Bat (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_179 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_137 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_167 (Bat (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_138 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_168 (Bat (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_139 (LeakyReLU)  (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_169 (Bat (None, 16, 16, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 16, 16, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_140 (LeakyReLU)  (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_170 (Bat (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_183 (Conv2D)          (None, 16, 16, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_141 (LeakyReLU)  (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_171 (Bat (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_184 (Conv2D)          (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_142 (LeakyReLU)  (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_172 (Bat (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_185 (Conv2D)          (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_143 (LeakyReLU)  (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_173 (Bat (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_186 (Conv2D)          (None, 16, 16, 1024)      9438208   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_144 (LeakyReLU)  (None, 16, 16, 1024)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_174 (Bat (None, 16, 16, 1024)      4096      \n",
      "_________________________________________________________________\n",
      "conv2d_187 (Conv2D)          (None, 16, 16, 512)       4719104   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_145 (LeakyReLU)  (None, 16, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_175 (Bat (None, 16, 16, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv2d_188 (Conv2D)          (None, 16, 16, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_146 (LeakyReLU)  (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_189 (Conv2D)          (None, 32, 32, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "activation_43 (Activation)   (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_176 (Bat (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_190 (Conv2D)          (None, 32, 32, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_147 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_177 (Bat (None, 32, 32, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv2d_191 (Conv2D)          (None, 32, 32, 128)       295040    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_148 (LeakyReLU)  (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_178 (Bat (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_20 (UpSampling (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_192 (Conv2D)          (None, 64, 64, 128)       262272    \n",
      "_________________________________________________________________\n",
      "activation_44 (Activation)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_179 (Bat (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_193 (Conv2D)          (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_45 (Activation)   (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_180 (Bat (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_194 (Conv2D)          (None, 64, 64, 48)        55344     \n",
      "_________________________________________________________________\n",
      "activation_46 (Activation)   (None, 64, 64, 48)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 64, 64, 48)        192       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_21 (UpSampling (None, 128, 128, 48)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_195 (Conv2D)          (None, 128, 128, 48)      36912     \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 128, 128, 48)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_182 (Bat (None, 128, 128, 48)      192       \n",
      "_________________________________________________________________\n",
      "conv2d_196 (Conv2D)          (None, 128, 128, 24)      10392     \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 128, 128, 24)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_183 (Bat (None, 128, 128, 24)      96        \n",
      "_________________________________________________________________\n",
      "conv2d_197 (Conv2D)          (None, 128, 128, 1)       217       \n",
      "_________________________________________________________________\n",
      "activation_49 (Activation)   (None, 128, 128, 1)       0         \n",
      "=================================================================\n",
      "Total params: 44,859,601\n",
      "Trainable params: 44,844,673\n",
      "Non-trainable params: 14,928\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "    context_encoder = ContextEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:479: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 0.991020, acc: 52.34%] [G loss: 530.371155, mse: 535.717163]\n",
      "Generated images who pass discriminator: [3341] 3528\n",
      "0.697589554214\n",
      "50 [D loss: 0.136599, acc: 96.88%] [G loss: 32.007515, mse: 32.268585]\n",
      "Generated images who pass discriminator: [3503] 3528\n",
      "0.925784675442\n",
      "100 [D loss: 0.006517, acc: 100.00%] [G loss: 23.313082, mse: 23.458652]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.930221920922\n",
      "150 [D loss: 0.015011, acc: 100.00%] [G loss: 18.955936, mse: 19.079283]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.927433731875\n",
      "200 [D loss: 0.004585, acc: 100.00%] [G loss: 18.212379, mse: 18.277893]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.936541481624\n",
      "250 [D loss: 0.003642, acc: 100.00%] [G loss: 18.200161, mse: 18.283619]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.948291752614\n",
      "300 [D loss: 0.000304, acc: 100.00%] [G loss: 15.473019, mse: 15.509571]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.956735399034\n",
      "350 [D loss: 0.000199, acc: 100.00%] [G loss: 12.394780, mse: 12.368071]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.964273102429\n",
      "400 [D loss: 0.000379, acc: 100.00%] [G loss: 15.547949, mse: 15.580167]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.959831289694\n",
      "450 [D loss: 0.000333, acc: 100.00%] [G loss: 12.420877, mse: 12.403566]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.953655995479\n",
      "500 [D loss: 0.000016, acc: 100.00%] [G loss: 13.885215, mse: 13.890585]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.958990654978\n",
      "550 [D loss: 0.000005, acc: 100.00%] [G loss: 10.577817, mse: 10.527356]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.964891861896\n",
      "600 [D loss: 0.004130, acc: 100.00%] [G loss: 10.975698, mse: 10.926661]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.958524708305\n",
      "650 [D loss: 0.000019, acc: 100.00%] [G loss: 14.625528, mse: 14.610785]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.962427896167\n",
      "700 [D loss: 0.000123, acc: 100.00%] [G loss: 10.917520, mse: 10.873804]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.969710836605\n",
      "750 [D loss: 0.000465, acc: 100.00%] [G loss: 13.324086, mse: 13.301440]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.966142874997\n",
      "800 [D loss: 0.000070, acc: 100.00%] [G loss: 12.174183, mse: 12.142676]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.963620797847\n",
      "850 [D loss: 0.000005, acc: 100.00%] [G loss: 10.621859, mse: 10.585152]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.968647738433\n",
      "900 [D loss: 0.000054, acc: 100.00%] [G loss: 8.970900, mse: 8.914224]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.973050262382\n",
      "950 [D loss: 0.003191, acc: 100.00%] [G loss: 10.319118, mse: 10.323997]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.974015614343\n",
      "1000 [D loss: 0.000447, acc: 100.00%] [G loss: 8.963162, mse: 8.948557]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.972868783133\n",
      "1050 [D loss: 0.002456, acc: 100.00%] [G loss: 11.155699, mse: 11.124996]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.968450706292\n",
      "1100 [D loss: 0.000102, acc: 100.00%] [G loss: 10.584210, mse: 10.564180]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.974923771796\n",
      "1150 [D loss: 0.000007, acc: 100.00%] [G loss: 7.980451, mse: 7.912964]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.969161468298\n",
      "1200 [D loss: 0.000033, acc: 100.00%] [G loss: 11.843754, mse: 11.801538]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.974023364839\n",
      "1250 [D loss: 0.000057, acc: 100.00%] [G loss: 9.584798, mse: 9.523708]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.974442012726\n",
      "1300 [D loss: 0.000013, acc: 100.00%] [G loss: 7.676610, mse: 7.605729]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.976503904174\n",
      "1350 [D loss: 0.000449, acc: 100.00%] [G loss: 9.135590, mse: 9.069397]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.973568369202\n",
      "1400 [D loss: 0.000001, acc: 100.00%] [G loss: 8.273458, mse: 8.201173]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.973484774566\n",
      "1450 [D loss: 0.000001, acc: 100.00%] [G loss: 9.292310, mse: 9.228696]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.971781274359\n",
      "1500 [D loss: 0.000008, acc: 100.00%] [G loss: 11.191832, mse: 11.152054]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.973408013515\n",
      "1550 [D loss: 0.000000, acc: 100.00%] [G loss: 8.843299, mse: 8.770906]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.975904555548\n",
      "1600 [D loss: 0.000000, acc: 100.00%] [G loss: 7.785367, mse: 7.701586]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.976033822749\n",
      "1650 [D loss: 0.000005, acc: 100.00%] [G loss: 7.869206, mse: 7.785966]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.97338097328\n",
      "1700 [D loss: 0.000022, acc: 100.00%] [G loss: 7.940050, mse: 7.857786]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.976751106937\n",
      "1750 [D loss: 0.000000, acc: 100.00%] [G loss: 9.550000, mse: 9.484989]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.975039354528\n",
      "1800 [D loss: 0.000001, acc: 100.00%] [G loss: 8.863465, mse: 8.790327]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.97625744532\n",
      "1850 [D loss: 0.000013, acc: 100.00%] [G loss: 7.356421, mse: 7.269165]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.976499475319\n",
      "1900 [D loss: 0.000011, acc: 100.00%] [G loss: 9.920549, mse: 9.863981]\n",
      "Generated images who pass discriminator: [3528] 3528\n",
      "0.9777004908\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[64,48,128,128]\n\t [[Node: training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad = FusedBatchNormGrad[T=DT_FLOAT, _class=[\"loc:@model_22/sequential_16/batch_normalization_182/FusedBatchNorm\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/cond/Switch_1_grad/cond_grad, model_22/sequential_16/activation_47/Relu, batch_normalization_182/gamma/read, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:3, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:4)]]\n\nCaused by op 'training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-2ee1cc13c041>\", line 1, in <module>\n    context_encoder.train_map(data, epochs=8000, batch_size=64, sample_interval=50)\n  File \"<ipython-input-35-75c52334717c>\", line 105, in train_map\n    g_loss = self.combined.train_on_batch(batch_masked_imgs, [batch_missing_parts, valid])\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1214, in train_on_batch\n    self._make_train_function()\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 498, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 392, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 89, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 2708, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_grad.py\", line 770, in _FusedBatchNormGrad\n    return _BaseFusedBatchNormGrad(op, False, *grad)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_grad.py\", line 747, in _BaseFusedBatchNormGrad\n    is_training=is_training)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2121, in fused_batch_norm_grad\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'model_22/sequential_16/batch_normalization_182/FusedBatchNorm', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 16 identical lines from previous traceback]\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-b9866e6c2e34>\", line 1, in <module>\n    context_encoder = ContextEncoder()\n  File \"<ipython-input-35-75c52334717c>\", line 31, in __init__\n    gen_missing = self.generator(masked_img)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 457, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 570, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 724, in run_internal_graph\n    output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 570, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 724, in run_internal_graph\n    output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/normalization.py\", line 183, in call\n    epsilon=self.epsilon)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 1842, in normalize_batch_in_training\n    epsilon=epsilon)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 1817, in _fused_normalize_batch_in_training\n    data_format=tf_data_format)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py\", line 831, in fused_batch_norm\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2034, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,48,128,128]\n\t [[Node: training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad = FusedBatchNormGrad[T=DT_FLOAT, _class=[\"loc:@model_22/sequential_16/batch_normalization_182/FusedBatchNorm\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/cond/Switch_1_grad/cond_grad, model_22/sequential_16/activation_47/Relu, batch_normalization_182/gamma/read, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:3, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:4)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,48,128,128]\n\t [[Node: training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad = FusedBatchNormGrad[T=DT_FLOAT, _class=[\"loc:@model_22/sequential_16/batch_normalization_182/FusedBatchNorm\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/cond/Switch_1_grad/cond_grad, model_22/sequential_16/activation_47/Relu, batch_normalization_182/gamma/read, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:3, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:4)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-2ee1cc13c041>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcontext_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-75c52334717c>\u001b[0m in \u001b[0;36mtrain_map\u001b[0;34m(self, data, epochs, batch_size, sample_interval)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;31m# ---------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_masked_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_missing_parts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0;31m# If at save interval => save generated image samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2670\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2671\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2672\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2652\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2654\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2655\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[64,48,128,128]\n\t [[Node: training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad = FusedBatchNormGrad[T=DT_FLOAT, _class=[\"loc:@model_22/sequential_16/batch_normalization_182/FusedBatchNorm\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/cond/Switch_1_grad/cond_grad, model_22/sequential_16/activation_47/Relu, batch_normalization_182/gamma/read, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:3, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:4)]]\n\nCaused by op 'training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/usr/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/usr/local/lib/python3.5/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/usr/local/lib/python3.5/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2856, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-38-2ee1cc13c041>\", line 1, in <module>\n    context_encoder.train_map(data, epochs=8000, batch_size=64, sample_interval=50)\n  File \"<ipython-input-35-75c52334717c>\", line 105, in train_map\n    g_loss = self.combined.train_on_batch(batch_masked_imgs, [batch_missing_parts, valid])\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 1214, in train_on_batch\n    self._make_train_function()\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\", line 498, in _make_train_function\n    loss=self.total_loss)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/legacy/interfaces.py\", line 91, in wrapper\n    return func(*args, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 392, in get_updates\n    grads = self.get_gradients(loss, params)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/optimizers.py\", line 89, in get_gradients\n    grads = K.gradients(loss, params)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 2708, in gradients\n    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in gradients\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 353, in _MaybeCompile\n    return grad_fn()  # Exit early\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gradients_impl.py\", line 581, in <lambda>\n    grad_scope, op, func_call, lambda: grad_fn(op, *out_grads))\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_grad.py\", line 770, in _FusedBatchNormGrad\n    return _BaseFusedBatchNormGrad(op, False, *grad)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_grad.py\", line 747, in _BaseFusedBatchNormGrad\n    is_training=is_training)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2121, in fused_batch_norm_grad\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\n...which was originally created as op 'model_22/sequential_16/batch_normalization_182/FusedBatchNorm', defined at:\n  File \"/usr/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n[elided 16 identical lines from previous traceback]\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/usr/local/lib/python3.5/dist-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-37-b9866e6c2e34>\", line 1, in <module>\n    context_encoder = ContextEncoder()\n  File \"<ipython-input-35-75c52334717c>\", line 31, in __init__\n    gen_missing = self.generator(masked_img)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\", line 457, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 570, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 724, in run_internal_graph\n    output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 570, in call\n    output_tensors, _, _ = self.run_internal_graph(inputs, masks)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\", line 724, in run_internal_graph\n    output_tensors = to_list(layer.call(computed_tensor, **kwargs))\n  File \"/usr/local/lib/python3.5/dist-packages/keras/layers/normalization.py\", line 183, in call\n    epsilon=self.epsilon)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 1842, in normalize_batch_in_training\n    epsilon=epsilon)\n  File \"/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\", line 1817, in _fused_normalize_batch_in_training\n    data_format=tf_data_format)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/nn_impl.py\", line 831, in fused_batch_norm\n    name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2034, in _fused_batch_norm\n    is_training=is_training, name=name)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 2956, in create_op\n    op_def=op_def)\n  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 1470, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[64,48,128,128]\n\t [[Node: training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/FusedBatchNorm_grad/FusedBatchNormGrad = FusedBatchNormGrad[T=DT_FLOAT, _class=[\"loc:@model_22/sequential_16/batch_normalization_182/FusedBatchNorm\"], data_format=\"NHWC\", epsilon=0.001, is_training=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](training_10/Adadelta/gradients/model_22/sequential_16/batch_normalization_182/cond/Switch_1_grad/cond_grad, model_22/sequential_16/activation_47/Relu, batch_normalization_182/gamma/read, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:3, model_22/sequential_16/batch_normalization_182/FusedBatchNorm:4)]]\n"
     ]
    }
   ],
   "source": [
    "    context_encoder.train_map(data, epochs=8000, batch_size=64, sample_interval=50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    context_encoder.save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    (_, _), (X_test, y_test) = data\n",
    "    idx = [278, 451, 2354]\n",
    "    img_in, img_out = X_test[idx], y_test[idx]\n",
    "    context_encoder.sample_maps(50000, img_in, img_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, _), (X_test, y_test) = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_pred = context_encoder.predict(X_test)\n",
    "y_test_bi = np.bool8(y_test > 0)  # -1 - 1\n",
    "X_test_pred_bi = np.bool8(X_test_pred > 0)\n",
    "\n",
    "X_test_pred_bi = np.reshape(X_test_pred_bi, (-1, 128, 128))\n",
    "y_test_bi = np.reshape(y_test_bi, (-1, 128, 128))\n",
    "accs = [accuracy_score(i.flatten(), j.flatten()) for i, j in zip(X_test_pred_bi, y_test_bi)]\n",
    "print(np.mean(accs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
