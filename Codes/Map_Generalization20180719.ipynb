{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## Map Generalization for Polygons using Autoencode-like strucutures\n",
    "## Adatped based on Master Thesis of SERCAN CAKIR \"ROAD NETWORK EXTRACTION USING CNN\"\n",
    "## Author: Yu Feng, yuzz.feng@gmail.com\n",
    "## 1. Version Author: SERCAN CAKIR\n",
    "\n",
    "## Changes:\n",
    "## 1. Two conv layers were added before the first down convlusional layer\n",
    "## 2. Output can be any size during the evaluation\n",
    "## 3. Adapt the code to support more images as training examples\n",
    "## 4. Dropouot may make the sharpe corners vanishing, we delete half of them, but we should used some\n",
    "## 5. \n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg') # necessary for linux kernal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "np.random.seed(7)\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import History\n",
    "from keras.layers.core import Dropout\n",
    "#from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.layers import MaxPooling2D, Conv2DTranspose, BatchNormalization, Activation\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, Dropout, UpSampling2D, Activation, Concatenate\n",
    "\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create\n",
    "\n",
    "from time import gmtime, strftime\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "\n",
    "def prediction_independent(model_ex1, image_arr):\n",
    "    \n",
    "    conc2 = np.reshape(model_ex1.predict(np.reshape(image_arr, (1, image_arr.shape[0], image_arr.shape[1], 1))), \n",
    "                   (image_arr.shape[0], image_arr.shape[1]))\n",
    "    return conc2\n",
    "\n",
    "\n",
    "############ Path Setting ##############\n",
    "\n",
    "trainPath = r\"Data/Training_Validation/\"\n",
    "\n",
    "trainPath = r\"Data/geb10/\"\n",
    "\n",
    "\n",
    "testPath = r\"Data/Testing/\"\n",
    "tmpPath = r\"../tmp_data/\"\n",
    "\n",
    "outPath = r\"Prediction/\"\n",
    "check_and_create(outPath + timestr)\n",
    "outPath = outPath + timestr + \"/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "_EPSILON = 10e-8\n",
    "\n",
    "def IoU(yTrue,yPred):  \n",
    "    \n",
    "    I = tf.multiply(yTrue, yPred, name=\"intersection\")\n",
    "    U = yTrue + yPred - I + _EPSILON\n",
    "    \n",
    "    IoU = tf.reduce_sum(I) / tf.reduce_sum(U)\n",
    "    return -tf.log(IoU + _EPSILON) + binary_crossentropy(yTrue,yPred)\n",
    "    \n",
    "    #IoU = tf.divide(I, U, name='IoU')\n",
    "    #L = -tf.log(IoU + _EPSILON)\n",
    "    #return tf.reduce_mean(L)\n",
    "    \n",
    "    \n",
    "def MSE_CROSS(yTrue,yPred):\n",
    "    return binary_crossentropy(yTrue,yPred) + K.abs(K.sum(yTrue) - K.sum(yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "# set the working directory\n",
    "#os.chdir(r'F:\\sercan\\input_images')\n",
    "PATH = os.getcwd()\n",
    "#plt.gray()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def data_collector(fns_input, fns_output):\n",
    "    \n",
    "    sim_input = []\n",
    "    sim_output = []\n",
    "    \n",
    "    for fn_input, fn_output in zip(fns_input, fns_output):\n",
    "        \n",
    "        # load simulated heat map (TRAJECTORY SIMULATION) and target road for Hannover ####\n",
    "        sim_heatmap_hannover = readImg(fn_input)\n",
    "        sim_road_hannover = readImg(fn_output)\n",
    "        \n",
    "        sim_hm_patches_overlap = imagePatches(sim_heatmap_hannover, p_size_1, p_size_1, int(p_size_1))\n",
    "        sim_road_patches_overlap = imagePatches(sim_road_hannover, p_size_1, p_size_1, int(p_size_1))\n",
    "        sim_road_patches_overlap_new = removeCorrespondence(sim_road_patches_overlap, sim_hm_patches_overlap)\n",
    "        sim_hm_patches_overlap_new = removeCorrespondence(sim_hm_patches_overlap, sim_road_patches_overlap)\n",
    "        sim_road_patches_overlap_new_new = removeBlackImg(sim_road_patches_overlap)\n",
    "        \n",
    "        sim_input += sim_hm_patches_overlap_new\n",
    "        sim_output += sim_road_patches_overlap_new_new\n",
    "    \n",
    "    return sim_input, sim_output\n",
    "\n",
    "#fns_input = [trainPath + r\"traininput_inv.png\"]\n",
    "#fns_output = [trainPath + r\"trainoutput_inv.png\"]\n",
    "\n",
    "fns_input = [trainPath + r\"geb.png\"]\n",
    "fns_output = [trainPath + r\"geb10.png\"]\n",
    "\n",
    "#fns_input = [r\"data/input2.tif\"]#, r\"data/geb1_inp_inv_cut.tif\"]\n",
    "#fns_output = [r\"data/output2.tif\"]#, r\"data/geb1_out_inv_cut.tif\"]\n",
    "\n",
    "sim_hm_patches_32_new, sim_road_patches_32_new_new = data_collector(fns_input, fns_output)\n",
    "print('Number of tiles: ', len(sim_hm_patches_32_new))\n",
    "\n",
    "#### experience 1 - simulated hm\n",
    "index_list_sim = list(range(len(sim_hm_patches_32_new)))\n",
    "random.shuffle(index_list_sim)\n",
    "\n",
    "idx_sim = 1000\n",
    "index_list_test_sim = index_list_sim[-idx_sim:]\n",
    "index_list_test_sim.sort()\n",
    "sim_hm_test = [sim_hm_patches_32_new[i] for i in index_list_test_sim]\n",
    "sim_road_test = [sim_road_patches_32_new_new[i] for i in index_list_test_sim]\n",
    "\n",
    "index_list_train_sim = index_list_sim[:-idx_sim]\n",
    "index_list_train_sim.sort()\n",
    "sim_hm_train = [sim_hm_patches_32_new[i] for i in index_list_train_sim]\n",
    "sim_road_train = [sim_road_patches_32_new_new[i] for i in index_list_train_sim]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sim_hm_train, sim_hm_test, sim_road_train, sim_road_test = train_test_split(sim_hm_patches_32_new, \n",
    "#                                                                            sim_road_patches_32_new_new,\n",
    "#                                                                            test_size=0.33, random_state=42)\n",
    "\n",
    "print(len(sim_hm_train), len(sim_hm_test), len(sim_road_train), len(sim_road_test))\n",
    "\n",
    "x_train_sim = np.reshape(sim_hm_train, (len(sim_hm_train), p_size_1, p_size_1, 1))\n",
    "y_train_sim = np.reshape(sim_road_train, (len(sim_road_train), p_size_1, p_size_1, 1))\n",
    "x_test_sim = np.reshape(sim_hm_test, (len(sim_hm_test), p_size_1, p_size_1, 1))\n",
    "y_test_sim = np.reshape(sim_road_test, (len(sim_road_test), p_size_1, p_size_1, 1))\n",
    "\n",
    "# save image patch arrays\n",
    "np.save(tmpPath + \"x_train_sim.npy\", x_train_sim)\n",
    "np.save(tmpPath + \"y_train_sim.npy\", y_train_sim)\n",
    "np.save(tmpPath + \"x_test_sim.npy\", x_test_sim)\n",
    "np.save(tmpPath + \"y_test_sim.npy\", y_test_sim)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(x_test_sim[2], (p_size_1,p_size_1)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(y_test_sim[2], (p_size_1,p_size_1)))\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the models', x_train_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image patch arrays\n",
    "x_train_sim = np.load(tmpPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(tmpPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(tmpPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(tmpPath + \"y_test_sim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the models (35083, 128, 128, 1)\n",
      "Tet Shape of the models (1000, 128, 128, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7244577c88>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f73604a7080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.reshape(x_test_sim[2], (p_size_1,p_size_1)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(y_test_sim[2], (p_size_1,p_size_1)))\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the models', x_train_sim.shape)\n",
    "print('Tet Shape of the models', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, None, None, 1 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_a (Conv2D)            (None, None, None, 2 240         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_b (Conv2D)            (None, None, None, 2 5208        flat_conv_a[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "down_conv_1 (Conv2D)            (None, None, None, 2 5208        flat_conv_b[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_1 (Conv2D)            (None, None, None, 6 13888       down_conv_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_2 (Conv2D)            (None, None, None, 6 36928       flat_conv_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "down_conv_2 (Conv2D)            (None, None, None, 6 36928       flat_conv_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_3 (Conv2D)            (None, None, None, 1 73856       down_conv_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_4 (Conv2D)            (None, None, None, 1 147584      flat_conv_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "xx_conv_3 (Conv2D)              (None, None, None, 1 147584      flat_conv_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_5 (Conv2D)            (None, None, None, 2 295168      xx_conv_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_6 (Conv2D)            (None, None, None, 2 590080      flat_conv_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_7 (Conv2D)            (None, None, None, 2 590080      flat_conv_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, None, 2 0           flat_conv_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "xx_conv_0 (Conv2D)              (None, None, None, 1 295040      dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_8 (Conv2D)            (None, None, None, 1 147584      xx_conv_0[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_8b (Conv2D)           (None, None, None, 1 147584      flat_conv_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "up_samp_1 (UpSampling2D)        (None, None, None, 1 0           flat_conv_8b[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_conv_1 (Conv2D)              (None, None, None, 6 131136      up_samp_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 1 0           flat_conv_2[0][0]                \n",
      "                                                                 up_conv_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_9 (Conv2D)            (None, None, None, 6 8256        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_10 (Conv2D)           (None, None, None, 6 36928       flat_conv_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "up_samp_2 (UpSampling2D)        (None, None, None, 6 0           flat_conv_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "up_conv_2 (Conv2D)              (None, None, None, 2 24600       up_samp_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           flat_conv_b[0][0]                \n",
      "                                                                 up_conv_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_11 (Conv2D)           (None, None, None, 1 588         concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_11b (Conv2D)          (None, None, None, 1 1308        flat_conv_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flat_conv_12 (Conv2D)           (None, None, None, 1 109         flat_conv_11b[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 2,735,885\n",
      "Trainable params: 2,735,885\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "opt1 = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "#opt1 = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "#model_ex1 = create_model(opt1, input_shape1)\n",
    "\n",
    "#model_ex1 = create_model_batch(opt1, input_shape1) # Not good as the skip connection\n",
    "\n",
    "#model_ex1 = create_model_add_skips(opt1, input_shape1)\n",
    "\n",
    "model_ex1 = create_model_add_skips_2(opt1, input_shape1)\n",
    "\n",
    "#model_ex1 = create_model_add_skips_3(opt1, input_shape1) # Add max pool but not better\n",
    "\n",
    "model_ex1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5000/5000 [==============================] - 407s 81ms/step - loss: 0.0365 - acc: 0.9693 - val_loss: 0.0252 - val_acc: 0.9940\n",
      "Epoch 2/50\n",
      "5000/5000 [==============================] - 484s 97ms/step - loss: 0.0318 - acc: 0.9705 - val_loss: 0.0238 - val_acc: 0.9941\n",
      "Epoch 3/50\n",
      "5000/5000 [==============================] - 471s 94ms/step - loss: 0.0308 - acc: 0.9705 - val_loss: 0.0237 - val_acc: 0.9942\n",
      "Epoch 4/50\n",
      "3785/5000 [=====================>........] - ETA: 1:48 - loss: 0.0304 - acc: 0.9705"
     ]
    }
   ],
   "source": [
    "data_gen_args = dict(rotation_range=180.)\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "BATCH_SIZE = 16\n",
    "result_generator = zip(image_datagen.flow(x_train_sim, batch_size=BATCH_SIZE, seed=seed), \n",
    "                       mask_datagen.flow(y_train_sim, batch_size=BATCH_SIZE, seed=seed))\n",
    "\n",
    "History1 = History()\n",
    "hist1 = model_ex1.fit_generator(  result_generator,\n",
    "                                  epochs = 50,\n",
    "                                  steps_per_epoch=5000,\n",
    "                                  verbose=1,\n",
    "                                  shuffle=True,\n",
    "                                  callbacks=[History1, \n",
    "                                             EarlyStopping(patience=10), \n",
    "                                             ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                             ModelCheckpoint(outPath + \"weights.hdf5\", \n",
    "                                                             save_best_only = True, \n",
    "                                                             save_weights_only = False)],\n",
    "                                  validation_data=(x_test_sim, y_test_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "##### Train the model with out generator\n",
    "History1 = History()\n",
    "hist1 = model_ex1.fit(x_train_sim, y_train_sim,\n",
    "                      batch_size=16,\n",
    "                      epochs = 50,\n",
    "                      verbose=1,\n",
    "                      shuffle=True,\n",
    "                      callbacks=[History1, \n",
    "                                 EarlyStopping(patience = 10), \n",
    "                                 ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                 ModelCheckpoint(outPath + \"weights.hdf5\", \n",
    "                                                 save_best_only = True, \n",
    "                                                 save_weights_only = False)],\n",
    "                      validation_data=(x_test_sim, y_test_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "\n",
    "### Save history\n",
    "save_hist(History1, outPath)\n",
    "### Save model\n",
    "#save_model(model_ex1, outPath)\n",
    "# Retain best model\n",
    "#from keras import models\n",
    "#model_ex1 = models.load_model(outPath + \"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records = []\n",
    "\n",
    "records = predict_15k(outPath, testPath, outPath, \n",
    "                      r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\")\n",
    "all_records.extend(records)\n",
    "\n",
    "records = predict_15k(outPath, testPath, outPath, \n",
    "                      r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\")\n",
    "all_records.extend(records)\n",
    "\n",
    "df = pd.DataFrame(np.transpose(all_records))\n",
    "df.columns = [\"Input vs Target (Test1)\", \"Prediction vs Target (Test1)\", \n",
    "              \"Input vs Target (Test2)\", \"Prediction vs Target (Test2)\"]\n",
    "\n",
    "df = df.rename({0: \"Accuracy\", 1: 'IoU'})\n",
    "df.index.name = 'Metrics'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
