{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.applications import VGG19\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapgenlib.losses_extend import dice_coef_loss, iou_loss, wasserstein_loss\n",
    "from mapgenlib.res_unit import res_block, decoder, encoder, build_res_unet\n",
    "from mapgenlib.discrimnator import build_discriminator_simple_dc, build_discriminator_critic\n",
    "from mapgenlib.discrimnator import build_discriminator_patchgan_srgan, build_discriminator_patchgan_cycle\n",
    "\n",
    "from functools import partial\n",
    "from keras.layers.merge import _Merge\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    \"\"\"Citation: https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py#L110\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN4MapGen(): # Based on u-net, residual u-net and pix2pix\n",
    "    # Reference: https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "    \n",
    "    def __init__(self, method, batch_size=16):\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        #self.clip_value = 0.01\n",
    "\n",
    "        ## Calculate output shape of D (PatchGAN) better version\n",
    "        #self.patch_size = 32\n",
    "        #self.nb_patches = int((self.img_rows / self.patch_size) * (self.img_cols / self.patch_size))\n",
    "        #self.patch_gan_dim = (self.patch_size, self.patch_size, self.channels)\n",
    "        \n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.method = method\n",
    "        \n",
    "        if method == \"dc\":\n",
    "            self.initial_dc()\n",
    "            self.train_cell = self.train_cell_dc\n",
    "            self.valid_cell = self.valid_cell_dc\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            self.valid = np.ones((batch_size, 1))\n",
    "            self.fake = np.zeros((batch_size, 1))\n",
    "        \"\"\"\n",
    "        if method == \"wgangp\":\n",
    "            self.initial_wgangp()\n",
    "            self.train_cell = self.train_cell_wgangp\n",
    "            self.valid_cell = self.valid_cell_wgangp\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            self.valid = -np.ones((batch_size, 1))\n",
    "            self.fake =  np.ones((batch_size, 1))\n",
    "        \n",
    "        if method == \"srgan\":\n",
    "            # Calculate output shape of D (PatchGAN)\n",
    "            patch = int(self.img_rows / 2**4)\n",
    "            self.disc_patch = (patch, patch, 1)\n",
    "            \n",
    "            self.initial_srgan()\n",
    "            self.train_cell = self.train_cell_srgan\n",
    "            self.valid_cell = self.valid_cell_srgan\n",
    "            \n",
    "            self.valid = np.ones((batch_size,) + self.disc_patch)\n",
    "            self.fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        if method == \"p2p\":\n",
    "            # Calculate output shape of D (PatchGAN)\n",
    "            patch = int(self.img_rows / 2**4)\n",
    "            self.disc_patch = (patch, patch, 1)\n",
    "            \n",
    "            self.initial_p2p()\n",
    "            self.train_cell = self.train_cell_p2p\n",
    "            self.valid_cell = self.valid_cell_p2p\n",
    "            \n",
    "            self.valid = np.ones((batch_size,) + self.disc_patch)\n",
    "            self.fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        #optimizer = Adam(0.0002, 0.5) # Original\n",
    "        #optimizer = Adam(0.0001, 0.5) # Original # Latest achieved by 0.00008\n",
    "        #optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # An old version of Pix2pix\n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator() # Old generator from \n",
    "        self.generator = self.build_res_unet_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        #img_A = Input(shape=self.img_shape) # Target\n",
    "        img_B = Input(shape=self.img_shape) # Input\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_simple_dc(self.img_shape)\n",
    "        \n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        #valid = self.discriminator([fake_A, img_B])\n",
    "        #self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        \n",
    "        valid = self.discriminator(fake_A)\n",
    "        self.combined = Model(inputs= img_B, outputs=[valid, fake_A])\n",
    "        \n",
    "        # Original Pix2Pix - low weight for discriminator\n",
    "        self.combined.compile(loss=['mse', 'mae'], #['mse', 'mae'] original\n",
    "                              loss_weights=[1, 100], # [1, 100] original\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" WGAN_GP \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "    \n",
    "        #Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "    \n",
    "    def initial_wgangp(self):\n",
    "        \n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        self.critic = build_discriminator_critic(self.img_shape)\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "        \n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "        \n",
    "        # Input images and generate imgs\n",
    "        src_img = Input(shape=self.img_shape)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "        fake_img = self.generator(src_img)\n",
    "        \n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "        \n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "        \n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss, averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, src_img], outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic_model.compile(loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Generate images based of noise\n",
    "        fake_img = self.generator(src_img)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(fake_img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(src_img, [valid, fake_img])\n",
    "        self.generator_model.compile(loss=[wasserstein_loss, \"mse\"], optimizer=optimizer)\n",
    "        \n",
    "    def train_cell_wgangp(self, imgs_A, imgs_B, valid, fake):\n",
    "        \n",
    "        dummy = np.zeros((self.batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        \n",
    "        for _ in range(self.n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            # Train the critic\n",
    "            d_loss = self.critic_model.train_on_batch([imgs_A, imgs_B], [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        g_loss = self.generator_model.train_on_batch(imgs_B, valid)\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_wgangp(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,1))\n",
    "        t_loss = self.generator_model.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "        return t_loss\n",
    "    WGAN_GP ends\"\"\" \n",
    "    \n",
    "    \n",
    "    \"\"\" Pix2Pix \n",
    "    def initial_p2p(self):\n",
    "        \n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_patchgan_cycle(self.img_shape, self.df)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Input images and generate imgs\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake_A)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs=img_B, outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 100], \n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "        #self.combined.compile(loss=['mse', 'mse'], loss_weights=[1, 100], \n",
    "        #                      optimizer=optimizer, metrics=['accuracy']) # Very bad try!!!\n",
    "        \n",
    "    def train_cell_p2p(self, imgs_A, imgs_B, valid, fake):\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "        d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train the generators\n",
    "        g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_p2p(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,) + self.disc_patch)\n",
    "        t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "        return t_loss\n",
    "    Pix2Pix ends\"\"\" \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" SRGAN \n",
    "    def initial_srgan(self):\n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.vgg = self.build_vgg()\n",
    "        self.vgg.trainable = False\n",
    "        self.vgg.compile(loss='mse',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_patchgan_srgan(self.img_shape, self.df)\n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        self.generator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Input images and generate imgs\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        \n",
    "        # Generate image\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Extract image features of the generated img\n",
    "        fake_features = self.vgg(fake_A)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake_A)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs=[img_B, img_A], outputs=[valid, fake_features])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'], \n",
    "                              loss_weights=[1, 10], optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    def train_cell_srgan(self, imgs_A, imgs_B, valid, fake):\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "        d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        valid = np.ones((self.batch_size,) + self.disc_patch)\n",
    "        \n",
    "        image_features = self.vgg.predict(imgs_A)\n",
    "        \n",
    "        # Train the generators\n",
    "        g_loss = self.combined.train_on_batch([imgs_B, imgs_A], [valid, image_features])\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_srgan(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,) + self.disc_patch)\n",
    "        t_loss = self.generator.evaluate(x_test_sim, y_test_sim, verbose=0)\n",
    "        return t_loss\n",
    "     SRGAN ends\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" DCGAN \"\"\"\n",
    "    def initial_dc(self):\n",
    "        \n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_simple_dc(self.img_shape)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Input images and generate imgs\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake_A)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs=img_B, outputs=[valid, fake_A])\n",
    "        #self.combined.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 100], optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        #20190314 15:20\n",
    "        self.combined.compile(loss=['mse', 'mae'], loss_weights=[1, 100], optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    def train_cell_dc(self, imgs_A, imgs_B, valid, fake):\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "        d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train the generators\n",
    "        g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_dc(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,1))\n",
    "        t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "        return t_loss\n",
    "    \"\"\" DC ends\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, sample_interval=50, patience = 5):\n",
    "    \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        valid = self.valid\n",
    "        fake  = self.fake\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / self.batch_size)\n",
    "        \n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        valid_acc = []\n",
    "        valid_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, self.batch_size)):\n",
    "            \n",
    "                d_loss, g_loss = self.train_cell(imgs_A, imgs_B, valid, fake)\n",
    "            \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.train_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss)\n",
    "                    print(g_loss)\n",
    "            \n",
    "            if epoch >= 0:\n",
    "                t_loss = self.valid_cell(x_test_sim, y_test_sim)\n",
    "                \n",
    "                if self.method == \"srgan\":\n",
    "                    self.valid_log_sr(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss)\n",
    "                    \n",
    "                    # Change to this if srgan\n",
    "                    valid_loss.append(t_loss[0])\n",
    "                    valid_acc.append(t_loss[1])\n",
    "                else:\n",
    "                    self.valid_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss)\n",
    "                    \n",
    "                    valid_loss.append(t_loss[2])\n",
    "                    valid_acc.append(t_loss[4])\n",
    "                \n",
    "                train_loss.append(g_loss[2])\n",
    "                train_acc.append(g_loss[4])\n",
    "                \n",
    "                waited = len(valid_loss) - 1 - np.argmin(valid_loss)\n",
    "                print('waited for', waited, valid_loss)\n",
    "                    \n",
    "                if waited == 0:\n",
    "                    self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')   \n",
    "                        \n",
    "                if waited > patience:\n",
    "                    break\n",
    "            \n",
    "        return train_acc, train_loss, valid_acc, valid_loss\n",
    "    \n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        vgg = VGG19(weights=\"imagenet\")\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        imgc = Concatenate()([img, img, img]) \n",
    "\n",
    "        # Extract image features\n",
    "        img_features = vgg(imgc)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "\n",
    "    \n",
    "    def train_log(self, start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss):\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                elapsed_time))    \n",
    "    \n",
    "    def valid_log_sr(self, start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss):\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                elapsed_time))   \n",
    "        print(t_loss)\n",
    "    \n",
    "    def valid_log(self, start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss):\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] [Test loss&acc: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                100*t_loss[2], 100*t_loss[3], 100*t_loss[4],\n",
    "                                                                                elapsed_time))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"p2p\", batch_size = 16)\n",
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the trains (32289, 128, 128, 1)\n",
      "Input Shape of the tests (3587, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "scale = 15\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng_/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the trains', x_train_sim.shape)\n",
    "print('Input Shape of the tests', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  \n",
    "\n",
    "def save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath):    \n",
    "    ### Save history\n",
    "    History1_loss = train_loss\n",
    "    History1_acc = train_acc\n",
    "    History1_val_loss = valid_loss\n",
    "    History1_val_acc = valid_acc\n",
    "\n",
    "    thefile1 = open(outPath + 'History1_loss.txt', 'w')\n",
    "    for item in History1_loss:\n",
    "        thefile1.write(\"%s\\n\" % item)\n",
    "    thefile1.close()\n",
    "\n",
    "    thefile2 = open(outPath + 'History1_acc.txt', 'w')\n",
    "    for item in History1_acc:\n",
    "        thefile2.write(\"%s\\n\" % item)\n",
    "    thefile2.close()\n",
    "\n",
    "    thefile3 = open(outPath + 'History1_val_loss.txt', 'w')\n",
    "    for item in History1_val_loss:\n",
    "        thefile3.write(\"%s\\n\" % item)\n",
    "    thefile3.close()\n",
    "\n",
    "    thefile4 = open(outPath + 'History1_val_acc.txt', 'w')\n",
    "    for item in History1_val_acc:\n",
    "        thefile4.write(\"%s\\n\" % item)\n",
    "    thefile4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residual unet + Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp_results/predictions/U128GAN_2019-03-19 18-33-51_15/\n"
     ]
    }
   ],
   "source": [
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "timestr = strftime(\"U\"+str(p_size_1)+\"GAN_%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)\n",
    "print(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/tmp/MapGen/mapgenlib/res_unit.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "  return Model(input=inputs, output=path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 33, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 17, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73984)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 73985     \n",
      "=================================================================\n",
      "Total params: 463,617\n",
      "Trainable params: 462,721\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50-0/2018] [D loss&acc: 3.319, 6.250%] [G loss&accA&accB: 57.922, 100.000%, 31.525%] time: 0:00:11.883887\n",
      "[57.93411, 0.012103392, 0.57922006, 1.0, 0.3152542]\n",
      "[Epoch 0/50-500/2018] [D loss&acc: 1.021, 31.250%] [G loss&accA&accB: 1.518, 68.750%, 98.480%] time: 0:01:49.505618\n",
      "[1.7664652, 0.24855304, 0.015179121, 0.6875, 0.98480225]\n",
      "[Epoch 0/50-1000/2018] [D loss&acc: 0.730, 53.125%] [G loss&accA&accB: 1.205, 25.000%, 98.796%] time: 0:03:26.215028\n",
      "[1.5894856, 0.38459238, 0.012048933, 0.25, 0.987957]\n",
      "[Epoch 0/50-1500/2018] [D loss&acc: 0.873, 37.500%] [G loss&accA&accB: 1.459, 62.500%, 98.542%] time: 0:05:02.385781\n",
      "[1.6791234, 0.22027695, 0.014588465, 0.625, 0.98542404]\n",
      "[Epoch 0/50-2000/2018] [D loss&acc: 0.934, 43.750%] [G loss&accA&accB: 0.816, 43.750%, 99.184%] time: 0:06:38.197346\n",
      "[1.1410089, 0.32497376, 0.008160352, 0.4375, 0.9918442]\n",
      "[Epoch 0/50-2016/2018] [D loss&acc: 0.841, 37.500%] [G loss&accA&accB: 0.974, 50.000%, 99.026%] [Test loss&acc: 1.005, 64.065%, 98.996%] time: 0:06:52.710204\n",
      "waited for 0 [0.01004717296214241]\n",
      "[Epoch 1/50-0/2018] [D loss&acc: 0.965, 40.625%] [G loss&accA&accB: 0.927, 43.750%, 99.074%] time: 0:06:57.434033\n",
      "[1.2752445, 0.34820583, 0.009270387, 0.4375, 0.9907417]\n",
      "[Epoch 1/50-500/2018] [D loss&acc: 0.812, 40.625%] [G loss&accA&accB: 1.272, 50.000%, 98.729%] time: 0:08:33.020194\n",
      "[1.5458133, 0.27374095, 0.012720725, 0.5, 0.9872894]\n",
      "[Epoch 1/50-1000/2018] [D loss&acc: 0.710, 62.500%] [G loss&accA&accB: 0.652, 56.250%, 99.347%] time: 0:10:08.482750\n",
      "[0.8989332, 0.24673305, 0.006522001, 0.5625, 0.99347305]\n",
      "[Epoch 1/50-1500/2018] [D loss&acc: 0.777, 53.125%] [G loss&accA&accB: 0.831, 43.750%, 99.170%] time: 0:11:43.905762\n",
      "[1.0930927, 0.26181164, 0.00831281, 0.4375, 0.9916954]\n",
      "[Epoch 1/50-2000/2018] [D loss&acc: 0.828, 40.625%] [G loss&accA&accB: 0.952, 43.750%, 99.049%] time: 0:13:19.052910\n",
      "[1.2369303, 0.2854242, 0.00951506, 0.4375, 0.9904938]\n",
      "[Epoch 1/50-2016/2018] [D loss&acc: 0.739, 37.500%] [G loss&accA&accB: 0.786, 31.250%, 99.214%] [Test loss&acc: 0.977, 67.438%, 99.024%] time: 0:13:32.099244\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568]\n",
      "[Epoch 2/50-0/2018] [D loss&acc: 0.831, 50.000%] [G loss&accA&accB: 0.794, 62.500%, 99.208%] time: 0:13:32.676476\n",
      "[1.0080522, 0.21381769, 0.007942345, 0.625, 0.9920769]\n",
      "[Epoch 2/50-500/2018] [D loss&acc: 0.853, 50.000%] [G loss&accA&accB: 0.672, 43.750%, 99.328%] time: 0:15:07.874067\n",
      "[0.9263805, 0.25419182, 0.006721887, 0.4375, 0.9932823]\n",
      "[Epoch 2/50-1000/2018] [D loss&acc: 0.846, 46.875%] [G loss&accA&accB: 0.841, 56.250%, 99.160%] time: 0:16:42.825463\n",
      "[1.0925478, 0.2517166, 0.008408312, 0.5625, 0.99160004]\n",
      "[Epoch 2/50-1500/2018] [D loss&acc: 0.821, 31.250%] [G loss&accA&accB: 0.586, 31.250%, 99.414%] time: 0:18:17.685134\n",
      "[0.86590195, 0.2799249, 0.005859771, 0.3125, 0.99414444]\n",
      "[Epoch 2/50-2000/2018] [D loss&acc: 0.771, 46.875%] [G loss&accA&accB: 0.811, 56.250%, 99.189%] time: 0:19:52.349150\n",
      "[1.0498283, 0.23915873, 0.0081066955, 0.5625, 0.99188614]\n",
      "[Epoch 2/50-2016/2018] [D loss&acc: 0.896, 37.500%] [G loss&accA&accB: 1.008, 43.750%, 98.992%] [Test loss&acc: 0.934, 26.122%, 99.067%] time: 0:20:05.356410\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637]\n",
      "[Epoch 3/50-0/2018] [D loss&acc: 0.831, 34.375%] [G loss&accA&accB: 0.777, 31.250%, 99.223%] time: 0:20:05.925129\n",
      "[1.079921, 0.30250618, 0.007774148, 0.3125, 0.99222565]\n",
      "[Epoch 3/50-500/2018] [D loss&acc: 0.745, 46.875%] [G loss&accA&accB: 1.449, 31.250%, 98.551%] time: 0:21:40.399006\n",
      "[1.7388959, 0.2902941, 0.014486019, 0.3125, 0.9855118]\n",
      "[Epoch 3/50-1000/2018] [D loss&acc: 0.816, 53.125%] [G loss&accA&accB: 0.515, 37.500%, 99.484%] time: 0:23:14.472602\n",
      "[0.7817128, 0.26708767, 0.0051462515, 0.375, 0.9948387]\n",
      "[Epoch 3/50-1500/2018] [D loss&acc: 0.831, 50.000%] [G loss&accA&accB: 0.456, 43.750%, 99.547%] time: 0:24:48.720348\n",
      "[0.7457957, 0.2901764, 0.004556193, 0.4375, 0.99546814]\n",
      "[Epoch 3/50-2000/2018] [D loss&acc: 0.742, 40.625%] [G loss&accA&accB: 0.445, 62.500%, 99.556%] time: 0:26:23.091011\n",
      "[0.6674808, 0.2225109, 0.0044496995, 0.625, 0.9955559]\n",
      "[Epoch 3/50-2016/2018] [D loss&acc: 0.862, 50.000%] [G loss&accA&accB: 0.936, 43.750%, 99.063%] [Test loss&acc: 0.792, 10.427%, 99.210%] time: 0:26:36.015280\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392]\n",
      "[Epoch 4/50-0/2018] [D loss&acc: 0.803, 28.125%] [G loss&accA&accB: 0.742, 31.250%, 99.260%] time: 0:26:36.585455\n",
      "[1.045299, 0.30308744, 0.0074221157, 0.3125, 0.9925957]\n",
      "[Epoch 4/50-500/2018] [D loss&acc: 0.710, 56.250%] [G loss&accA&accB: 0.693, 50.000%, 99.307%] time: 0:28:10.600832\n",
      "[0.943745, 0.2507796, 0.0069296537, 0.5, 0.9930725]\n",
      "[Epoch 4/50-1000/2018] [D loss&acc: 0.777, 46.875%] [G loss&accA&accB: 0.842, 68.750%, 99.158%] time: 0:29:44.665043\n",
      "[1.0788323, 0.23702495, 0.008418074, 0.6875, 0.99157715]\n",
      "[Epoch 4/50-1500/2018] [D loss&acc: 0.744, 53.125%] [G loss&accA&accB: 1.075, 50.000%, 98.925%] time: 0:31:18.504954\n",
      "[1.3230093, 0.24789077, 0.010751186, 0.5, 0.9892502]\n",
      "[Epoch 4/50-2000/2018] [D loss&acc: 0.733, 46.875%] [G loss&accA&accB: 0.624, 62.500%, 99.378%] time: 0:32:52.469426\n",
      "[0.8336548, 0.20989938, 0.006237555, 0.625, 0.99378204]\n",
      "[Epoch 4/50-2016/2018] [D loss&acc: 0.753, 46.875%] [G loss&accA&accB: 0.699, 43.750%, 99.303%] [Test loss&acc: 0.756, 43.016%, 99.245%] time: 0:33:05.318681\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653]\n",
      "[Epoch 5/50-0/2018] [D loss&acc: 0.713, 46.875%] [G loss&accA&accB: 0.523, 43.750%, 99.477%] time: 0:33:05.902599\n",
      "[0.8064658, 0.2838753, 0.005225905, 0.4375, 0.99477386]\n",
      "[Epoch 5/50-500/2018] [D loss&acc: 0.740, 50.000%] [G loss&accA&accB: 0.923, 56.250%, 99.078%] time: 0:34:39.688103\n",
      "[1.164765, 0.24161145, 0.009231536, 0.5625, 0.99077606]\n",
      "[Epoch 5/50-1000/2018] [D loss&acc: 0.784, 50.000%] [G loss&accA&accB: 0.584, 50.000%, 99.416%] time: 0:36:13.384574\n",
      "[0.8425406, 0.25813147, 0.005844091, 0.5, 0.9941597]\n",
      "[Epoch 5/50-1500/2018] [D loss&acc: 0.707, 50.000%] [G loss&accA&accB: 0.414, 31.250%, 99.586%] time: 0:37:47.076826\n",
      "[0.66435766, 0.25059164, 0.00413766, 0.3125, 0.99586105]\n",
      "[Epoch 5/50-2000/2018] [D loss&acc: 0.710, 43.750%] [G loss&accA&accB: 1.058, 31.250%, 98.943%] time: 0:39:20.682748\n",
      "[1.3096347, 0.25157177, 0.010580629, 0.3125, 0.9894295]\n",
      "[Epoch 5/50-2016/2018] [D loss&acc: 0.727, 46.875%] [G loss&accA&accB: 0.784, 56.250%, 99.217%] [Test loss&acc: 0.726, 78.283%, 99.275%] time: 0:39:33.527810\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268]\n",
      "[Epoch 6/50-0/2018] [D loss&acc: 0.782, 40.625%] [G loss&accA&accB: 0.735, 50.000%, 99.264%] time: 0:39:34.089914\n",
      "[1.0064559, 0.27166224, 0.0073479367, 0.5, 0.99264145]\n",
      "[Epoch 6/50-500/2018] [D loss&acc: 0.740, 40.625%] [G loss&accA&accB: 0.583, 56.250%, 99.417%] time: 0:41:07.993870\n",
      "[0.8694419, 0.28606212, 0.005833798, 0.5625, 0.99417114]\n",
      "[Epoch 6/50-1000/2018] [D loss&acc: 0.741, 34.375%] [G loss&accA&accB: 0.747, 50.000%, 99.251%] time: 0:42:41.574547\n",
      "[1.0132111, 0.2658022, 0.0074740895, 0.5, 0.99251175]\n",
      "[Epoch 6/50-1500/2018] [D loss&acc: 0.695, 50.000%] [G loss&accA&accB: 0.775, 62.500%, 99.225%] time: 0:44:15.083856\n",
      "[0.98402226, 0.20852318, 0.0077549913, 0.625, 0.99224854]\n",
      "[Epoch 6/50-2000/2018] [D loss&acc: 0.777, 53.125%] [G loss&accA&accB: 0.871, 56.250%, 99.132%] time: 0:45:48.613478\n",
      "[1.0938404, 0.22320242, 0.00870638, 0.5625, 0.99132156]\n",
      "[Epoch 6/50-2016/2018] [D loss&acc: 0.659, 65.625%] [G loss&accA&accB: 0.563, 62.500%, 99.437%] [Test loss&acc: 0.708, 53.025%, 99.292%] time: 0:46:01.457957\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495]\n",
      "[Epoch 7/50-0/2018] [D loss&acc: 0.714, 53.125%] [G loss&accA&accB: 0.646, 68.750%, 99.352%] time: 0:46:02.036955\n",
      "[0.851492, 0.20528543, 0.0064620655, 0.6875, 0.993515]\n",
      "[Epoch 7/50-500/2018] [D loss&acc: 0.798, 40.625%] [G loss&accA&accB: 0.709, 62.500%, 99.291%] time: 0:47:35.347312\n",
      "[0.9344781, 0.225792, 0.0070868614, 0.625, 0.9929085]\n",
      "[Epoch 7/50-1000/2018] [D loss&acc: 0.773, 56.250%] [G loss&accA&accB: 0.676, 50.000%, 99.323%] time: 0:49:08.640684\n",
      "[0.98949945, 0.31345296, 0.006760465, 0.5, 0.9932251]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/50-1500/2018] [D loss&acc: 0.725, 50.000%] [G loss&accA&accB: 0.906, 37.500%, 99.095%] time: 0:50:42.114727\n",
      "[1.1476023, 0.24202877, 0.009055736, 0.375, 0.9909477]\n",
      "[Epoch 7/50-2000/2018] [D loss&acc: 0.764, 37.500%] [G loss&accA&accB: 0.615, 37.500%, 99.387%] time: 0:52:15.547389\n",
      "[0.8928885, 0.2774368, 0.0061545167, 0.375, 0.99386597]\n",
      "[Epoch 7/50-2016/2018] [D loss&acc: 0.753, 43.750%] [G loss&accA&accB: 0.885, 37.500%, 99.114%] [Test loss&acc: 0.690, 38.166%, 99.310%] time: 0:52:28.401117\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999]\n",
      "[Epoch 8/50-0/2018] [D loss&acc: 0.747, 34.375%] [G loss&accA&accB: 0.260, 75.000%, 99.739%] time: 0:52:28.960087\n",
      "[0.4908646, 0.23103589, 0.002598287, 0.75, 0.99739456]\n",
      "[Epoch 8/50-500/2018] [D loss&acc: 0.747, 53.125%] [G loss&accA&accB: 0.556, 37.500%, 99.443%] time: 0:54:02.292433\n",
      "[0.8212394, 0.26538706, 0.005558524, 0.375, 0.99443436]\n",
      "[Epoch 8/50-1000/2018] [D loss&acc: 0.717, 46.875%] [G loss&accA&accB: 0.794, 43.750%, 99.207%] time: 0:55:35.716246\n",
      "[1.0533535, 0.25930735, 0.007940462, 0.4375, 0.9920654]\n",
      "[Epoch 8/50-1500/2018] [D loss&acc: 0.747, 50.000%] [G loss&accA&accB: 0.540, 56.250%, 99.462%] time: 0:57:08.952417\n",
      "[0.816885, 0.27691227, 0.0053997273, 0.5625, 0.99461746]\n",
      "[Epoch 8/50-2000/2018] [D loss&acc: 0.716, 46.875%] [G loss&accA&accB: 0.269, 18.750%, 99.731%] time: 0:58:42.240095\n",
      "[0.5791036, 0.31060314, 0.0026850044, 0.1875, 0.9973068]\n",
      "[Epoch 8/50-2016/2018] [D loss&acc: 0.734, 37.500%] [G loss&accA&accB: 0.551, 56.250%, 99.449%] [Test loss&acc: 0.687, 45.916%, 99.313%] time: 0:58:55.142194\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882]\n",
      "[Epoch 9/50-0/2018] [D loss&acc: 0.757, 50.000%] [G loss&accA&accB: 0.655, 56.250%, 99.345%] time: 0:58:55.739976\n",
      "[0.8929584, 0.23749706, 0.006554613, 0.5625, 0.993454]\n",
      "[Epoch 9/50-500/2018] [D loss&acc: 0.690, 56.250%] [G loss&accA&accB: 0.624, 25.000%, 99.377%] time: 1:00:29.270203\n",
      "[0.9109076, 0.28653932, 0.0062436834, 0.25, 0.9937706]\n",
      "[Epoch 9/50-1000/2018] [D loss&acc: 0.722, 50.000%] [G loss&accA&accB: 1.083, 37.500%, 98.920%] time: 1:02:02.548364\n",
      "[1.3499421, 0.26696745, 0.010829747, 0.375, 0.9891968]\n",
      "[Epoch 9/50-1500/2018] [D loss&acc: 0.732, 31.250%] [G loss&accA&accB: 0.585, 50.000%, 99.417%] time: 1:03:35.975616\n",
      "[0.83532387, 0.2507208, 0.005846031, 0.5, 0.99417114]\n",
      "[Epoch 9/50-2000/2018] [D loss&acc: 0.764, 43.750%] [G loss&accA&accB: 0.637, 43.750%, 99.365%] time: 1:05:09.265960\n",
      "[0.8802452, 0.24315235, 0.006370929, 0.4375, 0.9936485]\n",
      "[Epoch 9/50-2016/2018] [D loss&acc: 0.657, 59.375%] [G loss&accA&accB: 0.451, 62.500%, 99.548%] [Test loss&acc: 0.680, 60.608%, 99.321%] time: 1:05:22.112632\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803]\n",
      "[Epoch 10/50-0/2018] [D loss&acc: 0.722, 46.875%] [G loss&accA&accB: 0.300, 62.500%, 99.700%] time: 1:05:22.652117\n",
      "[0.5457832, 0.24555945, 0.0030022375, 0.625, 0.99700165]\n",
      "[Epoch 10/50-500/2018] [D loss&acc: 0.729, 37.500%] [G loss&accA&accB: 0.603, 62.500%, 99.397%] time: 1:06:55.974131\n",
      "[0.8402348, 0.23735696, 0.006028779, 0.625, 0.99396896]\n",
      "[Epoch 10/50-1000/2018] [D loss&acc: 0.765, 25.000%] [G loss&accA&accB: 0.680, 50.000%, 99.320%] time: 1:08:29.517316\n",
      "[0.91528064, 0.23509336, 0.006801873, 0.5, 0.9931984]\n",
      "[Epoch 10/50-1500/2018] [D loss&acc: 0.732, 50.000%] [G loss&accA&accB: 0.413, 56.250%, 99.586%] time: 1:10:02.863717\n",
      "[0.6584882, 0.24535303, 0.0041313516, 0.5625, 0.99585724]\n",
      "[Epoch 10/50-2000/2018] [D loss&acc: 0.717, 56.250%] [G loss&accA&accB: 0.711, 68.750%, 99.289%] time: 1:11:36.288685\n",
      "[0.94050324, 0.22957256, 0.007109307, 0.6875, 0.9928894]\n",
      "[Epoch 10/50-2016/2018] [D loss&acc: 0.711, 53.125%] [G loss&accA&accB: 0.464, 56.250%, 99.537%] [Test loss&acc: 0.686, 31.363%, 99.314%] time: 1:11:49.128601\n",
      "waited for 1 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194]\n",
      "[Epoch 11/50-0/2018] [D loss&acc: 0.683, 50.000%] [G loss&accA&accB: 0.580, 56.250%, 99.420%] time: 1:11:49.319540\n",
      "[0.83440983, 0.25460812, 0.0057980176, 0.5625, 0.99420166]\n",
      "[Epoch 11/50-500/2018] [D loss&acc: 0.719, 28.125%] [G loss&accA&accB: 0.650, 62.500%, 99.350%] time: 1:13:22.738260\n",
      "[0.87582177, 0.22615457, 0.006496672, 0.625, 0.9935036]\n",
      "[Epoch 11/50-1000/2018] [D loss&acc: 0.739, 37.500%] [G loss&accA&accB: 0.770, 37.500%, 99.229%] time: 1:14:55.928153\n",
      "[1.0511812, 0.28155425, 0.00769627, 0.375, 0.9922943]\n",
      "[Epoch 11/50-1500/2018] [D loss&acc: 0.698, 56.250%] [G loss&accA&accB: 0.816, 50.000%, 99.183%] time: 1:16:29.077482\n",
      "[1.0786035, 0.26253027, 0.008160732, 0.5, 0.9918251]\n",
      "[Epoch 11/50-2000/2018] [D loss&acc: 0.710, 46.875%] [G loss&accA&accB: 0.375, 68.750%, 99.626%] time: 1:18:02.321942\n",
      "[0.5958667, 0.2211952, 0.003746715, 0.6875, 0.9962578]\n",
      "[Epoch 11/50-2016/2018] [D loss&acc: 0.682, 53.125%] [G loss&accA&accB: 0.802, 50.000%, 99.198%] [Test loss&acc: 0.675, 61.305%, 99.325%] time: 1:18:15.091228\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206]\n",
      "[Epoch 12/50-0/2018] [D loss&acc: 0.697, 53.125%] [G loss&accA&accB: 0.153, 31.250%, 99.847%] time: 1:18:15.665456\n",
      "[0.44522178, 0.29208073, 0.0015314107, 0.3125, 0.9984703]\n",
      "[Epoch 12/50-500/2018] [D loss&acc: 0.692, 50.000%] [G loss&accA&accB: 0.381, 50.000%, 99.620%] time: 1:19:48.460214\n",
      "[0.6789998, 0.29818052, 0.0038081924, 0.5, 0.99619675]\n",
      "[Epoch 12/50-1000/2018] [D loss&acc: 0.688, 56.250%] [G loss&accA&accB: 0.672, 50.000%, 99.329%] time: 1:21:21.425120\n",
      "[0.9111935, 0.23932518, 0.006718683, 0.5, 0.99328613]\n",
      "[Epoch 12/50-1500/2018] [D loss&acc: 0.668, 59.375%] [G loss&accA&accB: 0.611, 62.500%, 99.390%] time: 1:22:54.523977\n",
      "[0.84554714, 0.23500371, 0.006105434, 0.625, 0.9938965]\n",
      "[Epoch 12/50-2000/2018] [D loss&acc: 0.712, 43.750%] [G loss&accA&accB: 0.602, 62.500%, 99.399%] time: 1:24:27.596726\n",
      "[0.83716106, 0.23557934, 0.0060158176, 0.625, 0.99398804]\n",
      "[Epoch 12/50-2016/2018] [D loss&acc: 0.747, 43.750%] [G loss&accA&accB: 0.275, 43.750%, 99.724%] [Test loss&acc: 0.689, 41.818%, 99.311%] time: 1:24:40.308421\n",
      "waited for 1 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722]\n",
      "[Epoch 13/50-0/2018] [D loss&acc: 0.721, 46.875%] [G loss&accA&accB: 0.597, 62.500%, 99.404%] time: 1:24:40.497929\n",
      "[0.8237766, 0.22646639, 0.005973102, 0.625, 0.99404144]\n",
      "[Epoch 13/50-500/2018] [D loss&acc: 0.714, 53.125%] [G loss&accA&accB: 0.552, 43.750%, 99.449%] time: 1:26:13.241563\n",
      "[0.80310106, 0.25122115, 0.0055187987, 0.4375, 0.99448776]\n",
      "[Epoch 13/50-1000/2018] [D loss&acc: 0.733, 34.375%] [G loss&accA&accB: 0.395, 43.750%, 99.604%] time: 1:27:46.120996\n",
      "[0.6327974, 0.23774031, 0.003950571, 0.4375, 0.9960365]\n",
      "[Epoch 13/50-1500/2018] [D loss&acc: 0.696, 43.750%] [G loss&accA&accB: 0.536, 56.250%, 99.464%] time: 1:29:19.069060\n",
      "[0.7837864, 0.24747816, 0.0053630825, 0.5625, 0.99463654]\n",
      "[Epoch 13/50-2000/2018] [D loss&acc: 0.716, 50.000%] [G loss&accA&accB: 0.594, 50.000%, 99.405%] time: 1:30:51.907229\n",
      "[0.8262468, 0.2318766, 0.0059437016, 0.5, 0.9940529]\n",
      "[Epoch 13/50-2016/2018] [D loss&acc: 0.699, 46.875%] [G loss&accA&accB: 1.025, 75.000%, 98.974%] [Test loss&acc: 0.686, 54.837%, 99.314%] time: 1:31:04.684977\n",
      "waited for 2 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465]\n",
      "[Epoch 14/50-0/2018] [D loss&acc: 0.759, 43.750%] [G loss&accA&accB: 0.951, 62.500%, 99.047%] time: 1:31:04.875107\n",
      "[1.1757525, 0.2243499, 0.009514026, 0.625, 0.9904747]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 14/50-500/2018] [D loss&acc: 0.717, 40.625%] [G loss&accA&accB: 0.629, 56.250%, 99.372%] time: 1:32:38.003689\n",
      "[0.8753767, 0.24676219, 0.0062861447, 0.5625, 0.9937172]\n",
      "[Epoch 14/50-1000/2018] [D loss&acc: 0.764, 40.625%] [G loss&accA&accB: 0.855, 25.000%, 99.144%] time: 1:34:10.872384\n",
      "[1.1306777, 0.2752741, 0.008554036, 0.25, 0.99144363]\n",
      "[Epoch 14/50-1500/2018] [D loss&acc: 0.695, 50.000%] [G loss&accA&accB: 0.415, 25.000%, 99.585%] time: 1:35:43.715520\n",
      "[0.7181388, 0.30336487, 0.004147739, 0.25, 0.9958496]\n",
      "[Epoch 15/50-500/2018] [D loss&acc: 0.707, 53.125%] [G loss&accA&accB: 1.256, 25.000%, 98.744%] time: 1:39:02.282989\n",
      "[1.5275645, 0.27107924, 0.012564852, 0.25, 0.987442]\n",
      "[Epoch 15/50-1000/2018] [D loss&acc: 0.673, 62.500%] [G loss&accA&accB: 0.554, 50.000%, 99.445%] time: 1:40:35.223025\n",
      "[0.8110098, 0.25691152, 0.005540983, 0.5, 0.99445343]\n",
      "[Epoch 15/50-1500/2018] [D loss&acc: 0.673, 68.750%] [G loss&accA&accB: 0.231, 50.000%, 99.768%] time: 1:42:08.120294\n",
      "[0.4634719, 0.2325035, 0.002309684, 0.5, 0.99768066]\n",
      "[Epoch 15/50-2000/2018] [D loss&acc: 0.660, 68.750%] [G loss&accA&accB: 0.561, 31.250%, 99.439%] time: 1:43:41.037359\n",
      "[0.8518841, 0.29125595, 0.0056062816, 0.3125, 0.9943924]\n",
      "[Epoch 15/50-2016/2018] [D loss&acc: 0.752, 43.750%] [G loss&accA&accB: 0.609, 56.250%, 99.392%] [Test loss&acc: 0.670, 39.309%, 99.330%] time: 1:43:53.859016\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706]\n",
      "[Epoch 16/50-0/2018] [D loss&acc: 0.673, 65.625%] [G loss&accA&accB: 0.575, 25.000%, 99.425%] time: 1:43:54.437385\n",
      "[0.857259, 0.28199112, 0.0057526785, 0.25, 0.99424744]\n",
      "[Epoch 16/50-500/2018] [D loss&acc: 0.705, 50.000%] [G loss&accA&accB: 0.803, 50.000%, 99.197%] time: 1:45:27.316758\n",
      "[1.0477977, 0.24510401, 0.008026937, 0.5, 0.99196625]\n",
      "[Epoch 16/50-1000/2018] [D loss&acc: 0.692, 50.000%] [G loss&accA&accB: 0.671, 68.750%, 99.329%] time: 1:47:00.378036\n",
      "[0.9148439, 0.24367711, 0.0067116683, 0.6875, 0.99328613]\n",
      "[Epoch 16/50-1500/2018] [D loss&acc: 0.696, 40.625%] [G loss&accA&accB: 0.704, 56.250%, 99.296%] time: 1:48:33.173332\n",
      "[0.94463706, 0.24090715, 0.007037299, 0.5625, 0.9929619]\n",
      "[Epoch 16/50-2000/2018] [D loss&acc: 0.714, 50.000%] [G loss&accA&accB: 0.330, 50.000%, 99.669%] time: 1:50:05.874812\n",
      "[0.5668807, 0.2364156, 0.003304651, 0.5, 0.99669266]\n",
      "[Epoch 16/50-2016/2018] [D loss&acc: 0.694, 50.000%] [G loss&accA&accB: 0.478, 37.500%, 99.523%] [Test loss&acc: 0.670, 46.892%, 99.330%] time: 1:50:18.643662\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129]\n",
      "[Epoch 17/50-0/2018] [D loss&acc: 0.676, 53.125%] [G loss&accA&accB: 0.346, 56.250%, 99.654%] time: 1:50:19.233705\n",
      "[0.58643275, 0.2405603, 0.0034587246, 0.5625, 0.99654007]\n",
      "[Epoch 17/50-500/2018] [D loss&acc: 0.702, 50.000%] [G loss&accA&accB: 0.665, 62.500%, 99.334%] time: 1:51:52.022193\n",
      "[0.9046137, 0.2396205, 0.006649932, 0.625, 0.99333954]\n",
      "[Epoch 17/50-1000/2018] [D loss&acc: 0.701, 53.125%] [G loss&accA&accB: 0.716, 50.000%, 99.284%] time: 1:53:24.757847\n",
      "[0.9671849, 0.25118986, 0.00715995, 0.5, 0.9928436]\n",
      "[Epoch 17/50-1500/2018] [D loss&acc: 0.744, 37.500%] [G loss&accA&accB: 0.617, 56.250%, 99.382%] time: 1:54:57.733071\n",
      "[0.8637327, 0.24665514, 0.0061707757, 0.5625, 0.9938202]\n",
      "[Epoch 17/50-2000/2018] [D loss&acc: 0.682, 50.000%] [G loss&accA&accB: 0.574, 37.500%, 99.427%] time: 1:56:30.451320\n",
      "[0.8351898, 0.26090622, 0.005742836, 0.375, 0.9942665]\n",
      "[Epoch 17/50-2016/2018] [D loss&acc: 0.686, 53.125%] [G loss&accA&accB: 0.471, 62.500%, 99.527%] [Test loss&acc: 0.661, 56.370%, 99.340%] time: 1:56:43.212149\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982]\n",
      "[Epoch 18/50-0/2018] [D loss&acc: 0.726, 50.000%] [G loss&accA&accB: 0.447, 56.250%, 99.552%] time: 1:56:43.801799\n",
      "[0.6943966, 0.24715002, 0.004472466, 0.5625, 0.99551773]\n",
      "[Epoch 18/50-500/2018] [D loss&acc: 0.701, 43.750%] [G loss&accA&accB: 0.195, 50.000%, 99.805%] time: 1:58:16.611051\n",
      "[0.46493012, 0.27040303, 0.0019452709, 0.5, 0.9980545]\n",
      "[Epoch 18/50-1000/2018] [D loss&acc: 0.671, 59.375%] [G loss&accA&accB: 0.690, 50.000%, 99.311%] time: 1:59:49.518346\n",
      "[0.947683, 0.25750527, 0.0069017773, 0.5, 0.99311066]\n",
      "[Epoch 18/50-1500/2018] [D loss&acc: 0.725, 31.250%] [G loss&accA&accB: 0.498, 18.750%, 99.503%] time: 2:01:22.418347\n",
      "[0.780772, 0.28299987, 0.0049777213, 0.1875, 0.99502563]\n",
      "[Epoch 18/50-2000/2018] [D loss&acc: 0.699, 43.750%] [G loss&accA&accB: 0.760, 50.000%, 99.240%] time: 2:02:55.355387\n",
      "[1.0145084, 0.2548108, 0.0075969757, 0.5, 0.99240494]\n",
      "[Epoch 18/50-2016/2018] [D loss&acc: 0.707, 53.125%] [G loss&accA&accB: 0.559, 50.000%, 99.441%] [Test loss&acc: 0.667, 59.353%, 99.333%] time: 2:03:08.252438\n",
      "waited for 1 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526]\n",
      "[Epoch 19/50-0/2018] [D loss&acc: 0.693, 53.125%] [G loss&accA&accB: 0.370, 50.000%, 99.630%] time: 2:03:08.439050\n",
      "[0.61987954, 0.2496786, 0.0037020093, 0.5, 0.99630356]\n",
      "[Epoch 19/50-500/2018] [D loss&acc: 0.692, 46.875%] [G loss&accA&accB: 0.296, 50.000%, 99.704%] time: 2:04:41.287140\n",
      "[0.54589444, 0.25013968, 0.0029575475, 0.5, 0.9970398]\n",
      "[Epoch 19/50-1000/2018] [D loss&acc: 0.707, 46.875%] [G loss&accA&accB: 0.273, 50.000%, 99.728%] time: 2:06:13.918264\n",
      "[0.5263781, 0.2536075, 0.0027277058, 0.5, 0.9972763]\n",
      "[Epoch 19/50-1500/2018] [D loss&acc: 0.738, 56.250%] [G loss&accA&accB: 0.554, 43.750%, 99.446%] time: 2:07:46.693676\n",
      "[0.8095511, 0.25525743, 0.0055429367, 0.4375, 0.99445724]\n",
      "[Epoch 19/50-2000/2018] [D loss&acc: 0.698, 59.375%] [G loss&accA&accB: 0.574, 68.750%, 99.427%] time: 2:09:19.452480\n",
      "[0.80710876, 0.23357925, 0.005735295, 0.6875, 0.9942665]\n",
      "[Epoch 19/50-2016/2018] [D loss&acc: 0.677, 62.500%] [G loss&accA&accB: 0.614, 56.250%, 99.385%] [Test loss&acc: 0.674, 58.433%, 99.326%] time: 2:09:32.195463\n",
      "waited for 2 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565]\n",
      "[Epoch 20/50-0/2018] [D loss&acc: 0.745, 28.125%] [G loss&accA&accB: 1.039, 62.500%, 98.959%] time: 2:09:32.382929\n",
      "[1.2820461, 0.24266055, 0.010393856, 0.625, 0.9895897]\n",
      "[Epoch 20/50-500/2018] [D loss&acc: 0.738, 40.625%] [G loss&accA&accB: 0.653, 0.000%, 99.347%] time: 2:11:05.066940\n",
      "[0.9421084, 0.2890502, 0.0065305815, 0.0, 0.99346924]\n",
      "[Epoch 20/50-1000/2018] [D loss&acc: 0.709, 43.750%] [G loss&accA&accB: 0.415, 56.250%, 99.585%] time: 2:12:38.008993\n",
      "[0.6528357, 0.23800725, 0.0041482844, 0.5625, 0.9958458]\n",
      "[Epoch 20/50-1500/2018] [D loss&acc: 0.722, 40.625%] [G loss&accA&accB: 0.462, 31.250%, 99.538%] time: 2:14:10.672892\n",
      "[0.7276354, 0.2656892, 0.0046194615, 0.3125, 0.9953766]\n",
      "[Epoch 20/50-2000/2018] [D loss&acc: 0.699, 50.000%] [G loss&accA&accB: 1.042, 50.000%, 98.957%] time: 2:15:43.508753\n",
      "[1.3011559, 0.25913465, 0.0104202125, 0.5, 0.9895706]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 20/50-2016/2018] [D loss&acc: 0.712, 46.875%] [G loss&accA&accB: 0.648, 43.750%, 99.352%] [Test loss&acc: 0.658, 51.603%, 99.342%] time: 2:15:56.271554\n",
      "waited for 0 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376]\n",
      "[Epoch 21/50-0/2018] [D loss&acc: 0.702, 50.000%] [G loss&accA&accB: 0.459, 31.250%, 99.541%] time: 2:15:56.866871\n",
      "[0.7189609, 0.25991827, 0.004590426, 0.3125, 0.9954109]\n",
      "[Epoch 21/50-500/2018] [D loss&acc: 0.728, 43.750%] [G loss&accA&accB: 0.488, 56.250%, 99.512%] time: 2:17:29.529744\n",
      "[0.72087574, 0.23332322, 0.004875525, 0.5625, 0.9951248]\n",
      "[Epoch 21/50-1000/2018] [D loss&acc: 0.713, 40.625%] [G loss&accA&accB: 0.537, 68.750%, 99.463%] time: 2:19:02.342538\n",
      "[0.7794965, 0.24272181, 0.005367747, 0.6875, 0.9946251]\n",
      "[Epoch 21/50-1500/2018] [D loss&acc: 0.703, 62.500%] [G loss&accA&accB: 0.298, 50.000%, 99.702%] time: 2:20:35.276036\n",
      "[0.5461368, 0.24832964, 0.0029780716, 0.5, 0.9970207]\n",
      "[Epoch 21/50-2000/2018] [D loss&acc: 0.707, 50.000%] [G loss&accA&accB: 0.513, 56.250%, 99.488%] time: 2:22:07.995231\n",
      "[0.7700569, 0.25738442, 0.005126725, 0.5625, 0.99487686]\n",
      "[Epoch 21/50-2016/2018] [D loss&acc: 0.705, 43.750%] [G loss&accA&accB: 0.462, 37.500%, 99.538%] [Test loss&acc: 0.660, 43.853%, 99.340%] time: 2:22:20.782008\n",
      "waited for 1 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376, 0.006598576264762807]\n",
      "[Epoch 22/50-0/2018] [D loss&acc: 0.690, 53.125%] [G loss&accA&accB: 0.580, 50.000%, 99.421%] time: 2:22:20.974453\n",
      "[0.8385234, 0.25890246, 0.005796209, 0.5, 0.9942055]\n",
      "[Epoch 22/50-500/2018] [D loss&acc: 0.703, 46.875%] [G loss&accA&accB: 0.425, 62.500%, 99.575%] time: 2:23:53.795489\n",
      "[0.6716279, 0.24692833, 0.004246996, 0.625, 0.99575424]\n",
      "[Epoch 22/50-1000/2018] [D loss&acc: 0.712, 46.875%] [G loss&accA&accB: 0.332, 43.750%, 99.667%] time: 2:25:26.542190\n",
      "[0.5796228, 0.24770077, 0.0033192202, 0.4375, 0.9966736]\n",
      "[Epoch 22/50-1500/2018] [D loss&acc: 0.701, 53.125%] [G loss&accA&accB: 0.472, 56.250%, 99.528%] time: 2:26:59.247668\n",
      "[0.72678876, 0.25472915, 0.004720596, 0.5625, 0.9952812]\n",
      "[Epoch 22/50-2000/2018] [D loss&acc: 0.690, 50.000%] [G loss&accA&accB: 0.367, 43.750%, 99.633%] time: 2:28:32.018503\n",
      "[0.6145247, 0.2473067, 0.0036721802, 0.4375, 0.99632645]\n",
      "[Epoch 22/50-2016/2018] [D loss&acc: 0.712, 46.875%] [G loss&accA&accB: 0.405, 56.250%, 99.595%] [Test loss&acc: 0.667, 59.214%, 99.333%] time: 2:28:44.829738\n",
      "waited for 2 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376, 0.006598576264762807, 0.006667928190022741]\n",
      "[Epoch 23/50-0/2018] [D loss&acc: 0.702, 40.625%] [G loss&accA&accB: 0.582, 62.500%, 99.418%] time: 2:28:45.016316\n",
      "[0.8258094, 0.24357074, 0.0058223866, 0.625, 0.9941826]\n",
      "[Epoch 23/50-500/2018] [D loss&acc: 0.685, 46.875%] [G loss&accA&accB: 0.670, 37.500%, 99.329%] time: 2:30:17.592677\n",
      "[0.922992, 0.25331134, 0.0066968063, 0.375, 0.99329376]\n",
      "[Epoch 23/50-1000/2018] [D loss&acc: 0.696, 43.750%] [G loss&accA&accB: 0.489, 31.250%, 99.512%] time: 2:31:50.418132\n",
      "[0.7560804, 0.26701355, 0.004890668, 0.3125, 0.9951248]\n",
      "[Epoch 23/50-1500/2018] [D loss&acc: 0.705, 43.750%] [G loss&accA&accB: 0.930, 43.750%, 99.070%] time: 2:33:23.159800\n",
      "[1.1799788, 0.2500279, 0.009299509, 0.4375, 0.9907036]\n",
      "[Epoch 23/50-2000/2018] [D loss&acc: 0.698, 46.875%] [G loss&accA&accB: 0.355, 56.250%, 99.644%] time: 2:34:55.919896\n",
      "[0.6086801, 0.253286, 0.0035539414, 0.5625, 0.9964447]\n",
      "[Epoch 23/50-2016/2018] [D loss&acc: 0.686, 59.375%] [G loss&accA&accB: 0.494, 56.250%, 99.506%] [Test loss&acc: 0.661, 36.521%, 99.339%] time: 2:35:08.676834\n",
      "waited for 3 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376, 0.006598576264762807, 0.006667928190022741, 0.006610862626573411]\n",
      "[Epoch 24/50-0/2018] [D loss&acc: 0.691, 50.000%] [G loss&accA&accB: 0.278, 25.000%, 99.720%] time: 2:35:08.871555\n",
      "[0.5374552, 0.25978762, 0.0027766756, 0.25, 0.9972038]\n",
      "[Epoch 24/50-500/2018] [D loss&acc: 0.699, 50.000%] [G loss&accA&accB: 0.420, 62.500%, 99.580%] time: 2:36:41.783070\n",
      "[0.6660832, 0.2460939, 0.004199893, 0.625, 0.9957962]\n",
      "[Epoch 24/50-1000/2018] [D loss&acc: 0.696, 46.875%] [G loss&accA&accB: 0.766, 68.750%, 99.234%] time: 2:38:14.277756\n",
      "[1.0023522, 0.2367599, 0.0076559237, 0.6875, 0.9923439]\n",
      "[Epoch 24/50-1500/2018] [D loss&acc: 0.707, 46.875%] [G loss&accA&accB: 0.604, 37.500%, 99.397%] time: 2:39:46.975073\n",
      "[0.8561603, 0.25236621, 0.006037941, 0.375, 0.9939728]\n",
      "[Epoch 24/50-2000/2018] [D loss&acc: 0.689, 59.375%] [G loss&accA&accB: 0.648, 56.250%, 99.352%] time: 2:41:19.521571\n",
      "[0.88856083, 0.24083452, 0.006477263, 0.5625, 0.9935188]\n",
      "[Epoch 24/50-2016/2018] [D loss&acc: 0.700, 56.250%] [G loss&accA&accB: 0.563, 56.250%, 99.437%] [Test loss&acc: 0.672, 40.563%, 99.328%] time: 2:41:32.292882\n",
      "waited for 4 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376, 0.006598576264762807, 0.006667928190022741, 0.006610862626573411, 0.006719214960637782]\n",
      "[Epoch 25/50-0/2018] [D loss&acc: 0.690, 62.500%] [G loss&accA&accB: 0.639, 43.750%, 99.361%] time: 2:41:32.484053\n",
      "[0.90387756, 0.2644326, 0.0063944496, 0.4375, 0.9936142]\n",
      "[Epoch 25/50-500/2018] [D loss&acc: 0.701, 46.875%] [G loss&accA&accB: 0.547, 56.250%, 99.453%] time: 2:43:05.162234\n",
      "[0.7999549, 0.2526869, 0.0054726796, 0.5625, 0.9945297]\n",
      "[Epoch 25/50-1000/2018] [D loss&acc: 0.694, 56.250%] [G loss&accA&accB: 0.292, 37.500%, 99.707%] time: 2:44:37.843023\n",
      "[0.5451435, 0.25357583, 0.0029156767, 0.375, 0.9970741]\n",
      "[Epoch 25/50-1500/2018] [D loss&acc: 0.706, 43.750%] [G loss&accA&accB: 0.702, 31.250%, 99.298%] time: 2:46:10.524215\n",
      "[0.9544162, 0.25235426, 0.0070206197, 0.3125, 0.99297714]\n",
      "[Epoch 25/50-2000/2018] [D loss&acc: 0.687, 50.000%] [G loss&accA&accB: 0.754, 37.500%, 99.246%] time: 2:47:43.217932\n",
      "[1.0123478, 0.25831762, 0.0075403024, 0.375, 0.99245834]\n",
      "[Epoch 25/50-2016/2018] [D loss&acc: 0.700, 40.625%] [G loss&accA&accB: 0.669, 62.500%, 99.330%] [Test loss&acc: 0.664, 77.725%, 99.336%] time: 2:47:55.987860\n",
      "waited for 5 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376, 0.006598576264762807, 0.006667928190022741, 0.006610862626573411, 0.006719214960637782, 0.006636985024872981]\n",
      "[Epoch 26/50-0/2018] [D loss&acc: 0.693, 46.875%] [G loss&accA&accB: 0.486, 50.000%, 99.514%] time: 2:47:56.180104\n",
      "[0.73467773, 0.24906048, 0.0048561725, 0.5, 0.9951401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 26/50-500/2018] [D loss&acc: 0.680, 62.500%] [G loss&accA&accB: 0.568, 68.750%, 99.432%] time: 2:49:29.055019\n",
      "[0.8129293, 0.24506077, 0.0056786845, 0.6875, 0.9943199]\n",
      "[Epoch 26/50-1000/2018] [D loss&acc: 0.684, 53.125%] [G loss&accA&accB: 0.461, 25.000%, 99.540%] time: 2:51:01.644231\n",
      "[0.7246095, 0.26346684, 0.004611427, 0.25, 0.99539566]\n",
      "[Epoch 26/50-1500/2018] [D loss&acc: 0.701, 40.625%] [G loss&accA&accB: 0.449, 62.500%, 99.551%] time: 2:52:34.289413\n",
      "[0.69088554, 0.24217561, 0.0044870996, 0.625, 0.9955101]\n",
      "[Epoch 26/50-2000/2018] [D loss&acc: 0.692, 46.875%] [G loss&accA&accB: 0.804, 43.750%, 99.195%] time: 2:54:07.092106\n",
      "[1.0582613, 0.25395608, 0.008043052, 0.4375, 0.991951]\n",
      "[Epoch 26/50-2016/2018] [D loss&acc: 0.703, 34.375%] [G loss&accA&accB: 0.195, 37.500%, 99.805%] [Test loss&acc: 0.664, 29.579%, 99.337%] time: 2:54:19.835604\n",
      "waited for 6 [0.01004717296214241, 0.009768815507324568, 0.00934266658413637, 0.007916009215472392, 0.007559107356775653, 0.0072569459529268, 0.007082200821661495, 0.006903687165387999, 0.006868530663027882, 0.006796380373588803, 0.006862369040793194, 0.0067516930715747206, 0.006886414380612722, 0.0068582261454003465, 0.006767707003178223, 0.006698096887233706, 0.006697004151943129, 0.006605529214678982, 0.006667516233359526, 0.0067359845270565, 0.006575954473178376, 0.006598576264762807, 0.006667928190022741, 0.006610862626573411, 0.006719214960637782, 0.006636985024872981, 0.006635406790760763]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN4MapGen(\"dc\", batch_size = 16)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"wgangp\", batch_size = 16)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"srgan\", batch_size = 2)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=500, patience = 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"p2p\", batch_size = 1)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=1000, patience = 15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a,b,c = gan.combined.predict([x_test_sim, y_test_sim])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tloss = gan.valid_cell_srgan( x_test_sim, y_test_sim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tloss."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan.generator.predict(x_test_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen()\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs=50, batch_size=16, sample_interval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,1))\n",
    "        fake = -1 * np.ones((batch_size,1))\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        valid_acc = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        patience = 5\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "                #d_loss_real = self.discriminator.train_on_batch(imgs_B, valid)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                #self.discriminator.trainable = False\n",
    "                g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "                #self.discriminator.trainable = True\n",
    "                \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.train_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss)\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch >= 0:\n",
    "\n",
    "                valid_test = np.ones((len(x_test_sim),1))\n",
    "                t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "                    \n",
    "                self.valid_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss)\n",
    "                    \n",
    "                train_loss.append(g_loss[2])\n",
    "                train_acc.append(g_loss[4])\n",
    "                valid_loss.append(t_loss[2])\n",
    "                valid_acc.append(t_loss[4])\n",
    "\n",
    "                waited = len(valid_loss) - 1 - np.argmin(valid_loss)\n",
    "                print('waited for', waited, valid_loss)\n",
    "                    \n",
    "                if waited == 0:\n",
    "                    self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')   \n",
    "                        \n",
    "                if waited > patience:\n",
    "                    break\n",
    "            \n",
    "        return train_acc, train_loss, valid_acc, valid_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
