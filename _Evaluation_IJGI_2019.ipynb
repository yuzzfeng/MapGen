{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for MapGen models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "#matplotlib.use('Agg') # necessary for linux kernal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "from keras import models\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "from data_helper import readImg\n",
    "\n",
    "def update_gan_generator_to_any_size(old_model):\n",
    "    # Remove the top layer and add input with no limit\n",
    "    old_model.layers.pop(0) \n",
    "    newInput = Input(shape=(None, None, 1)) # New image input\n",
    "    newOutputs = old_model(newInput)\n",
    "    newModel = Model(newInput, newOutputs)\n",
    "\n",
    "    return newModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    #scale = 25\n",
    "    #modelPath = '../tmp_results/predictions/2018-07-20 07-53-50_25/'\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/Unet_25k/\"\n",
    "    \n",
    "    #scale = 15\n",
    "    #modelPath = '../tmp_results/predictions/2018-07-19 13-13-22_15/'\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/Unet_15k/\"\n",
    "    \n",
    "    scale = 10\n",
    "    modelPath = '../tmp_results/predictions/2018-07-19 15-14-38_10/'\n",
    "    out_evaluation = r\"../tmp_results/Evaluations/Unet_10k/\"\n",
    "\n",
    "    modelname = \"weights.hdf5\"\n",
    "    saved_model = models.load_model(modelPath + modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1:\n",
    "    #scale = 25\n",
    "    ##modelPath = '../tmp_results/predictions/2019-02-20 12-55-43_25/' # L2 Best with 0.011985763888888834 error\n",
    "    ##modelPath = '../tmp_results/predictions/2019-02-20 15-20-05_25/' # L1\n",
    "    #modelPath = '../tmp_results/predictions/2019-02-21 12-14-20_25/' # L2 2 Error: 0.011566840277777768\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/Runet_25k/\"\n",
    "    \n",
    "    scale = 15\n",
    "    #modelPath = '../tmp_results/predictions/2019-02-21 14-40-38_15/'\n",
    "    modelPath = '../tmp_results/predictions/2019-02-26 12-55-00_15/' # 0.0004 adam mse \n",
    "    out_evaluation = r\"../tmp_results/Evaluations/Runet_15k/\"\n",
    "    \n",
    "    #scale = 10\n",
    "    #modelPath = '../tmp_results/predictions/2019-02-21 16-24-04_10/'\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/Runet_10k/\"\n",
    "\n",
    "    modelname = \"weights.h5\"\n",
    "    saved_model = load_model(modelPath + modelname)\n",
    "    saved_model = update_gan_generator_to_any_size(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    scale = 15\n",
    "\n",
    "    #modelPath = '../tmp_results/predictions/2019-01-17 12-16-50_15/'\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_15k/\"\n",
    "    #epoch = max(epochs)\n",
    "\n",
    "    #modelPath = '../tmp_results/predictions/2019-01-17 16-22-56_15/' # MSE MAE 1:1000, not better than the case with 1:100\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_15k_1000/\"\n",
    "    #epoch = max(epochs)\n",
    "    \n",
    "    epoch = 22\n",
    "    modelPath = '../tmp_results/predictions/2019-01-22 18-19-09_15/' # MSE MAE 1:1000, not better than the case with 1:100\n",
    "    out_evaluation = r\"../tmp_results/Evaluations/GAN_15k_clip/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    scale = 10\n",
    "    \n",
    "    #modelPath = '../tmp_results/predictions/2019-01-17 20-29-09_10/'\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_10k/\",\n",
    "    #epoch = max(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    scale = 25\n",
    "    \n",
    "    #epoch = 3\n",
    "    #modelPath = '../tmp_results/predictions/2019-01-18 11-13-51_25/'\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_25k/\"\n",
    "    \n",
    "    #epoch = 5\n",
    "    #modelPath = '../tmp_results/predictions/2019-01-18 14-03-51_25/' # Best 25k\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_25k/\"\n",
    "    \n",
    "    #epoch = 11\n",
    "    #modelPath = '../tmp_results/predictions/2019-01-23 13-20-19_25/' # Best 25k\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_25k_clip/\"\n",
    "    \n",
    "    #epoch = 12 # Error best 0.01410190972222225\n",
    "    #modelPath = '../tmp_results/predictions/2019-01-23 16-48-27_25/' # Best 25k\n",
    "    #out_evaluation = r\"../tmp_results/Evaluations/GAN_25k_clip/\" \n",
    "    \n",
    "    epoch = 19\n",
    "    modelPath = '../tmp_results/predictions/2019-02-20 08-53-59_25/' # Best 25k\n",
    "    out_evaluation = r\"../tmp_results/Evaluations/GAN_25k_clip/\" \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    modelnames = [modelname for modelname in os.listdir(modelPath) if '.h5' in modelname]\n",
    "    epochs = [int(modelname.split('epoch')[1].split('.')[0]) for modelname in modelnames]\n",
    "    \n",
    "    modelname = \"model_epoch\" +str(epoch)+ \".h5\"\n",
    "    print(modelPath + modelname, 'loaded')\n",
    "    \n",
    "    saved_model = load_model(modelPath + modelname)\n",
    "    saved_model = update_gan_generator_to_any_size(saved_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoUcheck(img_input, img_output):\n",
    "    # Pixelwise IoU score\n",
    "    logic_and = np.sum(np.logical_and(img_output, img_input))\n",
    "    logic_or = np.sum(np.logical_or(img_output, img_input))\n",
    "    return logic_and/logic_or\n",
    "\n",
    "def rescaleImg(image_arr):\n",
    "    # Rescale the image to 8 x n\n",
    "    if image_arr.shape[0] % 8 != 0:\n",
    "        n = image_arr.shape[0] % 8\n",
    "        new_x = image_arr.shape[0] - n\n",
    "    else:\n",
    "        new_x = image_arr.shape[0]\n",
    "\n",
    "    if image_arr.shape[1] % 8 != 0:\n",
    "        n = image_arr.shape[1] % 8\n",
    "        new_y = image_arr.shape[1] - n\n",
    "    else:\n",
    "        new_y = image_arr.shape[1]\n",
    "\n",
    "    image_arr = image_arr[:new_x, :new_y]\n",
    "\n",
    "    return image_arr\n",
    "\n",
    "def save_prediction(img, out_path, out_filename, subfix):\n",
    "    fig = plt.figure(figsize=(img.shape[1] / 1000, img.shape[0] / 1000), dpi=100, frameon=False)\n",
    "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    fig.savefig(out_path + out_filename + subfix, dpi=1000)\n",
    "    \n",
    "def evaluate(image_arrA, image_arrB):\n",
    "    \n",
    "    target_names = ['0', '1']\n",
    "    \n",
    "    y_true = image_arrB.flatten().astype(bool) \n",
    "    y_pred = image_arrA.flatten().astype(bool)\n",
    "        \n",
    "    Accuracy = accuracy_score(y_true, y_pred)\n",
    "    IntOverUnion = IoUcheck(y_true, y_pred)\n",
    "    conf = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, target_names=target_names)\n",
    "        \n",
    "    print('Acc:', Accuracy)\n",
    "    print('Error:', 1 - Accuracy)\n",
    "    print('IoU:', IntOverUnion)\n",
    "    print(conf)\n",
    "    #print(report)\n",
    "\n",
    "    return Accuracy, IntOverUnion, conf, report\n",
    "\n",
    "def model_predict(newModel, input_image, num_runs):\n",
    "    # Predict with model for n times\n",
    "    m,n = input_image.shape\n",
    "    for i in range(num_runs):\n",
    "        input_image = np.reshape(input_image, (1, m, n, 1))\n",
    "        input_image = newModel.predict([input_image])\n",
    "        input_image = np.reshape(input_image,(m, n)) > 0.5\n",
    "    return input_image\n",
    "\n",
    "def predict_eval(SavedModel, img_range, testPath, fn_input, fn_target, out_path, scale, nr = 1):\n",
    "\n",
    "    image_arr = readImg(testPath + fn_input)\n",
    "    image_tar = readImg(testPath + fn_target)\n",
    "    \n",
    "    if len(img_range) == 4: # If range was set\n",
    "        xmin, xmax, ymin, ymax = img_range\n",
    "        image_arr = image_arr[xmin:xmax, ymin:ymax]\n",
    "        image_tar = image_tar[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    print(\"- \" + str(scale) +\"k\", 'Example: ', image_arr.shape)\n",
    "    evals_orig = evaluate(image_arr, image_tar)\n",
    "    \n",
    "    image_arr = rescaleImg(image_arr)\n",
    "    image_tar = rescaleImg(image_tar)\n",
    "    \n",
    "    print(\"+ \" + str(scale) +\"k\", 'Prediction: ', image_arr.shape)\n",
    "    pred = model_predict(SavedModel, image_arr, num_runs = nr)\n",
    "    evals_pred = evaluate(pred > 0.5, image_tar)\n",
    "    \n",
    "    save_prediction(pred, out_path, fn_input[:-4], '_' + str(scale) + '_' + str(nr) + '_out.png')\n",
    "    return [evals_orig, evals_pred]\n",
    "\n",
    "def predict_only(SavedModel, img_range, testPath, fn_input, out_path, scale, nr = 1):\n",
    "\n",
    "    image_arr = readImg(testPath + fn_input)\n",
    "    \n",
    "    if len(img_range) == 4: # If range was set\n",
    "        xmin, xmax, ymin, ymax = img_range\n",
    "        image_arr = image_arr[xmin:xmax, ymin:ymax]\n",
    "    \n",
    "    image_arr = rescaleImg(image_arr)\n",
    "    \n",
    "    print(\"+ \" + str(scale) +\"k\", 'Prediction: ', image_arr.shape)\n",
    "    pred = model_predict(SavedModel, image_arr, num_runs = nr)\n",
    "    \n",
    "    save_prediction(pred > 0.5, out_path, fn_input[:-4], '_' + str(scale) + '_' + str(nr) + '_out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For all scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_path = r\"../tmp_data/Data/Testing_large/4270/\"\n",
    "\n",
    "all_records = []\n",
    "records = predict_eval(saved_model, [0,2400,500,2900], tester_path, r\"geb_clip_4270.png\", r\"geb\"+str(scale)+\"_clip_4270.png\", out_evaluation, scale, nr = 1)\n",
    "all_records.extend(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = predict_eval(saved_model, [0,2400,500,2900], tester_path, r\"geb_clip_4270.png\", r\"geb\"+str(scale)+\"_clip_4270.png\", out_evaluation, scale, nr = 2)\n",
    "all_records.extend(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_records = [(acc * 100, (1-acc) * 100, iou * 100) for acc, iou, conf, report in all_records]\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(np.transpose(_records))\n",
    "df.columns = [\"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\", \n",
    "              \"Input vs Target (4270_)\", \"Prediction vs Target (4270 - 10)\"]\n",
    "\n",
    "#df.columns = [\"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\"]\n",
    "\n",
    "df = df.rename({0: \"Accuracy\", 1: 'Error', 2: 'IoU'})\n",
    "df.index.name = 'Metrics'\n",
    "\n",
    "df[[\"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\", \"Prediction vs Target (4270 - 10)\"]]\n",
    "#df[[\"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_path = r\"../tmp_data/Data/Testing/\"\n",
    "\n",
    "predict_only(saved_model, [], tester_path, r\"h1.tif\", out_evaluation, scale, nr = 1)\n",
    "predict_only(saved_model, [], tester_path, r\"h2.tif\", out_evaluation, scale, nr = 1)\n",
    "predict_only(saved_model, [], tester_path, r\"h3.tif\", out_evaluation, scale, nr = 1)\n",
    "predict_only(saved_model, [], tester_path, r\"h4.tif\", out_evaluation, scale, nr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_path = r\"../tmp_data/Data/\"\n",
    "\n",
    "predict_only(saved_model, [], tester_path, r\"geb_clip_res.png\", out_evaluation, scale, nr = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only for 15k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if scale == 15:\n",
    "    tester_path = r\"../tmp_data/Data/Testing/\"\n",
    "\n",
    "    all_records = []\n",
    "\n",
    "    records = predict_eval(saved_model, [], tester_path, r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\", out_evaluation, scale, nr = 1)\n",
    "    all_records.extend(records)\n",
    "    records = predict_eval(saved_model, [], tester_path, r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\", out_evaluation, scale, nr = 1)\n",
    "    all_records.extend(records)\n",
    "    records = predict_eval(saved_model, [], tester_path, r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\", out_evaluation, scale, nr = 2)\n",
    "    all_records.extend(records)\n",
    "    records = predict_eval(saved_model, [], tester_path, r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\", out_evaluation, scale, nr = 2)\n",
    "    all_records.extend(records)\n",
    "\n",
    "    _records = [(acc * 100, (1-acc) * 100, iou * 100) for acc, iou, conf, report in all_records]\n",
    "\n",
    "    import pandas as pd\n",
    "    df = pd.DataFrame(np.transpose(_records))\n",
    "    df.columns = [\"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 1)\", \n",
    "                  \"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 1)\",\n",
    "                  \"Input vs Target (Test1)_\", \"Prediction vs Target (Test1 - 10)\", \n",
    "                  \"Input vs Target (Test2)_\", \"Prediction vs Target (Test2 - 10)\"]\n",
    "\n",
    "    df = df.rename({0: \"Accuracy\", 1: 'Error', 2: 'IoU'})\n",
    "    df.index.name = 'Metrics'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df[[\"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 1)\", \"Prediction vs Target (Test1 - 10)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    df[[\"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 1)\", \"Prediction vs Target (Test2 - 10)\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"Prediction vs Target (Test1 - 1)\"\n",
    "[key] + df[key].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Input vs Target (Test1)',\n",
    " 97.64130991339968,\n",
    " 2.3586900866003235,\n",
    " 88.5579034522481]\n",
    "['Prediction vs Target (Test1 - 1) - Unet',\n",
    " 98.88473731884058,\n",
    " 1.1152626811594235,\n",
    " 94.52899355698734]\n",
    "['Prediction vs Target (Test1 - 1) - 16Epoch - GAN',\n",
    " 99.01456823671498,\n",
    " 0.9854317632850274,\n",
    " 95.13979375302483]\n",
    "['Prediction vs Target (Test1 - 1) - 22Epoch - GAN',\n",
    " 98.98890398550725,\n",
    " 1.0110960144927517,\n",
    " 95.06629834254143]\n",
    "['Prediction vs Target (Test1 - 1) - 25Epoch - GAN',\n",
    " 99.03872282608695,\n",
    " 0.9612771739130443,\n",
    " 95.27694846737256]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"Prediction vs Target (Test2 - 1)\"\n",
    "[key] + df[key].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "['Input vs Target (Test2)',\n",
    " 98.28963317384371,\n",
    " 1.7103668261562954,\n",
    " 92.61015173858516]\n",
    "\n",
    "['Prediction vs Target (Test2 - 1) - Unet',\n",
    " 99.19322528949544,\n",
    " 0.8067747105045542,\n",
    " 96.47341289456959]\n",
    "\n",
    "['Prediction vs Target (Test2 - 1) - 16Epoch - GAN',\n",
    " 99.36899038461539,\n",
    " 0.6310096153846145,\n",
    " 97.23057288712423]\n",
    "\n",
    "['Prediction vs Target (Test2 - 1) - 22Epoch - GAN',\n",
    " 99.37900641025641,\n",
    " 0.6209935897435903,\n",
    " 97.28860423779025]\n",
    "\n",
    "['Prediction vs Target (Test2 - 1) - 25Epoch - GAN',\n",
    " 99.44427212572374,\n",
    " 0.5557278742762572,\n",
    " 97.56045670519822]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
