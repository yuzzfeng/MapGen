{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "from keras.applications import VGG19\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapgenlib.losses_extend import dice_coef_loss, iou_loss, wasserstein_loss\n",
    "from mapgenlib.res_unit import res_block, decoder, encoder, build_res_unet\n",
    "from mapgenlib.discrimnator import build_discriminator_simple_dc, build_discriminator_critic\n",
    "from mapgenlib.discrimnator import build_discriminator_patchgan_srgan, build_discriminator_patchgan_cycle\n",
    "\n",
    "from functools import partial\n",
    "from keras.layers.merge import _Merge\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    \"\"\"Citation: https://github.com/eriklindernoren/Keras-GAN/blob/master/wgan_gp/wgan_gp.py#L110\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN4MapGen(): # Based on u-net, residual u-net and pix2pix\n",
    "    # Reference: https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "    \n",
    "    def __init__(self, method, batch_size=16):\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        #self.clip_value = 0.01\n",
    "\n",
    "        ## Calculate output shape of D (PatchGAN) better version\n",
    "        #self.patch_size = 32\n",
    "        #self.nb_patches = int((self.img_rows / self.patch_size) * (self.img_cols / self.patch_size))\n",
    "        #self.patch_gan_dim = (self.patch_size, self.patch_size, self.channels)\n",
    "        \n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.method = method\n",
    "        \n",
    "        if method == \"dc\":\n",
    "            self.initial_dc()\n",
    "            self.train_cell = self.train_cell_dc\n",
    "            self.valid_cell = self.valid_cell_dc\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            self.valid = np.ones((batch_size, 1))\n",
    "            self.fake = np.zeros((batch_size, 1))\n",
    "        \"\"\"\n",
    "        if method == \"wgangp\":\n",
    "            self.initial_wgangp()\n",
    "            self.train_cell = self.train_cell_wgangp\n",
    "            self.valid_cell = self.valid_cell_wgangp\n",
    "            \n",
    "            # Adversarial ground truths\n",
    "            self.valid = -np.ones((batch_size, 1))\n",
    "            self.fake =  np.ones((batch_size, 1))\n",
    "        \n",
    "        if method == \"srgan\":\n",
    "            # Calculate output shape of D (PatchGAN)\n",
    "            patch = int(self.img_rows / 2**4)\n",
    "            self.disc_patch = (patch, patch, 1)\n",
    "            \n",
    "            self.initial_srgan()\n",
    "            self.train_cell = self.train_cell_srgan\n",
    "            self.valid_cell = self.valid_cell_srgan\n",
    "            \n",
    "            self.valid = np.ones((batch_size,) + self.disc_patch)\n",
    "            self.fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        if method == \"p2p\":\n",
    "            # Calculate output shape of D (PatchGAN)\n",
    "            patch = int(self.img_rows / 2**4)\n",
    "            self.disc_patch = (patch, patch, 1)\n",
    "            \n",
    "            self.initial_p2p()\n",
    "            self.train_cell = self.train_cell_p2p\n",
    "            self.valid_cell = self.valid_cell_p2p\n",
    "            \n",
    "            self.valid = np.ones((batch_size,) + self.disc_patch)\n",
    "            self.fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"\n",
    "        #optimizer = Adam(0.0002, 0.5) # Original\n",
    "        #optimizer = Adam(0.0001, 0.5) # Original # Latest achieved by 0.00008\n",
    "        #optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # An old version of Pix2pix\n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator() # Old generator from \n",
    "        self.generator = self.build_res_unet_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        #img_A = Input(shape=self.img_shape) # Target\n",
    "        img_B = Input(shape=self.img_shape) # Input\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_simple_dc(self.img_shape)\n",
    "        \n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        #valid = self.discriminator([fake_A, img_B])\n",
    "        #self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        \n",
    "        valid = self.discriminator(fake_A)\n",
    "        self.combined = Model(inputs= img_B, outputs=[valid, fake_A])\n",
    "        \n",
    "        # Original Pix2Pix - low weight for discriminator\n",
    "        self.combined.compile(loss=['mse', 'mae'], #['mse', 'mae'] original\n",
    "                              loss_weights=[1, 100], # [1, 100] original\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" WGAN_GP \n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "    \n",
    "        #Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "    \n",
    "    def initial_wgangp(self):\n",
    "        \n",
    "        # Following parameter and optimizer set as recommended in paper\n",
    "        self.n_critic = 5\n",
    "        optimizer = RMSprop(lr=0.00005)\n",
    "\n",
    "        # Build the generator and critic\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        self.critic = build_discriminator_critic(self.img_shape)\n",
    "        \n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #       for the Critic\n",
    "        #-------------------------------\n",
    "        \n",
    "        # Freeze generator's layers while training critic\n",
    "        self.generator.trainable = False\n",
    "        \n",
    "        # Input images and generate imgs\n",
    "        src_img = Input(shape=self.img_shape)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "        fake_img = self.generator(src_img)\n",
    "        \n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.critic(fake_img)\n",
    "        valid = self.critic(real_img)\n",
    "        \n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.critic(interpolated_img)\n",
    "        \n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss, averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.critic_model = Model(inputs=[real_img, src_img], outputs=[valid, fake, validity_interpolated])\n",
    "        self.critic_model.compile(loss=[wasserstein_loss, wasserstein_loss, partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the critic's layers\n",
    "        self.critic.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Generate images based of noise\n",
    "        fake_img = self.generator(src_img)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.critic(fake_img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(src_img, [valid, fake_img])\n",
    "        self.generator_model.compile(loss=[wasserstein_loss, \"mse\"], optimizer=optimizer)\n",
    "        \n",
    "    def train_cell_wgangp(self, imgs_A, imgs_B, valid, fake):\n",
    "        \n",
    "        dummy = np.zeros((self.batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        \n",
    "        for _ in range(self.n_critic):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "            # Train the critic\n",
    "            d_loss = self.critic_model.train_on_batch([imgs_A, imgs_B], [valid, fake, dummy])\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Generator\n",
    "        # ---------------------\n",
    "        g_loss = self.generator_model.train_on_batch(imgs_B, valid)\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_wgangp(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,1))\n",
    "        t_loss = self.generator_model.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "        return t_loss\n",
    "    WGAN_GP ends\"\"\" \n",
    "    \n",
    "    \n",
    "    \"\"\" Pix2Pix \n",
    "    def initial_p2p(self):\n",
    "        \n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_patchgan_cycle(self.img_shape, self.df)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Input images and generate imgs\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake_A)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs=img_B, outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 100], \n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "        #self.combined.compile(loss=['mse', 'mse'], loss_weights=[1, 100], \n",
    "        #                      optimizer=optimizer, metrics=['accuracy']) # Very bad try!!!\n",
    "        \n",
    "    def train_cell_p2p(self, imgs_A, imgs_B, valid, fake):\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "        d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train the generators\n",
    "        g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_p2p(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,) + self.disc_patch)\n",
    "        t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "        return t_loss\n",
    "    Pix2Pix ends\"\"\" \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" SRGAN \n",
    "    def initial_srgan(self):\n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        # We use a pre-trained VGG19 model to extract image features from the high resolution\n",
    "        # and the generated high resolution images and minimize the mse between them\n",
    "        self.vgg = self.build_vgg()\n",
    "        self.vgg.trainable = False\n",
    "        self.vgg.compile(loss='mse',optimizer=optimizer,metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_patchgan_srgan(self.img_shape, self.df)\n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        self.generator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Input images and generate imgs\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        \n",
    "        # Generate image\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Extract image features of the generated img\n",
    "        fake_features = self.vgg(fake_A)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake_A)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs=[img_B, img_A], outputs=[valid, fake_features])\n",
    "        self.combined.compile(loss=['binary_crossentropy', 'mse'], \n",
    "                              loss_weights=[1, 10], optimizer=optimizer, metrics=['mae'])\n",
    "    \n",
    "    def train_cell_srgan(self, imgs_A, imgs_B, valid, fake):\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "        d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        valid = np.ones((self.batch_size,) + self.disc_patch)\n",
    "        \n",
    "        image_features = self.vgg.predict(imgs_A)\n",
    "        \n",
    "        # Train the generators\n",
    "        g_loss = self.combined.train_on_batch([imgs_B, imgs_A], [valid, image_features])\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_srgan(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,) + self.disc_patch)\n",
    "        t_loss = self.generator.evaluate(x_test_sim, y_test_sim, verbose=0)\n",
    "        return t_loss\n",
    "     SRGAN ends\"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\" DCGAN \"\"\"\n",
    "    def initial_dc(self):\n",
    "        \n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        # Build the generator\n",
    "        self.generator = build_res_unet(self.img_shape)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = build_discriminator_simple_dc(self.img_shape)\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # Input images and generate imgs\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "        \n",
    "        # The discriminator takes generated images as input and determines validity\n",
    "        valid = self.discriminator(fake_A)\n",
    "        \n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains the generator to fool the discriminator\n",
    "        self.combined = Model(inputs=img_B, outputs=[valid, fake_A])\n",
    "        #self.combined.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1, 100], optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        #20190314 15:20\n",
    "        self.combined.compile(loss=['mse', 'mae'], loss_weights=[1, 10], optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "    def train_cell_dc(self, imgs_A, imgs_B, valid, fake):\n",
    "        # Condition on B and generate a translated version\n",
    "        fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "        # Train the discriminators (original images = real / generated = Fake)\n",
    "        d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "        d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "        \n",
    "        # Train the generators\n",
    "        g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "        return d_loss, g_loss\n",
    "    \n",
    "    def valid_cell_dc(self, x_test_sim, y_test_sim):\n",
    "        num_test = len(x_test_sim)\n",
    "        valid_test = np.ones((num_test,1))\n",
    "        t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "        return t_loss\n",
    "    \"\"\" DC ends\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, sample_interval=50, patience = 5):\n",
    "    \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        valid = self.valid\n",
    "        fake  = self.fake\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / self.batch_size)\n",
    "        \n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        valid_acc = []\n",
    "        valid_loss = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, self.batch_size)):\n",
    "            \n",
    "                d_loss, g_loss = self.train_cell(imgs_A, imgs_B, valid, fake)\n",
    "            \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.train_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss)\n",
    "                    print(g_loss)\n",
    "            \n",
    "            if epoch >= 0:\n",
    "                t_loss = self.valid_cell(x_test_sim, y_test_sim)\n",
    "                \n",
    "                if self.method == \"srgan\":\n",
    "                    self.valid_log_sr(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss)\n",
    "                    \n",
    "                    # Change to this if srgan\n",
    "                    valid_loss.append(t_loss[0])\n",
    "                    valid_acc.append(t_loss[1])\n",
    "                else:\n",
    "                    self.valid_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss)\n",
    "                    \n",
    "                    valid_loss.append(t_loss[2])\n",
    "                    valid_acc.append(t_loss[4])\n",
    "                \n",
    "                train_loss.append(g_loss[2])\n",
    "                train_acc.append(g_loss[4])\n",
    "                \n",
    "                waited = len(valid_loss) - 1 - np.argmin(valid_loss)\n",
    "                print('waited for', waited, valid_loss)\n",
    "                    \n",
    "                if waited == 0:\n",
    "                    self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')   \n",
    "                        \n",
    "                if waited > patience:\n",
    "                    break\n",
    "            \n",
    "        return train_acc, train_loss, valid_acc, valid_loss\n",
    "    \n",
    "    def build_vgg(self):\n",
    "        \"\"\"\n",
    "        Builds a pre-trained VGG19 model that outputs image features extracted at the\n",
    "        third block of the model\n",
    "        \"\"\"\n",
    "        vgg = VGG19(weights=\"imagenet\")\n",
    "        # Set outputs to outputs of last conv. layer in block 3\n",
    "        # See architecture at: https://github.com/keras-team/keras/blob/master/keras/applications/vgg19.py\n",
    "        vgg.outputs = [vgg.layers[9].output]\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        imgc = Concatenate()([img, img, img]) \n",
    "\n",
    "        # Extract image features\n",
    "        img_features = vgg(imgc)\n",
    "\n",
    "        return Model(img, img_features)\n",
    "\n",
    "    \n",
    "    def train_log(self, start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss):\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                elapsed_time))    \n",
    "    \n",
    "    def valid_log_sr(self, start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss):\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                elapsed_time))   \n",
    "        print(t_loss)\n",
    "    \n",
    "    def valid_log(self, start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss):\n",
    "        elapsed_time = datetime.datetime.now() - start_time\n",
    "        print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] [Test loss&acc: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                100*t_loss[2], 100*t_loss[3], 100*t_loss[4],\n",
    "                                                                                elapsed_time))    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"p2p\", batch_size = 16)\n",
    "gan.generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the trains (31760, 128, 128, 1)\n",
      "Input Shape of the tests (3528, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "scale = 25\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng_/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the trains', x_train_sim.shape)\n",
    "print('Input Shape of the tests', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  \n",
    "\n",
    "def save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath):    \n",
    "    ### Save history\n",
    "    History1_loss = train_loss\n",
    "    History1_acc = train_acc\n",
    "    History1_val_loss = valid_loss\n",
    "    History1_val_acc = valid_acc\n",
    "\n",
    "    thefile1 = open(outPath + 'History1_loss.txt', 'w')\n",
    "    for item in History1_loss:\n",
    "        thefile1.write(\"%s\\n\" % item)\n",
    "    thefile1.close()\n",
    "\n",
    "    thefile2 = open(outPath + 'History1_acc.txt', 'w')\n",
    "    for item in History1_acc:\n",
    "        thefile2.write(\"%s\\n\" % item)\n",
    "    thefile2.close()\n",
    "\n",
    "    thefile3 = open(outPath + 'History1_val_loss.txt', 'w')\n",
    "    for item in History1_val_loss:\n",
    "        thefile3.write(\"%s\\n\" % item)\n",
    "    thefile3.close()\n",
    "\n",
    "    thefile4 = open(outPath + 'History1_val_acc.txt', 'w')\n",
    "    for item in History1_val_acc:\n",
    "        thefile4.write(\"%s\\n\" % item)\n",
    "    thefile4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residual unet + Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp_results/predictions/U128GAN_2019-03-18 19-28-40_25/\n"
     ]
    }
   ],
   "source": [
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "timestr = strftime(\"U\"+str(p_size_1)+\"GAN_%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)\n",
    "print(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/tmp/MapGen/mapgenlib/res_unit.py:96: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n",
      "  return Model(input=inputs, output=path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 64, 64, 32)        320       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 33, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 17, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 17, 17, 256)       1024      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73984)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 73985     \n",
      "=================================================================\n",
      "Total params: 463,617\n",
      "Trainable params: 462,721\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50-0/1985] [D loss&acc: 3.803, 25.000%] [G loss&accA&accB: 41.833, 100.000%, 71.786%] time: 0:00:12.877561\n",
      "[4.1931343, 0.009802915, 0.41833317, 1.0, 0.717865]\n",
      "[Epoch 0/50-500/1985] [D loss&acc: 0.938, 46.875%] [G loss&accA&accB: 2.055, 25.000%, 97.953%] time: 0:01:50.670639\n",
      "[0.5865929, 0.3810957, 0.020549722, 0.25, 0.9795265]\n",
      "[Epoch 0/50-1000/1985] [D loss&acc: 0.925, 40.625%] [G loss&accA&accB: 1.642, 50.000%, 98.361%] time: 0:03:28.886927\n",
      "[0.46902764, 0.30478173, 0.016424589, 0.5, 0.98360825]\n",
      "[Epoch 0/50-1500/1985] [D loss&acc: 0.787, 46.875%] [G loss&accA&accB: 2.600, 43.750%, 97.402%] time: 0:05:06.649814\n",
      "[0.5980893, 0.3380429, 0.026004635, 0.4375, 0.9740181]\n",
      "[Epoch 0/50-1983/1985] [D loss&acc: 0.766, 53.125%] [G loss&accA&accB: 1.658, 37.500%, 98.344%] [Test loss&acc: 2.035, 90.023%, 97.967%] time: 0:06:51.986650\n",
      "waited for 0 [0.020348111822221286]\n",
      "[Epoch 1/50-0/1985] [D loss&acc: 0.856, 37.500%] [G loss&accA&accB: 1.786, 37.500%, 98.216%] time: 0:06:56.797116\n",
      "[0.48274645, 0.3041027, 0.017864376, 0.375, 0.9821625]\n",
      "[Epoch 1/50-500/1985] [D loss&acc: 0.752, 56.250%] [G loss&accA&accB: 2.272, 37.500%, 97.728%] time: 0:08:32.711064\n",
      "[0.5169369, 0.28973857, 0.022719832, 0.375, 0.9772835]\n",
      "[Epoch 1/50-1000/1985] [D loss&acc: 0.856, 43.750%] [G loss&accA&accB: 2.872, 50.000%, 97.130%] time: 0:10:08.010456\n",
      "[0.5989394, 0.31178612, 0.028715327, 0.5, 0.9712982]\n",
      "[Epoch 1/50-1500/1985] [D loss&acc: 0.373, 84.375%] [G loss&accA&accB: 1.488, 100.000%, 98.521%] time: 0:11:43.100363\n",
      "[0.15363668, 0.0047960454, 0.014884063, 1.0, 0.98521423]\n",
      "[Epoch 1/50-1983/1985] [D loss&acc: 0.748, 46.875%] [G loss&accA&accB: 2.248, 43.750%, 97.752%] [Test loss&acc: 2.033, 87.217%, 97.977%] time: 0:13:25.051085\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963]\n",
      "[Epoch 2/50-0/1985] [D loss&acc: 0.721, 40.625%] [G loss&accA&accB: 1.718, 37.500%, 98.290%] time: 0:13:25.636072\n",
      "[0.44526052, 0.27343088, 0.017182963, 0.375, 0.9828987]\n",
      "[Epoch 2/50-500/1985] [D loss&acc: 0.908, 43.750%] [G loss&accA&accB: 3.579, 37.500%, 96.426%] time: 0:15:00.901826\n",
      "[0.68506265, 0.32718298, 0.03578797, 0.375, 0.9642601]\n",
      "[Epoch 2/50-1000/1985] [D loss&acc: 0.802, 50.000%] [G loss&accA&accB: 1.100, 43.750%, 98.907%] time: 0:16:36.051785\n",
      "[0.38812226, 0.2781282, 0.010999404, 0.4375, 0.9890671]\n",
      "[Epoch 2/50-1500/1985] [D loss&acc: 0.813, 37.500%] [G loss&accA&accB: 2.029, 68.750%, 97.974%] time: 0:18:11.128800\n",
      "[0.44841135, 0.24554756, 0.020286378, 0.6875, 0.9797363]\n",
      "[Epoch 2/50-1983/1985] [D loss&acc: 0.743, 53.125%] [G loss&accA&accB: 1.869, 75.000%, 98.133%] [Test loss&acc: 2.020, 87.868%, 97.980%] time: 0:19:52.463409\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782]\n",
      "[Epoch 3/50-0/1985] [D loss&acc: 0.766, 62.500%] [G loss&accA&accB: 2.596, 37.500%, 97.404%] time: 0:19:53.062558\n",
      "[0.5218446, 0.26224267, 0.025960196, 0.375, 0.974041]\n",
      "[Epoch 3/50-500/1985] [D loss&acc: 0.683, 46.875%] [G loss&accA&accB: 2.981, 75.000%, 97.041%] time: 0:21:27.674626\n",
      "[0.52185583, 0.22375661, 0.029809926, 0.75, 0.9704056]\n",
      "[Epoch 3/50-1000/1985] [D loss&acc: 0.781, 53.125%] [G loss&accA&accB: 1.886, 56.250%, 98.118%] time: 0:23:02.152091\n",
      "[0.4760136, 0.2874453, 0.01885683, 0.5625, 0.9811821]\n",
      "[Epoch 3/50-1500/1985] [D loss&acc: 0.899, 56.250%] [G loss&accA&accB: 2.185, 62.500%, 97.815%] time: 0:24:36.681827\n",
      "[0.512586, 0.29412258, 0.02184634, 0.625, 0.9781532]\n",
      "[Epoch 3/50-1983/1985] [D loss&acc: 0.688, 46.875%] [G loss&accA&accB: 1.542, 25.000%, 98.458%] [Test loss&acc: 2.067, 72.959%, 97.936%] time: 0:26:17.336487\n",
      "waited for 1 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135]\n",
      "[Epoch 4/50-0/1985] [D loss&acc: 0.748, 46.875%] [G loss&accA&accB: 1.690, 56.250%, 98.314%] time: 0:26:17.525614\n",
      "[0.42389733, 0.25486594, 0.01690314, 0.5625, 0.9831352]\n",
      "[Epoch 4/50-500/1985] [D loss&acc: 0.768, 37.500%] [G loss&accA&accB: 0.905, 62.500%, 99.094%] time: 0:27:51.425716\n",
      "[0.30204624, 0.21151659, 0.009052966, 0.625, 0.9909401]\n",
      "[Epoch 4/50-1000/1985] [D loss&acc: 0.797, 37.500%] [G loss&accA&accB: 2.738, 37.500%, 97.272%] time: 0:29:25.554811\n",
      "[0.553496, 0.27972963, 0.027376635, 0.375, 0.9727173]\n",
      "[Epoch 4/50-1500/1985] [D loss&acc: 0.749, 43.750%] [G loss&accA&accB: 2.212, 31.250%, 97.791%] time: 0:30:59.504688\n",
      "[0.513153, 0.2919615, 0.022119153, 0.3125, 0.9779053]\n",
      "[Epoch 4/50-1983/1985] [D loss&acc: 0.744, 59.375%] [G loss&accA&accB: 1.926, 75.000%, 98.074%] [Test loss&acc: 2.705, 64.484%, 97.298%] time: 0:32:39.819093\n",
      "waited for 2 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729]\n",
      "[Epoch 5/50-0/1985] [D loss&acc: 0.731, 46.875%] [G loss&accA&accB: 2.284, 31.250%, 97.717%] time: 0:32:40.017391\n",
      "[0.5785592, 0.35013187, 0.022842737, 0.3125, 0.97717285]\n",
      "[Epoch 5/50-500/1985] [D loss&acc: 0.669, 56.250%] [G loss&accA&accB: 1.435, 37.500%, 98.568%] time: 0:34:13.903556\n",
      "[0.41913518, 0.2756435, 0.014349169, 0.375, 0.98568344]\n",
      "[Epoch 5/50-1000/1985] [D loss&acc: 0.717, 53.125%] [G loss&accA&accB: 2.244, 43.750%, 97.757%] time: 0:35:47.845899\n",
      "[0.49340296, 0.26904345, 0.022435952, 0.4375, 0.9775734]\n",
      "[Epoch 5/50-1500/1985] [D loss&acc: 0.695, 62.500%] [G loss&accA&accB: 1.914, 75.000%, 98.088%] time: 0:37:21.600849\n",
      "[0.37032068, 0.17889088, 0.01914298, 0.75, 0.98088455]\n",
      "[Epoch 5/50-1983/1985] [D loss&acc: 0.726, 40.625%] [G loss&accA&accB: 1.514, 56.250%, 98.488%] [Test loss&acc: 1.889, 87.783%, 98.112%] time: 0:39:01.995304\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151]\n",
      "[Epoch 6/50-0/1985] [D loss&acc: 0.703, 53.125%] [G loss&accA&accB: 1.941, 37.500%, 98.059%] time: 0:39:02.576983\n",
      "[0.52154094, 0.32742444, 0.019411651, 0.375, 0.980587]\n",
      "[Epoch 6/50-500/1985] [D loss&acc: 0.711, 59.375%] [G loss&accA&accB: 1.404, 25.000%, 98.598%] time: 0:40:36.214583\n",
      "[0.43609598, 0.2956834, 0.014041256, 0.25, 0.985981]\n",
      "[Epoch 6/50-1000/1985] [D loss&acc: 0.713, 56.250%] [G loss&accA&accB: 2.114, 56.250%, 97.887%] time: 0:42:09.866261\n",
      "[0.45261163, 0.24119999, 0.021141166, 0.5625, 0.9788742]\n",
      "[Epoch 6/50-1500/1985] [D loss&acc: 0.761, 43.750%] [G loss&accA&accB: 1.235, 68.750%, 98.762%] time: 0:43:43.303450\n",
      "[0.3470506, 0.22350113, 0.012354947, 0.6875, 0.9876213]\n",
      "[Epoch 6/50-1983/1985] [D loss&acc: 0.742, 43.750%] [G loss&accA&accB: 1.423, 68.750%, 98.577%] [Test loss&acc: 1.853, 94.218%, 98.148%] time: 0:45:23.495566\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374]\n",
      "[Epoch 7/50-0/1985] [D loss&acc: 0.724, 50.000%] [G loss&accA&accB: 2.188, 43.750%, 97.812%] time: 0:45:24.108468\n",
      "[0.4943522, 0.27555394, 0.021879826, 0.4375, 0.9781151]\n",
      "[Epoch 7/50-500/1985] [D loss&acc: 0.784, 43.750%] [G loss&accA&accB: 1.570, 50.000%, 98.431%] time: 0:46:57.850765\n",
      "[0.4132049, 0.25621384, 0.015699107, 0.5, 0.98431396]\n",
      "[Epoch 7/50-1000/1985] [D loss&acc: 0.726, 46.875%] [G loss&accA&accB: 2.123, 43.750%, 97.877%] time: 0:48:31.713400\n",
      "[0.56111944, 0.3487916, 0.021232782, 0.4375, 0.9787712]\n",
      "[Epoch 7/50-1500/1985] [D loss&acc: 0.703, 50.000%] [G loss&accA&accB: 1.608, 18.750%, 98.394%] time: 0:50:05.203712\n",
      "[0.47815067, 0.3173704, 0.016078027, 0.1875, 0.9839401]\n",
      "[Epoch 7/50-1983/1985] [D loss&acc: 0.738, 43.750%] [G loss&accA&accB: 1.653, 93.750%, 98.345%] [Test loss&acc: 1.955, 64.342%, 98.069%] time: 0:51:45.242477\n",
      "waited for 1 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983]\n",
      "[Epoch 8/50-0/1985] [D loss&acc: 0.739, 43.750%] [G loss&accA&accB: 2.043, 62.500%, 97.956%] time: 0:51:45.437166\n",
      "[0.4726205, 0.26827818, 0.02043423, 0.625, 0.97956085]\n",
      "[Epoch 8/50-500/1985] [D loss&acc: 0.692, 53.125%] [G loss&accA&accB: 1.875, 37.500%, 98.125%] time: 0:53:18.971411\n",
      "[0.4965413, 0.30900347, 0.018753782, 0.375, 0.9812546]\n",
      "[Epoch 8/50-1000/1985] [D loss&acc: 0.765, 43.750%] [G loss&accA&accB: 1.512, 75.000%, 98.488%] time: 0:54:52.559022\n",
      "[0.34346172, 0.19224682, 0.01512149, 0.75, 0.98487854]\n",
      "[Epoch 8/50-1500/1985] [D loss&acc: 0.799, 53.125%] [G loss&accA&accB: 2.078, 68.750%, 97.922%] time: 0:56:26.059480\n",
      "[0.4317731, 0.2239275, 0.02078456, 0.6875, 0.97922134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 8/50-1983/1985] [D loss&acc: 0.702, 43.750%] [G loss&accA&accB: 0.906, 37.500%, 99.095%] [Test loss&acc: 1.814, 84.892%, 98.188%] time: 0:58:06.277830\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441]\n",
      "[Epoch 9/50-0/1985] [D loss&acc: 0.745, 50.000%] [G loss&accA&accB: 2.437, 43.750%, 97.565%] time: 0:58:06.876169\n",
      "[0.49497324, 0.25124335, 0.02437299, 0.4375, 0.975647]\n",
      "[Epoch 9/50-500/1985] [D loss&acc: 0.746, 43.750%] [G loss&accA&accB: 1.548, 50.000%, 98.453%] time: 0:59:40.408105\n",
      "[0.38152996, 0.22674058, 0.015478938, 0.5, 0.9845276]\n",
      "[Epoch 9/50-1000/1985] [D loss&acc: 0.674, 46.875%] [G loss&accA&accB: 1.379, 68.750%, 98.621%] time: 1:01:14.004195\n",
      "[0.38054746, 0.24265698, 0.01378905, 0.6875, 0.9862137]\n",
      "[Epoch 9/50-1500/1985] [D loss&acc: 0.744, 53.125%] [G loss&accA&accB: 0.955, 56.250%, 99.044%] time: 1:02:47.332491\n",
      "[0.34116194, 0.2456244, 0.009553755, 0.5625, 0.99044037]\n",
      "[Epoch 9/50-1983/1985] [D loss&acc: 0.727, 40.625%] [G loss&accA&accB: 1.927, 75.000%, 98.073%] [Test loss&acc: 1.768, 94.841%, 98.232%] time: 1:04:27.433063\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164]\n",
      "[Epoch 10/50-0/1985] [D loss&acc: 0.706, 46.875%] [G loss&accA&accB: 1.085, 56.250%, 98.915%] time: 1:04:28.017702\n",
      "[0.3695281, 0.26104984, 0.0108478265, 0.5625, 0.989151]\n",
      "[Epoch 10/50-500/1985] [D loss&acc: 0.738, 50.000%] [G loss&accA&accB: 1.587, 50.000%, 98.412%] time: 1:06:01.582336\n",
      "[0.4203003, 0.2615813, 0.015871901, 0.5, 0.9841194]\n",
      "[Epoch 10/50-1000/1985] [D loss&acc: 0.712, 46.875%] [G loss&accA&accB: 2.203, 37.500%, 97.799%] time: 1:07:34.907933\n",
      "[0.5001262, 0.27981442, 0.022031177, 0.375, 0.9779854]\n",
      "[Epoch 10/50-1500/1985] [D loss&acc: 0.743, 37.500%] [G loss&accA&accB: 1.938, 50.000%, 98.065%] time: 1:09:08.312301\n",
      "[0.44155115, 0.24772397, 0.019382717, 0.5, 0.98064804]\n",
      "[Epoch 10/50-1983/1985] [D loss&acc: 0.688, 53.125%] [G loss&accA&accB: 2.413, 25.000%, 97.588%] [Test loss&acc: 1.699, 94.870%, 98.301%] time: 1:10:48.260272\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518]\n",
      "[Epoch 11/50-0/1985] [D loss&acc: 0.770, 43.750%] [G loss&accA&accB: 2.093, 56.250%, 97.905%] time: 1:10:48.858570\n",
      "[0.48004815, 0.27073777, 0.020931039, 0.5625, 0.9790535]\n",
      "[Epoch 11/50-500/1985] [D loss&acc: 0.669, 56.250%] [G loss&accA&accB: 1.338, 56.250%, 98.664%] time: 1:12:21.997492\n",
      "[0.38675663, 0.25294858, 0.013380805, 0.5625, 0.9866371]\n",
      "[Epoch 11/50-1000/1985] [D loss&acc: 0.749, 37.500%] [G loss&accA&accB: 1.474, 62.500%, 98.525%] time: 1:13:55.453644\n",
      "[0.39085358, 0.2435013, 0.014735229, 0.625, 0.98524857]\n",
      "[Epoch 11/50-1500/1985] [D loss&acc: 0.713, 53.125%] [G loss&accA&accB: 1.451, 43.750%, 98.549%] time: 1:15:29.017445\n",
      "[0.4157077, 0.2705954, 0.0145112295, 0.4375, 0.9854927]\n",
      "[Epoch 11/50-1983/1985] [D loss&acc: 0.694, 43.750%] [G loss&accA&accB: 1.597, 62.500%, 98.403%] [Test loss&acc: 1.671, 92.375%, 98.329%] time: 1:17:09.305922\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772]\n",
      "[Epoch 12/50-0/1985] [D loss&acc: 0.697, 50.000%] [G loss&accA&accB: 1.322, 43.750%, 98.679%] time: 1:17:09.899716\n",
      "[0.4024188, 0.27025765, 0.013216114, 0.4375, 0.9867897]\n",
      "[Epoch 12/50-500/1985] [D loss&acc: 0.767, 43.750%] [G loss&accA&accB: 1.124, 6.250%, 98.876%] time: 1:18:43.477867\n",
      "[0.4425031, 0.33007848, 0.01124246, 0.0625, 0.9887619]\n",
      "[Epoch 12/50-1000/1985] [D loss&acc: 0.711, 56.250%] [G loss&accA&accB: 1.773, 25.000%, 98.229%] time: 1:20:16.963723\n",
      "[0.45802438, 0.28072992, 0.017729446, 0.25, 0.9822922]\n",
      "[Epoch 12/50-1500/1985] [D loss&acc: 0.721, 43.750%] [G loss&accA&accB: 2.865, 43.750%, 97.137%] time: 1:21:50.566624\n",
      "[0.53009737, 0.24358968, 0.028650768, 0.4375, 0.9713669]\n",
      "[Epoch 12/50-1983/1985] [D loss&acc: 0.690, 50.000%] [G loss&accA&accB: 1.698, 31.250%, 98.303%] [Test loss&acc: 1.657, 88.946%, 98.343%] time: 1:23:30.489337\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792]\n",
      "[Epoch 13/50-0/1985] [D loss&acc: 0.726, 53.125%] [G loss&accA&accB: 1.731, 18.750%, 98.270%] time: 1:23:31.097061\n",
      "[0.45443282, 0.28128666, 0.017314617, 0.1875, 0.98270035]\n",
      "[Epoch 13/50-500/1985] [D loss&acc: 0.738, 50.000%] [G loss&accA&accB: 0.701, 25.000%, 99.300%] time: 1:25:04.489009\n",
      "[0.36146927, 0.29136914, 0.0070100143, 0.25, 0.99300385]\n",
      "[Epoch 13/50-1000/1985] [D loss&acc: 0.764, 34.375%] [G loss&accA&accB: 1.462, 68.750%, 98.539%] time: 1:26:37.844792\n",
      "[0.3894038, 0.24315962, 0.014624417, 0.6875, 0.9853935]\n",
      "[Epoch 13/50-1500/1985] [D loss&acc: 0.669, 68.750%] [G loss&accA&accB: 1.539, 43.750%, 98.462%] time: 1:28:11.071983\n",
      "[0.43227232, 0.27835852, 0.015391381, 0.4375, 0.98461914]\n",
      "[Epoch 13/50-1983/1985] [D loss&acc: 0.719, 40.625%] [G loss&accA&accB: 1.083, 50.000%, 98.917%] [Test loss&acc: 1.636, 88.379%, 98.365%] time: 1:29:50.776497\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827]\n",
      "[Epoch 14/50-0/1985] [D loss&acc: 0.712, 53.125%] [G loss&accA&accB: 2.670, 37.500%, 97.330%] time: 1:29:51.334628\n",
      "[0.51905537, 0.25206473, 0.02669906, 0.375, 0.9732971]\n",
      "[Epoch 14/50-500/1985] [D loss&acc: 0.753, 37.500%] [G loss&accA&accB: 1.226, 50.000%, 98.775%] time: 1:31:24.368583\n",
      "[0.392182, 0.26958275, 0.012259926, 0.5, 0.9877472]\n",
      "[Epoch 14/50-1000/1985] [D loss&acc: 0.707, 40.625%] [G loss&accA&accB: 0.811, 56.250%, 99.189%] time: 1:32:57.822634\n",
      "[0.3040384, 0.22291273, 0.008112569, 0.5625, 0.99188995]\n",
      "[Epoch 14/50-1500/1985] [D loss&acc: 0.706, 34.375%] [G loss&accA&accB: 1.794, 68.750%, 98.206%] time: 1:34:30.699713\n",
      "[0.41285536, 0.23347043, 0.017938493, 0.6875, 0.9820595]\n",
      "[Epoch 14/50-1983/1985] [D loss&acc: 0.721, 43.750%] [G loss&accA&accB: 0.675, 50.000%, 99.324%] [Test loss&acc: 1.576, 11.593%, 98.425%] time: 1:36:10.234365\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797]\n",
      "[Epoch 15/50-0/1985] [D loss&acc: 0.709, 59.375%] [G loss&accA&accB: 0.592, 31.250%, 99.409%] time: 1:36:10.800795\n",
      "[0.31735831, 0.258188, 0.0059170313, 0.3125, 0.99409103]\n",
      "[Epoch 15/50-500/1985] [D loss&acc: 0.724, 43.750%] [G loss&accA&accB: 1.334, 56.250%, 98.666%] time: 1:37:43.652288\n",
      "[0.3634284, 0.23007181, 0.013335658, 0.5625, 0.9866638]\n",
      "[Epoch 15/50-1000/1985] [D loss&acc: 0.657, 56.250%] [G loss&accA&accB: 1.112, 68.750%, 98.888%] time: 1:39:16.324567\n",
      "[0.35016906, 0.23894256, 0.01112265, 0.6875, 0.98888016]\n",
      "[Epoch 15/50-1500/1985] [D loss&acc: 0.751, 37.500%] [G loss&accA&accB: 1.257, 56.250%, 98.743%] time: 1:40:49.272839\n",
      "[0.38194004, 0.25625002, 0.012569001, 0.5625, 0.9874344]\n",
      "[Epoch 15/50-1983/1985] [D loss&acc: 0.677, 53.125%] [G loss&accA&accB: 2.881, 75.000%, 97.121%] [Test loss&acc: 1.596, 96.514%, 98.404%] time: 1:42:28.798743\n",
      "waited for 1 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686]\n",
      "[Epoch 16/50-0/1985] [D loss&acc: 0.679, 68.750%] [G loss&accA&accB: 0.943, 43.750%, 99.059%] time: 1:42:28.995077\n",
      "[0.36045986, 0.2661873, 0.009427257, 0.4375, 0.99059296]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 16/50-500/1985] [D loss&acc: 0.701, 53.125%] [G loss&accA&accB: 1.479, 37.500%, 98.521%] time: 1:44:02.033825\n",
      "[0.41506165, 0.26714462, 0.014791704, 0.375, 0.9852066]\n",
      "[Epoch 16/50-1000/1985] [D loss&acc: 0.718, 50.000%] [G loss&accA&accB: 1.430, 37.500%, 98.570%] time: 1:45:35.040956\n",
      "[0.41070044, 0.26770988, 0.014299057, 0.375, 0.9856987]\n",
      "[Epoch 16/50-1500/1985] [D loss&acc: 0.738, 31.250%] [G loss&accA&accB: 1.668, 62.500%, 98.334%] time: 1:47:07.976175\n",
      "[0.3951096, 0.22826755, 0.016684206, 0.625, 0.9833412]\n",
      "[Epoch 16/50-1983/1985] [D loss&acc: 0.688, 53.125%] [G loss&accA&accB: 1.396, 56.250%, 98.604%] [Test loss&acc: 1.566, 82.596%, 98.434%] time: 1:48:47.298223\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435]\n",
      "[Epoch 17/50-0/1985] [D loss&acc: 0.691, 62.500%] [G loss&accA&accB: 1.670, 43.750%, 98.331%] time: 1:48:47.883387\n",
      "[0.40913236, 0.24212529, 0.016700706, 0.4375, 0.9833107]\n",
      "[Epoch 17/50-500/1985] [D loss&acc: 0.724, 46.875%] [G loss&accA&accB: 1.530, 37.500%, 98.471%] time: 1:50:20.848416\n",
      "[0.41594455, 0.26293454, 0.015301, 0.375, 0.9847145]\n",
      "[Epoch 17/50-1000/1985] [D loss&acc: 0.678, 59.375%] [G loss&accA&accB: 1.383, 43.750%, 98.617%] time: 1:51:53.522454\n",
      "[0.4092666, 0.27097863, 0.013828797, 0.4375, 0.9861717]\n",
      "[Epoch 17/50-1983/1985] [D loss&acc: 0.736, 46.875%] [G loss&accA&accB: 2.048, 43.750%, 97.953%] [Test loss&acc: 1.530, 94.048%, 98.470%] time: 1:55:05.820986\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536]\n",
      "[Epoch 18/50-0/1985] [D loss&acc: 0.657, 50.000%] [G loss&accA&accB: 1.529, 37.500%, 98.471%] time: 1:55:06.427092\n",
      "[0.4193492, 0.26643407, 0.01529151, 0.375, 0.9847145]\n",
      "[Epoch 18/50-500/1985] [D loss&acc: 0.695, 43.750%] [G loss&accA&accB: 1.477, 31.250%, 98.523%] time: 1:56:39.395783\n",
      "[0.41633296, 0.26859438, 0.014773859, 0.3125, 0.9852295]\n",
      "[Epoch 18/50-1000/1985] [D loss&acc: 0.730, 34.375%] [G loss&accA&accB: 2.576, 56.250%, 97.421%] time: 1:58:12.367653\n",
      "[0.5152181, 0.25757253, 0.025764555, 0.5625, 0.97420883]\n",
      "[Epoch 18/50-1500/1985] [D loss&acc: 0.683, 65.625%] [G loss&accA&accB: 0.812, 50.000%, 99.188%] time: 1:59:45.304710\n",
      "[0.3190031, 0.23781055, 0.008119257, 0.5, 0.9918823]\n",
      "[Epoch 18/50-1983/1985] [D loss&acc: 0.737, 46.875%] [G loss&accA&accB: 1.094, 50.000%, 98.906%] [Test loss&acc: 1.570, 48.101%, 98.430%] time: 2:01:24.792602\n",
      "waited for 1 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993]\n",
      "[Epoch 19/50-0/1985] [D loss&acc: 0.726, 40.625%] [G loss&accA&accB: 1.527, 56.250%, 98.473%] time: 2:01:24.988031\n",
      "[0.39366856, 0.24096334, 0.015270522, 0.5625, 0.98472977]\n",
      "[Epoch 19/50-500/1985] [D loss&acc: 0.719, 46.875%] [G loss&accA&accB: 2.016, 37.500%, 97.984%] time: 2:02:58.094582\n",
      "[0.47251573, 0.27096117, 0.020155456, 0.375, 0.97984314]\n",
      "[Epoch 19/50-1000/1985] [D loss&acc: 0.719, 50.000%] [G loss&accA&accB: 1.616, 25.000%, 98.384%] time: 2:04:31.097151\n",
      "[0.4863848, 0.3247583, 0.01616265, 0.25, 0.98384094]\n",
      "[Epoch 19/50-1500/1985] [D loss&acc: 0.667, 59.375%] [G loss&accA&accB: 2.323, 62.500%, 97.678%] time: 2:06:04.025381\n",
      "[0.46098787, 0.22867987, 0.023230799, 0.625, 0.9767761]\n",
      "[Epoch 19/50-1983/1985] [D loss&acc: 0.664, 68.750%] [G loss&accA&accB: 1.099, 68.750%, 98.901%] [Test loss&acc: 1.517, 95.181%, 98.484%] time: 2:07:43.244364\n",
      "waited for 0 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309]\n",
      "[Epoch 20/50-0/1985] [D loss&acc: 0.718, 40.625%] [G loss&accA&accB: 1.447, 56.250%, 98.553%] time: 2:07:43.835514\n",
      "[0.4015513, 0.25684774, 0.014470357, 0.5625, 0.98553467]\n",
      "[Epoch 20/50-500/1985] [D loss&acc: 0.692, 59.375%] [G loss&accA&accB: 0.917, 43.750%, 99.083%] time: 2:09:16.881193\n",
      "[0.34893143, 0.257224, 0.009170743, 0.4375, 0.99082565]\n",
      "[Epoch 20/50-1000/1985] [D loss&acc: 0.700, 56.250%] [G loss&accA&accB: 2.632, 37.500%, 97.370%] time: 2:10:49.740535\n",
      "[0.52108663, 0.2579112, 0.026317544, 0.375, 0.97369766]\n",
      "[Epoch 20/50-1500/1985] [D loss&acc: 0.693, 50.000%] [G loss&accA&accB: 2.481, 43.750%, 97.520%] time: 2:12:22.652173\n",
      "[0.50225484, 0.25417513, 0.024807971, 0.4375, 0.97520447]\n",
      "[Epoch 20/50-1983/1985] [D loss&acc: 0.706, 43.750%] [G loss&accA&accB: 3.146, 50.000%, 96.853%] [Test loss&acc: 1.558, 98.810%, 98.442%] time: 2:14:01.845114\n",
      "waited for 1 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309, 0.015584487750991131]\n",
      "[Epoch 21/50-0/1985] [D loss&acc: 0.691, 62.500%] [G loss&accA&accB: 1.189, 62.500%, 98.810%] time: 2:14:02.034081\n",
      "[0.36828166, 0.2493928, 0.011888889, 0.625, 0.98810196]\n",
      "[Epoch 21/50-500/1985] [D loss&acc: 0.705, 50.000%] [G loss&accA&accB: 1.475, 43.750%, 98.525%] time: 2:15:34.893758\n",
      "[0.42701778, 0.2795267, 0.014749106, 0.4375, 0.98524857]\n",
      "[Epoch 21/50-1000/1985] [D loss&acc: 0.716, 37.500%] [G loss&accA&accB: 2.269, 62.500%, 97.731%] time: 2:17:07.784877\n",
      "[0.4875183, 0.2605734, 0.02269449, 0.625, 0.97730637]\n",
      "[Epoch 21/50-1500/1985] [D loss&acc: 0.694, 46.875%] [G loss&accA&accB: 1.218, 50.000%, 98.782%] time: 2:18:40.625034\n",
      "[0.36909926, 0.24727494, 0.012182433, 0.5, 0.9878235]\n",
      "[Epoch 21/50-1983/1985] [D loss&acc: 0.689, 56.250%] [G loss&accA&accB: 1.448, 56.250%, 98.553%] [Test loss&acc: 1.558, 53.600%, 98.442%] time: 2:20:19.914764\n",
      "waited for 2 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309, 0.015584487750991131, 0.015584104261498333]\n",
      "[Epoch 22/50-0/1985] [D loss&acc: 0.695, 62.500%] [G loss&accA&accB: 2.238, 50.000%, 97.762%] time: 2:20:20.111390\n",
      "[0.47446567, 0.2506743, 0.022379138, 0.5, 0.97761536]\n",
      "[Epoch 22/50-500/1985] [D loss&acc: 0.667, 53.125%] [G loss&accA&accB: 1.849, 37.500%, 98.153%] time: 2:21:52.904582\n",
      "[0.46334547, 0.27845767, 0.018488782, 0.375, 0.98153305]\n",
      "[Epoch 22/50-1000/1985] [D loss&acc: 0.675, 53.125%] [G loss&accA&accB: 1.664, 56.250%, 98.337%] time: 2:23:25.872677\n",
      "[0.42533588, 0.25898388, 0.0166352, 0.5625, 0.98337173]\n",
      "[Epoch 22/50-1500/1985] [D loss&acc: 0.688, 53.125%] [G loss&accA&accB: 2.244, 37.500%, 97.758%] time: 2:24:58.603476\n",
      "[0.47373354, 0.24934149, 0.022439204, 0.375, 0.9775772]\n",
      "[Epoch 22/50-1983/1985] [D loss&acc: 0.735, 43.750%] [G loss&accA&accB: 0.424, 43.750%, 99.576%] [Test loss&acc: 1.765, 85.573%, 98.236%] time: 2:26:38.017975\n",
      "waited for 3 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309, 0.015584487750991131, 0.015584104261498333, 0.017647435208634724]\n",
      "[Epoch 23/50-0/1985] [D loss&acc: 0.664, 68.750%] [G loss&accA&accB: 1.473, 43.750%, 98.526%] time: 2:26:38.206741\n",
      "[0.408729, 0.26138628, 0.014734272, 0.4375, 0.9852638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 23/50-500/1985] [D loss&acc: 0.689, 46.875%] [G loss&accA&accB: 1.723, 43.750%, 98.274%] time: 2:28:10.873305\n",
      "[0.4245352, 0.25219187, 0.017234333, 0.4375, 0.9827423]\n",
      "[Epoch 23/50-1000/1985] [D loss&acc: 0.687, 46.875%] [G loss&accA&accB: 1.301, 37.500%, 98.699%] time: 2:29:43.938413\n",
      "[0.40013108, 0.26999763, 0.013013344, 0.375, 0.9869919]\n",
      "[Epoch 23/50-1500/1985] [D loss&acc: 0.721, 50.000%] [G loss&accA&accB: 1.095, 43.750%, 98.906%] time: 2:31:16.799640\n",
      "[0.35880446, 0.2492646, 0.010953987, 0.4375, 0.98905563]\n",
      "[Epoch 23/50-1983/1985] [D loss&acc: 0.719, 40.625%] [G loss&accA&accB: 1.025, 50.000%, 98.974%] [Test loss&acc: 1.677, 98.781%, 98.323%] time: 2:32:56.102719\n",
      "waited for 4 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309, 0.015584487750991131, 0.015584104261498333, 0.017647435208634724, 0.0167715185556282]\n",
      "[Epoch 24/50-0/1985] [D loss&acc: 0.692, 50.000%] [G loss&accA&accB: 1.734, 31.250%, 98.268%] time: 2:32:56.290317\n",
      "[0.4489253, 0.27550507, 0.017342022, 0.3125, 0.9826813]\n",
      "[Epoch 24/50-500/1985] [D loss&acc: 0.714, 50.000%] [G loss&accA&accB: 2.054, 62.500%, 97.945%] time: 2:34:29.177208\n",
      "[0.45841, 0.2529619, 0.020544808, 0.625, 0.97945404]\n",
      "[Epoch 24/50-1000/1985] [D loss&acc: 0.688, 62.500%] [G loss&accA&accB: 0.839, 43.750%, 99.163%] time: 2:36:02.171706\n",
      "[0.34851986, 0.26465988, 0.008385999, 0.4375, 0.99162674]\n",
      "[Epoch 24/50-1500/1985] [D loss&acc: 0.676, 65.625%] [G loss&accA&accB: 1.310, 25.000%, 98.689%] time: 2:37:35.110955\n",
      "[0.40506715, 0.2740176, 0.013104955, 0.25, 0.9868927]\n",
      "[Epoch 24/50-1983/1985] [D loss&acc: 0.683, 53.125%] [G loss&accA&accB: 1.434, 50.000%, 98.568%] [Test loss&acc: 1.549, 90.675%, 98.451%] time: 2:39:14.140544\n",
      "waited for 5 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309, 0.015584487750991131, 0.015584104261498333, 0.017647435208634724, 0.0167715185556282, 0.015492290929973531]\n",
      "[Epoch 25/50-0/1985] [D loss&acc: 0.706, 53.125%] [G loss&accA&accB: 1.485, 50.000%, 98.517%] time: 2:39:14.330211\n",
      "[0.41104135, 0.26255348, 0.014848786, 0.5, 0.98516846]\n",
      "[Epoch 25/50-500/1985] [D loss&acc: 0.727, 40.625%] [G loss&accA&accB: 1.503, 37.500%, 98.498%] time: 2:40:47.074148\n",
      "[0.42742646, 0.27709547, 0.0150331, 0.375, 0.9849777]\n",
      "[Epoch 25/50-1000/1985] [D loss&acc: 0.694, 56.250%] [G loss&accA&accB: 0.718, 56.250%, 99.283%] time: 2:42:19.771751\n",
      "[0.31472182, 0.24294333, 0.007177849, 0.5625, 0.9928322]\n",
      "[Epoch 25/50-1500/1985] [D loss&acc: 0.699, 43.750%] [G loss&accA&accB: 0.840, 56.250%, 99.159%] time: 2:43:52.711671\n",
      "[0.32842618, 0.24441718, 0.008400902, 0.5625, 0.9915886]\n",
      "[Epoch 25/50-1983/1985] [D loss&acc: 0.669, 62.500%] [G loss&accA&accB: 1.897, 31.250%, 98.103%] [Test loss&acc: 1.558, 95.777%, 98.443%] time: 2:45:32.054684\n",
      "waited for 6 [0.020348111822221286, 0.020326752697123963, 0.020202090165444782, 0.020665434533371135, 0.02705035775122729, 0.01888645693761151, 0.01852817222256374, 0.019551664075545983, 0.01813785691667441, 0.017684137452892164, 0.01699220614672518, 0.01670867692501772, 0.016573811068184792, 0.01635577854047827, 0.01575726416392797, 0.015963409349423686, 0.0156595710994435, 0.015300782959139536, 0.01570105412446993, 0.015165217085625309, 0.015584487750991131, 0.015584104261498333, 0.017647435208634724, 0.0167715185556282, 0.015492290929973531, 0.015577343345330416]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN4MapGen(\"dc\", batch_size = 16)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"wgangp\", batch_size = 16)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"srgan\", batch_size = 2)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=500, patience = 4)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(\"p2p\", batch_size = 1)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, \n",
    "                                                         outPath, epochs=50, sample_interval=1000, patience = 15)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "a,b,c = gan.combined.predict([x_test_sim, y_test_sim])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tloss = gan.valid_cell_srgan( x_test_sim, y_test_sim)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "tloss."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan.generator.predict(x_test_sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen()\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs=50, batch_size=16, sample_interval=500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,1))\n",
    "        fake = -1 * np.ones((batch_size,1))\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        valid_acc = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        patience = 5\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch(imgs_A, valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch(fake_A, fake)\n",
    "                #d_loss_real = self.discriminator.train_on_batch(imgs_B, valid)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                #self.discriminator.trainable = False\n",
    "                g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "                #self.discriminator.trainable = True\n",
    "                \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.train_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss)\n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch >= 0:\n",
    "\n",
    "                valid_test = np.ones((len(x_test_sim),1))\n",
    "                t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "                    \n",
    "                self.valid_log(start_time, epoch, epochs, batch_i, n_batches, d_loss, g_loss, t_loss)\n",
    "                    \n",
    "                train_loss.append(g_loss[2])\n",
    "                train_acc.append(g_loss[4])\n",
    "                valid_loss.append(t_loss[2])\n",
    "                valid_acc.append(t_loss[4])\n",
    "\n",
    "                waited = len(valid_loss) - 1 - np.argmin(valid_loss)\n",
    "                print('waited for', waited, valid_loss)\n",
    "                    \n",
    "                if waited == 0:\n",
    "                    self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')   \n",
    "                        \n",
    "                if waited > patience:\n",
    "                    break\n",
    "            \n",
    "        return train_acc, train_loss, valid_acc, valid_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
