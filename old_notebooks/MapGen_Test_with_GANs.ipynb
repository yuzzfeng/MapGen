{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading stored image patches \n"
     ]
    }
   ],
   "source": [
    "# load image patch arrays\n",
    "print(\"loading stored image patches \")        \n",
    "x_train_sim = np.load(\"x_train_sim.npy\")\n",
    "y_train_sim = np.load(\"y_train_sim.npy\")\n",
    "x_test_sim = np.load(\"x_test_sim.npy\")\n",
    "y_test_sim = np.load(\"y_test_sim.npy\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#    p_size_1 = 128\n",
    "    plt.subplot(321)\n",
    "    plt.imshow(np.reshape(x_train_sim[1], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "    plt.subplot(322)\n",
    "    plt.imshow(np.reshape(y_train_sim[1], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "\n",
    "    plt.subplot(323)\n",
    "    plt.imshow(np.reshape(x_train_sim[6], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "    plt.subplot(324)\n",
    "    plt.imshow(np.reshape(y_train_sim[6], (p_size_1,p_size_1)),cmap=plt.cm.gray)    \n",
    "\n",
    "    print(\"figure plotted\")\n",
    "\n",
    "    plt.subplot(325)\n",
    "    plt.imshow(np.reshape(x_train_sim[10], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "    plt.subplot(326)\n",
    "    plt.imshow(np.reshape(y_train_sim[10], (p_size_1,p_size_1)),cmap=plt.cm.gray)    \n",
    "\n",
    "    print(\"figure plotted\")\n",
    "        \n",
    "    input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "    print('Input Shape of the models', x_train_sim.shape)\n",
    "    print('Input Shape of the weights', y_test_sim.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print('Input Shape of the models', x_train_sim.shape)\n",
    "print('Input Shape of the models', y_train_sim.shape)\n",
    "\n",
    "def load_batch(batch_size=2, is_testing=False):\n",
    "\n",
    "\n",
    "        self.n_batches = int(13318 / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure plotted\n",
      "figure plotted\n",
      "Input Shape of the models (13318, 128, 128, 1)\n",
      "Input Shape of the weights (1000, 128, 128, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7ae2be7780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_examples(x_train_sim, y_train_sim):\n",
    "    \n",
    "    p_size_1 = 128\n",
    "    plt.subplot(321)\n",
    "    plt.imshow(np.reshape(x_train_sim[0], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "    plt.subplot(322)\n",
    "    plt.imshow(np.reshape(y_train_sim[0], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "\n",
    "    plt.subplot(323)\n",
    "    plt.imshow(np.reshape(x_train_sim[1], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "    plt.subplot(324)\n",
    "    plt.imshow(np.reshape(y_train_sim[1], (p_size_1,p_size_1)),cmap=plt.cm.gray)    \n",
    "\n",
    "    print(\"figure plotted\")\n",
    "\n",
    "    plt.subplot(325)\n",
    "    plt.imshow(np.reshape(x_train_sim[2], (p_size_1,p_size_1)),cmap=plt.cm.gray)\n",
    "    plt.subplot(326)\n",
    "    plt.imshow(np.reshape(y_train_sim[2], (p_size_1,p_size_1)),cmap=plt.cm.gray)    \n",
    "\n",
    "    print(\"figure plotted\")\n",
    "        \n",
    "    input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "    print('Input Shape of the models',  x_train_sim.shape)\n",
    "    print('Input Shape of the weights', y_test_sim.shape)\n",
    "    \n",
    "\n",
    "#y_train_sim = np.ones(y_train_sim.shape) - y_train_sim\n",
    "show_examples(x_train_sim, y_train_sim)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "figure plotted\n",
      "figure plotted\n",
      "Input Shape of the models (1000, 128, 128, 1)\n",
      "Input Shape of the weights (1000, 128, 128, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f7aeab20940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#y_test_sim = np.ones(y_test_sim.shape) - y_test_sim\n",
    "show_examples(x_test_sim, y_test_sim)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size=2):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "    print(batch_i, (imgs_A.shape, imgs_B.shape))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset_name, img_res=(128, 128)):\n",
    "        self.img_res = img_res\n",
    "\n",
    "    def load_data(self, domain, batch_size=1, is_testing=False):\n",
    "        data_type = \"train%s\" % domain if not is_testing else \"test%s\" % domain\n",
    "        path = glob('./datasets/%s/%s/*' % (self.dataset_name, data_type))\n",
    "\n",
    "        batch_images = np.random.choice(path, size=batch_size)\n",
    "\n",
    "        imgs = []\n",
    "        for img_path in batch_images:\n",
    "            img = self.imread(img_path)\n",
    "            if not is_testing:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "\n",
    "                if np.random.random() > 0.5:\n",
    "                    img = np.fliplr(img)\n",
    "            else:\n",
    "                img = scipy.misc.imresize(img, self.img_res)\n",
    "            imgs.append(img)\n",
    "\n",
    "        imgs = np.array(imgs)/127.5 - 1.\n",
    "\n",
    "        return imgs\n",
    "\n",
    "    def load_batch(self, batch_size=1, is_testing=False):\n",
    "        data_type = \"train\" if not is_testing else \"val\"\n",
    "        path_A = glob('./datasets/%s/%sA/*' % (self.dataset_name, data_type))\n",
    "        path_B = glob('./datasets/%s/%sB/*' % (self.dataset_name, data_type))\n",
    "\n",
    "        self.n_batches = int(min(len(path_A), len(path_B)) / batch_size)\n",
    "        total_samples = self.n_batches * batch_size\n",
    "\n",
    "        # Sample n_batches * batch_size from each path list so that model sees all\n",
    "        # samples from both domains\n",
    "        path_A = np.random.choice(path_A, total_samples, replace=False)\n",
    "        path_B = np.random.choice(path_B, total_samples, replace=False)\n",
    "\n",
    "        for i in range(self.n_batches-1):\n",
    "            batch_A = path_A[i*batch_size:(i+1)*batch_size]\n",
    "            batch_B = path_B[i*batch_size:(i+1)*batch_size]\n",
    "            imgs_A, imgs_B = [], []\n",
    "            for img_A, img_B in zip(batch_A, batch_B):\n",
    "                img_A = self.imread(img_A)\n",
    "                img_B = self.imread(img_B)\n",
    "\n",
    "                img_A = scipy.misc.imresize(img_A, self.img_res)\n",
    "                img_B = scipy.misc.imresize(img_B, self.img_res)\n",
    "\n",
    "                if not is_testing and np.random.random() > 0.5:\n",
    "                        img_A = np.fliplr(img_A)\n",
    "                        img_B = np.fliplr(img_B)\n",
    "\n",
    "                imgs_A.append(img_A)\n",
    "                imgs_B.append(img_B)\n",
    "\n",
    "            imgs_A = np.array(imgs_A)/127.5 - 1.\n",
    "            imgs_B = np.array(imgs_B)/127.5 - 1.\n",
    "\n",
    "            yield imgs_A, imgs_B\n",
    "\n",
    "    def load_img(self, path):\n",
    "        img = self.imread(path)\n",
    "        img = scipy.misc.imresize(img, self.img_res)\n",
    "        img = img/127.5 - 1.\n",
    "        return img[np.newaxis, :, :, :]\n",
    "\n",
    "    def imread(self, path):\n",
    "        return scipy.misc.imread(path, mode='RGB').astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen'\n",
    "        #self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "        #                              img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 32\n",
    "        self.df = 64\n",
    "\n",
    "        # Loss weights\n",
    "        self.lambda_cycle = 10.0                    # Cycle-consistency loss\n",
    "        self.lambda_id = 0.1 * self.lambda_cycle    # Identity loss\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "        # Identity mapping of images\n",
    "        img_A_id = self.g_BA(img_A)\n",
    "        img_B_id = self.g_AB(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Combined model trains generators to fool discriminators\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        reconstr_A, reconstr_B,\n",
    "                                        img_A_id, img_B_id ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mae', 'mae'],\n",
    "                            loss_weights=[  1, 1,\n",
    "                                            self.lambda_cycle, self.lambda_cycle,\n",
    "                                            self.lambda_id, self.lambda_id ],\n",
    "                            optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            #u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        \n",
    "        d4 = Dropout(0.1)(d4)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d4, d3, self.gf*4)\n",
    "        u2 = deconv2d(u1, d2, self.gf*2)\n",
    "        u3 = deconv2d(u2, d1, self.gf)\n",
    "\n",
    "        u4 = UpSampling2D(size=2)(u3)\n",
    "        output_img = Conv2D(self.channels, kernel_size=3, strides=1, padding='same', activation='sigmoid')(u4)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "            #for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B],\n",
    "                                                        [valid, valid,\n",
    "                                                        imgs_A, imgs_B,\n",
    "                                                        imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                \n",
    "                # Plot the progress\n",
    "                if batch_i % 20 == 0:\n",
    "                    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %05f, adv: %05f, recon: %05f, id: %05f] time: %s \" \\\n",
    "                                                                            % ( epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                g_loss[0],\n",
    "                                                                                np.mean(g_loss[1:3]),\n",
    "                                                                                np.mean(g_loss[3:5]),\n",
    "                                                                                np.mean(g_loss[5:6]),\n",
    "                                                                                elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3 # Original at github\n",
    "        p_size_1 = 128\n",
    "        \n",
    "        imgs_A = x_test_sim[[2]]\n",
    "        imgs_B = y_test_sim[[2]]\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A)\n",
    "        fake_A = self.g_BA.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                \n",
    "                #axs[i,j].imshow(gen_imgs[cnt])\n",
    "                \n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i), \n",
    "                    format='png', transparent=True, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class DiscoGAN():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen_disco'\n",
    "        #self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "        #                              img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminators\n",
    "        self.d_A = self.build_discriminator()\n",
    "        self.d_B = self.build_discriminator()\n",
    "        self.d_A.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "        self.d_B.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generators\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generators\n",
    "        self.g_AB = self.build_generator()\n",
    "        self.g_BA = self.build_generator()\n",
    "\n",
    "        # Input images from both domains\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB(img_A)\n",
    "        fake_A = self.g_BA(img_B)\n",
    "        # Translate images back to original domain\n",
    "        reconstr_A = self.g_BA(fake_B)\n",
    "        reconstr_B = self.g_AB(fake_A)\n",
    "\n",
    "        # For the combined model we will only train the generators\n",
    "        self.d_A.trainable = False\n",
    "        self.d_B.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images\n",
    "        valid_A = self.d_A(fake_A)\n",
    "        valid_B = self.d_B(fake_B)\n",
    "\n",
    "        # Objectives\n",
    "        # + Adversarial: Fool domain discriminators\n",
    "        # + Translation: Minimize MAE between e.g. fake B and true B\n",
    "        # + Cycle-consistency: Minimize MAE between reconstructed images and original\n",
    "        self.combined = Model(inputs=[img_A, img_B],\n",
    "                              outputs=[ valid_A, valid_B,\n",
    "                                        fake_B, fake_A,\n",
    "                                        reconstr_A, reconstr_B ])\n",
    "        self.combined.compile(loss=['mse', 'mse',\n",
    "                                    'mae', 'mae',\n",
    "                                    'mae', 'mae'],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, normalize=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalize:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = InstanceNormalization()(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, normalize=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1,\n",
    "                            padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = InstanceNormalization()(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=128, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "        \n",
    "        #for epoch in range(epochs):\n",
    "\n",
    "            #for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                # ----------------------\n",
    "                #  Train Discriminators\n",
    "                # ----------------------\n",
    "\n",
    "                # Translate images to opposite domain\n",
    "                fake_B = self.g_AB.predict(imgs_A)\n",
    "                fake_A = self.g_BA.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / translated = Fake)\n",
    "                dA_loss_real = self.d_A.train_on_batch(imgs_A, valid)\n",
    "                dA_loss_fake = self.d_A.train_on_batch(fake_A, fake)\n",
    "                dA_loss = 0.5 * np.add(dA_loss_real, dA_loss_fake)\n",
    "\n",
    "                dB_loss_real = self.d_B.train_on_batch(imgs_B, valid)\n",
    "                dB_loss_fake = self.d_B.train_on_batch(fake_B, fake)\n",
    "                dB_loss = 0.5 * np.add(dB_loss_real, dB_loss_fake)\n",
    "\n",
    "                # Total disciminator loss\n",
    "                d_loss = 0.5 * np.add(dA_loss, dB_loss)\n",
    "\n",
    "                # ------------------\n",
    "                #  Train Generators\n",
    "                # ------------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, valid, \\\n",
    "                                                                         imgs_B, imgs_A, \\\n",
    "                                                                         imgs_A, imgs_B])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                # Plot the progress\n",
    "                if batch_i % 20 == 0:\n",
    "                    # Plot the progress\n",
    "                    print (\"[%d] [%d/%d] time: %s, [d_loss: %f, g_loss: %f]\" % (epoch, batch_i,\n",
    "                                                                            n_batches,\n",
    "                                                                            elapsed_time,\n",
    "                                                                            d_loss[0], g_loss[0]))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 2, 3 # Original at github\n",
    "        p_size_1 = 128\n",
    "        \n",
    "        imgs_A = x_test_sim[[2]]\n",
    "        imgs_B = y_test_sim[[2]]\n",
    "\n",
    "        # Demo (for GIF)\n",
    "        #imgs_A = self.data_loader.load_img('datasets/apple2orange/testA/n07740461_1541.jpg')\n",
    "        #imgs_B = self.data_loader.load_img('datasets/apple2orange/testB/n07749192_4241.jpg')\n",
    "\n",
    "        # Translate images to the other domain\n",
    "        fake_B = self.g_AB.predict(imgs_A)\n",
    "        fake_A = self.g_BA.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = self.g_BA.predict(fake_B)\n",
    "        reconstr_B = self.g_AB.predict(fake_A)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                \n",
    "                #axs[i,j].imshow(gen_imgs[cnt])\n",
    "                \n",
    "                axs[i, j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i), \n",
    "                    format='png', transparent=True, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "class Pix2Pix():\n",
    "    def __init__(self):\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen_pix2pix'\n",
    "        #self.data_loader = DataLoader(dataset_name=self.dataset_name,\n",
    "        #                              img_res=(self.img_rows, self.img_cols))\n",
    "\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "\n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        valid = self.discriminator([fake_A, img_B])\n",
    "\n",
    "        self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100],\n",
    "                              optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        # Concatenate image and conditioning image by channels to produce input\n",
    "        combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A, img_B], validity)\n",
    "\n",
    "    def train(self, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "                \n",
    "        #for epoch in range(epochs):\n",
    "        #    for batch_i, (imgs_A, imgs_B) in enumerate(self.data_loader.load_batch(batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "\n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "\n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "                \n",
    "                # Plot the progress\n",
    "                if batch_i % 20 == 0:\n",
    "                    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %3d%%] [G loss: %f] time: %s\" % (epoch, epochs,\n",
    "                                                                            batch_i, n_batches,\n",
    "                                                                            d_loss[0], 100*d_loss[1],\n",
    "                                                                            g_loss[0],\n",
    "                                                                            elapsed_time))\n",
    "\n",
    "                # If at save interval => save generated image samples\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    self.sample_images(epoch, batch_i)\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch, batch_i):\n",
    "        os.makedirs('images/%s' % self.dataset_name, exist_ok=True)\n",
    "        r, c = 3, 3\n",
    "        p_size_1 = 128\n",
    "        \n",
    "        imgs_A = y_test_sim[[0, 2, 3]]\n",
    "        imgs_B = x_test_sim[[0, 2, 3]]\n",
    "        \n",
    "        #imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n",
    "        fake_A = gan.generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Generated', 'Target']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                \n",
    "                #axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/%s/%d_%d.png\" % (self.dataset_name, epoch, batch_i),\n",
    "                   format='png', transparent=True, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/20] [Batch 0/832] [D loss: 7.870375, acc:  12%] [G loss: 64.371910] time: 0:00:36.721544\n",
      "[Epoch 0/20] [Batch 20/832] [D loss: 0.457253, acc:  47%] [G loss: 19.006721] time: 0:00:42.103269\n",
      "[Epoch 0/20] [Batch 40/832] [D loss: 0.297263, acc:  63%] [G loss: 16.023399] time: 0:00:46.835354\n",
      "[Epoch 0/20] [Batch 60/832] [D loss: 0.222060, acc:  68%] [G loss: 8.699268] time: 0:00:51.529475\n",
      "[Epoch 0/20] [Batch 80/832] [D loss: 0.257987, acc:  71%] [G loss: 4.944899] time: 0:00:56.192739\n",
      "[Epoch 0/20] [Batch 100/832] [D loss: 0.653818, acc:  66%] [G loss: 6.067722] time: 0:01:00.918885\n",
      "[Epoch 0/20] [Batch 120/832] [D loss: 0.262467, acc:  63%] [G loss: 7.106416] time: 0:01:06.273493\n",
      "[Epoch 0/20] [Batch 140/832] [D loss: 0.243340, acc:  58%] [G loss: 6.153952] time: 0:01:10.990335\n",
      "[Epoch 0/20] [Batch 160/832] [D loss: 0.522646, acc:  53%] [G loss: 9.050786] time: 0:01:15.688035\n",
      "[Epoch 0/20] [Batch 180/832] [D loss: 0.255223, acc:  62%] [G loss: 4.654811] time: 0:01:20.370161\n",
      "[Epoch 0/20] [Batch 200/832] [D loss: 0.208252, acc:  72%] [G loss: 4.749042] time: 0:01:25.087658\n",
      "[Epoch 0/20] [Batch 220/832] [D loss: 0.429353, acc:  35%] [G loss: 6.016152] time: 0:01:30.391981\n",
      "[Epoch 0/20] [Batch 240/832] [D loss: 0.303532, acc:  50%] [G loss: 5.560665] time: 0:01:35.138428\n",
      "[Epoch 0/20] [Batch 260/832] [D loss: 0.263480, acc:  51%] [G loss: 8.048349] time: 0:01:39.856525\n",
      "[Epoch 0/20] [Batch 280/832] [D loss: 0.258684, acc:  55%] [G loss: 5.034837] time: 0:01:44.594515\n",
      "[Epoch 0/20] [Batch 300/832] [D loss: 0.236277, acc:  62%] [G loss: 8.159453] time: 0:01:49.307118\n",
      "[Epoch 0/20] [Batch 320/832] [D loss: 0.318879, acc:  42%] [G loss: 6.670147] time: 0:01:54.687150\n",
      "[Epoch 0/20] [Batch 340/832] [D loss: 0.431992, acc:  64%] [G loss: 6.103582] time: 0:01:59.401404\n",
      "[Epoch 0/20] [Batch 360/832] [D loss: 0.263775, acc:  65%] [G loss: 4.524057] time: 0:02:04.151053\n",
      "[Epoch 0/20] [Batch 380/832] [D loss: 0.244505, acc:  59%] [G loss: 6.559028] time: 0:02:08.860322\n",
      "[Epoch 0/20] [Batch 400/832] [D loss: 0.316138, acc:  52%] [G loss: 3.955742] time: 0:02:13.580923\n",
      "[Epoch 0/20] [Batch 420/832] [D loss: 0.223177, acc:  66%] [G loss: 4.527837] time: 0:02:18.978010\n",
      "[Epoch 0/20] [Batch 440/832] [D loss: 0.274112, acc:  53%] [G loss: 4.839036] time: 0:02:23.696354\n",
      "[Epoch 0/20] [Batch 460/832] [D loss: 0.191335, acc:  75%] [G loss: 4.352562] time: 0:02:28.431838\n",
      "[Epoch 0/20] [Batch 480/832] [D loss: 0.286339, acc:  51%] [G loss: 3.106225] time: 0:02:33.158330\n",
      "[Epoch 0/20] [Batch 500/832] [D loss: 0.113836, acc:  88%] [G loss: 4.268525] time: 0:02:37.882492\n",
      "[Epoch 0/20] [Batch 520/832] [D loss: 0.075627, acc:  92%] [G loss: 6.348475] time: 0:02:43.290986\n",
      "[Epoch 0/20] [Batch 540/832] [D loss: 0.221371, acc:  66%] [G loss: 3.340463] time: 0:02:48.024054\n",
      "[Epoch 0/20] [Batch 560/832] [D loss: 0.381567, acc:  42%] [G loss: 5.130920] time: 0:02:52.771117\n",
      "[Epoch 0/20] [Batch 580/832] [D loss: 0.245914, acc:  67%] [G loss: 4.454350] time: 0:02:57.473752\n",
      "[Epoch 0/20] [Batch 600/832] [D loss: 0.462475, acc:  35%] [G loss: 6.723816] time: 0:03:02.206065\n",
      "[Epoch 0/20] [Batch 620/832] [D loss: 0.235714, acc:  64%] [G loss: 7.014801] time: 0:03:07.583187\n",
      "[Epoch 0/20] [Batch 640/832] [D loss: 0.198630, acc:  73%] [G loss: 5.111669] time: 0:03:12.305395\n",
      "[Epoch 0/20] [Batch 660/832] [D loss: 0.036012, acc:  97%] [G loss: 8.016106] time: 0:03:17.036044\n",
      "[Epoch 0/20] [Batch 680/832] [D loss: 0.184958, acc:  81%] [G loss: 5.115481] time: 0:03:21.774829\n",
      "[Epoch 0/20] [Batch 700/832] [D loss: 0.206045, acc:  71%] [G loss: 4.544554] time: 0:03:26.493648\n",
      "[Epoch 0/20] [Batch 720/832] [D loss: 0.116017, acc:  88%] [G loss: 5.731284] time: 0:03:31.812769\n",
      "[Epoch 0/20] [Batch 740/832] [D loss: 0.160729, acc:  80%] [G loss: 3.366436] time: 0:03:36.549292\n",
      "[Epoch 0/20] [Batch 760/832] [D loss: 0.043557, acc:  98%] [G loss: 4.425310] time: 0:03:41.281666\n",
      "[Epoch 0/20] [Batch 780/832] [D loss: 0.297160, acc:  52%] [G loss: 4.953235] time: 0:03:46.000703\n",
      "[Epoch 0/20] [Batch 800/832] [D loss: 0.215567, acc:  70%] [G loss: 8.786192] time: 0:03:50.726023\n",
      "[Epoch 0/20] [Batch 820/832] [D loss: 0.218559, acc:  77%] [G loss: 3.684267] time: 0:03:56.092210\n",
      "[Epoch 1/20] [Batch 0/832] [D loss: 0.049519, acc:  96%] [G loss: 8.460331] time: 0:03:58.698615\n",
      "[Epoch 1/20] [Batch 20/832] [D loss: 0.051332, acc:  96%] [G loss: 6.689686] time: 0:04:04.070190\n",
      "[Epoch 1/20] [Batch 40/832] [D loss: 0.055319, acc:  96%] [G loss: 5.860325] time: 0:04:08.786554\n",
      "[Epoch 1/20] [Batch 60/832] [D loss: 0.216024, acc:  72%] [G loss: 3.897783] time: 0:04:13.498450\n",
      "[Epoch 1/20] [Batch 80/832] [D loss: 0.109880, acc:  86%] [G loss: 4.855868] time: 0:04:18.237333\n",
      "[Epoch 1/20] [Batch 100/832] [D loss: 0.095646, acc:  89%] [G loss: 6.435910] time: 0:04:22.983427\n",
      "[Epoch 1/20] [Batch 120/832] [D loss: 0.046868, acc:  97%] [G loss: 6.003064] time: 0:04:28.357984\n",
      "[Epoch 1/20] [Batch 140/832] [D loss: 0.042619, acc:  97%] [G loss: 7.163069] time: 0:04:33.099103\n",
      "[Epoch 1/20] [Batch 160/832] [D loss: 0.341272, acc:  55%] [G loss: 5.271385] time: 0:04:37.839515\n",
      "[Epoch 1/20] [Batch 180/832] [D loss: 0.153490, acc:  79%] [G loss: 6.880702] time: 0:04:42.552586\n",
      "[Epoch 1/20] [Batch 200/832] [D loss: 0.031987, acc:  99%] [G loss: 5.405714] time: 0:04:47.281020\n",
      "[Epoch 1/20] [Batch 220/832] [D loss: 0.029889, acc:  99%] [G loss: 4.704172] time: 0:04:52.552945\n",
      "[Epoch 1/20] [Batch 240/832] [D loss: 0.201861, acc:  69%] [G loss: 4.170726] time: 0:04:57.291791\n",
      "[Epoch 1/20] [Batch 260/832] [D loss: 0.242067, acc:  53%] [G loss: 5.068550] time: 0:05:02.013107\n",
      "[Epoch 1/20] [Batch 280/832] [D loss: 0.025109, acc:  99%] [G loss: 6.806179] time: 0:05:06.762918\n",
      "[Epoch 1/20] [Batch 300/832] [D loss: 0.036087, acc:  99%] [G loss: 6.307057] time: 0:05:11.494751\n",
      "[Epoch 1/20] [Batch 320/832] [D loss: 0.207034, acc:  81%] [G loss: 4.233139] time: 0:05:16.868425\n",
      "[Epoch 1/20] [Batch 340/832] [D loss: 0.181410, acc:  68%] [G loss: 8.676469] time: 0:05:21.587531\n",
      "[Epoch 1/20] [Batch 360/832] [D loss: 0.049575, acc:  98%] [G loss: 4.890771] time: 0:05:26.330050\n",
      "[Epoch 1/20] [Batch 380/832] [D loss: 0.089868, acc:  91%] [G loss: 3.860642] time: 0:05:31.100798\n",
      "[Epoch 1/20] [Batch 400/832] [D loss: 0.073920, acc:  94%] [G loss: 6.497205] time: 0:05:35.803630\n",
      "[Epoch 1/20] [Batch 420/832] [D loss: 0.023121, acc:  99%] [G loss: 5.912514] time: 0:05:41.218498\n",
      "[Epoch 1/20] [Batch 440/832] [D loss: 0.440154, acc:  69%] [G loss: 4.040258] time: 0:05:45.932718\n",
      "[Epoch 1/20] [Batch 460/832] [D loss: 0.037097, acc:  99%] [G loss: 5.894375] time: 0:05:50.665453\n",
      "[Epoch 1/20] [Batch 480/832] [D loss: 0.070079, acc:  94%] [G loss: 6.786180] time: 0:05:55.392300\n",
      "[Epoch 1/20] [Batch 500/832] [D loss: 0.050567, acc:  98%] [G loss: 5.728910] time: 0:06:00.140750\n",
      "[Epoch 1/20] [Batch 520/832] [D loss: 0.078976, acc:  92%] [G loss: 6.524405] time: 0:06:05.469661\n",
      "[Epoch 1/20] [Batch 540/832] [D loss: 0.030701, acc:  97%] [G loss: 2.563033] time: 0:06:10.195869\n",
      "[Epoch 1/20] [Batch 560/832] [D loss: 0.128366, acc:  90%] [G loss: 4.908399] time: 0:06:14.900998\n",
      "[Epoch 1/20] [Batch 580/832] [D loss: 0.032265, acc:  98%] [G loss: 4.629943] time: 0:06:19.608398\n",
      "[Epoch 1/20] [Batch 600/832] [D loss: 0.014110, acc:  99%] [G loss: 5.265912] time: 0:06:24.307315\n",
      "[Epoch 1/20] [Batch 620/832] [D loss: 0.010904, acc:  99%] [G loss: 7.445070] time: 0:06:29.627835\n",
      "[Epoch 1/20] [Batch 640/832] [D loss: 0.090817, acc:  91%] [G loss: 2.882973] time: 0:06:34.382660\n",
      "[Epoch 1/20] [Batch 660/832] [D loss: 0.229805, acc:  69%] [G loss: 4.538099] time: 0:06:39.115351\n",
      "[Epoch 1/20] [Batch 680/832] [D loss: 0.174020, acc:  72%] [G loss: 5.675460] time: 0:06:43.845704\n",
      "[Epoch 1/20] [Batch 700/832] [D loss: 0.049140, acc:  96%] [G loss: 4.813382] time: 0:06:48.580379\n",
      "[Epoch 1/20] [Batch 720/832] [D loss: 0.022512, acc:  99%] [G loss: 6.417286] time: 0:06:53.931432\n",
      "[Epoch 1/20] [Batch 740/832] [D loss: 0.027384, acc:  99%] [G loss: 4.900811] time: 0:06:58.664970\n",
      "[Epoch 1/20] [Batch 760/832] [D loss: 0.149619, acc:  82%] [G loss: 3.607897] time: 0:07:03.419142\n",
      "[Epoch 1/20] [Batch 780/832] [D loss: 0.043111, acc:  98%] [G loss: 3.589197] time: 0:07:08.162405\n",
      "[Epoch 1/20] [Batch 800/832] [D loss: 0.056144, acc:  95%] [G loss: 5.744596] time: 0:07:12.898428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] [Batch 820/832] [D loss: 0.032941, acc:  98%] [G loss: 8.059990] time: 0:07:18.298779\n",
      "[Epoch 2/20] [Batch 0/832] [D loss: 0.041371, acc:  99%] [G loss: 3.240099] time: 0:07:20.933789\n",
      "[Epoch 2/20] [Batch 20/832] [D loss: 0.283128, acc:  69%] [G loss: 4.786171] time: 0:07:26.368196\n",
      "[Epoch 2/20] [Batch 40/832] [D loss: 0.015290, acc: 100%] [G loss: 3.611000] time: 0:07:31.157771\n",
      "[Epoch 2/20] [Batch 60/832] [D loss: 0.159652, acc:  77%] [G loss: 5.683451] time: 0:07:35.936112\n",
      "[Epoch 2/20] [Batch 80/832] [D loss: 0.054482, acc:  96%] [G loss: 5.780769] time: 0:07:40.703910\n",
      "[Epoch 2/20] [Batch 100/832] [D loss: 0.018447, acc:  99%] [G loss: 5.948878] time: 0:07:45.468577\n",
      "[Epoch 2/20] [Batch 120/832] [D loss: 0.024382, acc:  99%] [G loss: 6.815408] time: 0:07:50.828165\n",
      "[Epoch 2/20] [Batch 140/832] [D loss: 0.019887, acc:  99%] [G loss: 4.957947] time: 0:07:55.538269\n",
      "[Epoch 2/20] [Batch 160/832] [D loss: 0.038373, acc:  97%] [G loss: 5.940123] time: 0:08:00.283649\n",
      "[Epoch 2/20] [Batch 180/832] [D loss: 0.026929, acc:  99%] [G loss: 8.131242] time: 0:08:05.015548\n",
      "[Epoch 2/20] [Batch 200/832] [D loss: 0.014843, acc: 100%] [G loss: 4.405965] time: 0:08:09.743097\n",
      "[Epoch 2/20] [Batch 220/832] [D loss: 0.136181, acc:  83%] [G loss: 6.978172] time: 0:08:15.132676\n",
      "[Epoch 2/20] [Batch 240/832] [D loss: 0.079238, acc:  91%] [G loss: 3.562616] time: 0:08:19.915900\n",
      "[Epoch 2/20] [Batch 260/832] [D loss: 0.070447, acc:  95%] [G loss: 5.670635] time: 0:08:24.693629\n",
      "[Epoch 2/20] [Batch 280/832] [D loss: 0.018904, acc:  99%] [G loss: 6.982851] time: 0:08:29.449526\n",
      "[Epoch 2/20] [Batch 300/832] [D loss: 0.086847, acc:  90%] [G loss: 5.733577] time: 0:08:34.216581\n",
      "[Epoch 2/20] [Batch 320/832] [D loss: 0.035343, acc:  99%] [G loss: 5.277720] time: 0:08:39.567721\n",
      "[Epoch 2/20] [Batch 340/832] [D loss: 0.015169, acc: 100%] [G loss: 6.223784] time: 0:08:44.282214\n",
      "[Epoch 2/20] [Batch 360/832] [D loss: 0.018107, acc: 100%] [G loss: 3.829898] time: 0:08:49.038259\n",
      "[Epoch 2/20] [Batch 380/832] [D loss: 0.064565, acc:  96%] [G loss: 4.910382] time: 0:08:53.760795\n",
      "[Epoch 2/20] [Batch 400/832] [D loss: 0.032782, acc:  99%] [G loss: 7.142311] time: 0:08:58.493534\n",
      "[Epoch 2/20] [Batch 420/832] [D loss: 0.023006, acc:  99%] [G loss: 6.060633] time: 0:09:03.853494\n",
      "[Epoch 2/20] [Batch 440/832] [D loss: 0.017417, acc:  99%] [G loss: 6.501096] time: 0:09:08.561964\n",
      "[Epoch 2/20] [Batch 460/832] [D loss: 0.034429, acc:  98%] [G loss: 4.626886] time: 0:09:13.282411\n",
      "[Epoch 2/20] [Batch 480/832] [D loss: 0.162807, acc:  71%] [G loss: 5.092091] time: 0:09:17.998672\n",
      "[Epoch 2/20] [Batch 500/832] [D loss: 0.075331, acc:  93%] [G loss: 4.981497] time: 0:09:22.742980\n",
      "[Epoch 2/20] [Batch 520/832] [D loss: 0.031190, acc:  98%] [G loss: 4.237452] time: 0:09:28.143756\n",
      "[Epoch 2/20] [Batch 540/832] [D loss: 0.012142, acc:  99%] [G loss: 6.200469] time: 0:09:32.849025\n",
      "[Epoch 2/20] [Batch 560/832] [D loss: 0.072466, acc:  92%] [G loss: 5.467977] time: 0:09:37.592896\n",
      "[Epoch 2/20] [Batch 580/832] [D loss: 0.020437, acc:  99%] [G loss: 3.978318] time: 0:09:42.339673\n",
      "[Epoch 2/20] [Batch 600/832] [D loss: 0.063398, acc:  92%] [G loss: 2.798748] time: 0:09:47.047335\n",
      "[Epoch 2/20] [Batch 620/832] [D loss: 0.041648, acc:  99%] [G loss: 4.997132] time: 0:09:52.432557\n",
      "[Epoch 2/20] [Batch 640/832] [D loss: 0.044320, acc:  98%] [G loss: 4.835663] time: 0:09:57.187431\n",
      "[Epoch 2/20] [Batch 660/832] [D loss: 0.058807, acc:  94%] [G loss: 5.201452] time: 0:10:01.898745\n",
      "[Epoch 2/20] [Batch 680/832] [D loss: 0.022169, acc:  99%] [G loss: 7.212863] time: 0:10:06.603857\n",
      "[Epoch 2/20] [Batch 700/832] [D loss: 0.083843, acc:  89%] [G loss: 5.747927] time: 0:10:11.324786\n",
      "[Epoch 2/20] [Batch 720/832] [D loss: 0.063478, acc:  94%] [G loss: 3.983367] time: 0:10:16.666332\n",
      "[Epoch 2/20] [Batch 740/832] [D loss: 0.066380, acc:  94%] [G loss: 5.335331] time: 0:10:21.401902\n",
      "[Epoch 2/20] [Batch 760/832] [D loss: 0.070261, acc:  93%] [G loss: 5.437465] time: 0:10:26.150233\n",
      "[Epoch 2/20] [Batch 780/832] [D loss: 0.069865, acc:  94%] [G loss: 5.981445] time: 0:10:30.876532\n",
      "[Epoch 2/20] [Batch 800/832] [D loss: 0.038373, acc:  98%] [G loss: 3.084007] time: 0:10:35.610813\n",
      "[Epoch 2/20] [Batch 820/832] [D loss: 0.017550, acc:  99%] [G loss: 6.021601] time: 0:10:40.950568\n",
      "[Epoch 3/20] [Batch 0/832] [D loss: 0.018541, acc:  99%] [G loss: 4.325051] time: 0:10:43.570298\n",
      "[Epoch 3/20] [Batch 20/832] [D loss: 0.179709, acc:  67%] [G loss: 3.129156] time: 0:10:48.942342\n",
      "[Epoch 3/20] [Batch 40/832] [D loss: 0.044153, acc:  97%] [G loss: 3.787845] time: 0:10:53.692635\n",
      "[Epoch 3/20] [Batch 60/832] [D loss: 0.108629, acc:  87%] [G loss: 5.489848] time: 0:10:58.407866\n",
      "[Epoch 3/20] [Batch 80/832] [D loss: 0.024185, acc:  99%] [G loss: 4.264437] time: 0:11:03.134307\n",
      "[Epoch 3/20] [Batch 100/832] [D loss: 0.033669, acc:  98%] [G loss: 5.163568] time: 0:11:07.888934\n",
      "[Epoch 3/20] [Batch 120/832] [D loss: 0.020878, acc: 100%] [G loss: 4.673558] time: 0:11:13.286045\n",
      "[Epoch 3/20] [Batch 140/832] [D loss: 0.032760, acc:  99%] [G loss: 4.173801] time: 0:11:18.052076\n",
      "[Epoch 3/20] [Batch 160/832] [D loss: 0.036670, acc:  99%] [G loss: 5.504494] time: 0:11:22.861262\n",
      "[Epoch 3/20] [Batch 180/832] [D loss: 0.047952, acc:  98%] [G loss: 3.936183] time: 0:11:27.596879\n",
      "[Epoch 3/20] [Batch 200/832] [D loss: 0.030531, acc:  98%] [G loss: 3.895771] time: 0:11:32.342001\n",
      "[Epoch 3/20] [Batch 220/832] [D loss: 0.052935, acc:  96%] [G loss: 4.164468] time: 0:11:37.700541\n",
      "[Epoch 3/20] [Batch 240/832] [D loss: 0.090945, acc:  90%] [G loss: 4.093060] time: 0:11:42.408934\n",
      "[Epoch 3/20] [Batch 260/832] [D loss: 0.054673, acc:  96%] [G loss: 5.258191] time: 0:11:47.116135\n",
      "[Epoch 3/20] [Batch 280/832] [D loss: 0.032321, acc:  98%] [G loss: 6.272132] time: 0:11:51.843410\n",
      "[Epoch 3/20] [Batch 300/832] [D loss: 0.008247, acc: 100%] [G loss: 5.936466] time: 0:11:56.546408\n",
      "[Epoch 3/20] [Batch 320/832] [D loss: 0.017482, acc:  99%] [G loss: 5.044398] time: 0:12:01.870932\n",
      "[Epoch 3/20] [Batch 340/832] [D loss: 0.013765, acc:  99%] [G loss: 4.476747] time: 0:12:06.595197\n",
      "[Epoch 3/20] [Batch 360/832] [D loss: 0.044479, acc:  97%] [G loss: 5.425110] time: 0:12:11.327605\n",
      "[Epoch 3/20] [Batch 380/832] [D loss: 0.012963, acc: 100%] [G loss: 4.494838] time: 0:12:16.040404\n",
      "[Epoch 3/20] [Batch 400/832] [D loss: 0.020202, acc:  99%] [G loss: 4.559799] time: 0:12:20.774252\n",
      "[Epoch 3/20] [Batch 420/832] [D loss: 0.027906, acc:  99%] [G loss: 5.747956] time: 0:12:26.119580\n",
      "[Epoch 3/20] [Batch 440/832] [D loss: 0.023374, acc:  99%] [G loss: 4.433242] time: 0:12:30.839670\n",
      "[Epoch 3/20] [Batch 460/832] [D loss: 0.215864, acc:  74%] [G loss: 2.889271] time: 0:12:35.570158\n",
      "[Epoch 3/20] [Batch 480/832] [D loss: 0.022735, acc:  99%] [G loss: 4.296652] time: 0:12:40.294401\n",
      "[Epoch 3/20] [Batch 500/832] [D loss: 0.011310, acc:  99%] [G loss: 4.885954] time: 0:12:45.000442\n",
      "[Epoch 3/20] [Batch 520/832] [D loss: 0.018667, acc: 100%] [G loss: 7.215946] time: 0:12:50.331258\n",
      "[Epoch 3/20] [Batch 540/832] [D loss: 0.014568, acc: 100%] [G loss: 6.301968] time: 0:12:55.078309\n",
      "[Epoch 3/20] [Batch 560/832] [D loss: 0.020317, acc: 100%] [G loss: 4.565254] time: 0:12:59.793127\n",
      "[Epoch 3/20] [Batch 580/832] [D loss: 0.015769, acc: 100%] [G loss: 5.897212] time: 0:13:04.501984\n",
      "[Epoch 3/20] [Batch 600/832] [D loss: 0.014215, acc:  99%] [G loss: 4.408222] time: 0:13:09.219682\n",
      "[Epoch 3/20] [Batch 620/832] [D loss: 0.008353, acc: 100%] [G loss: 4.480127] time: 0:13:14.574281\n",
      "[Epoch 3/20] [Batch 640/832] [D loss: 0.017243, acc:  99%] [G loss: 4.169485] time: 0:13:19.317645\n",
      "[Epoch 3/20] [Batch 660/832] [D loss: 0.013309, acc:  99%] [G loss: 3.832821] time: 0:13:24.042554\n",
      "[Epoch 3/20] [Batch 680/832] [D loss: 0.014016, acc: 100%] [G loss: 7.450544] time: 0:13:28.775432\n",
      "[Epoch 3/20] [Batch 700/832] [D loss: 0.033939, acc:  99%] [G loss: 4.265064] time: 0:13:33.504687\n",
      "[Epoch 3/20] [Batch 720/832] [D loss: 0.026938, acc:  98%] [G loss: 4.881164] time: 0:13:38.858368\n",
      "[Epoch 3/20] [Batch 740/832] [D loss: 0.008855, acc: 100%] [G loss: 3.972209] time: 0:13:43.576757\n",
      "[Epoch 3/20] [Batch 760/832] [D loss: 0.028338, acc:  99%] [G loss: 5.117598] time: 0:13:48.307706\n",
      "[Epoch 3/20] [Batch 780/832] [D loss: 0.029491, acc:  98%] [G loss: 3.400798] time: 0:13:53.019239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/20] [Batch 800/832] [D loss: 0.010012, acc: 100%] [G loss: 4.137491] time: 0:13:57.743838\n",
      "[Epoch 3/20] [Batch 820/832] [D loss: 0.010149, acc: 100%] [G loss: 3.657351] time: 0:14:03.100263\n",
      "[Epoch 4/20] [Batch 0/832] [D loss: 0.035581, acc:  99%] [G loss: 5.659532] time: 0:14:05.691258\n",
      "[Epoch 4/20] [Batch 20/832] [D loss: 0.008170, acc:  99%] [G loss: 5.198228] time: 0:14:11.057699\n",
      "[Epoch 4/20] [Batch 40/832] [D loss: 0.010166, acc:  99%] [G loss: 4.558654] time: 0:14:15.767561\n",
      "[Epoch 4/20] [Batch 60/832] [D loss: 0.026406, acc:  99%] [G loss: 3.373536] time: 0:14:20.511153\n",
      "[Epoch 4/20] [Batch 80/832] [D loss: 0.007593, acc: 100%] [G loss: 4.832661] time: 0:14:25.221030\n",
      "[Epoch 4/20] [Batch 100/832] [D loss: 0.021590, acc: 100%] [G loss: 4.531067] time: 0:14:29.957492\n",
      "[Epoch 4/20] [Batch 120/832] [D loss: 0.019255, acc: 100%] [G loss: 4.816875] time: 0:14:35.341329\n",
      "[Epoch 4/20] [Batch 140/832] [D loss: 0.013504, acc:  99%] [G loss: 4.076936] time: 0:14:40.081451\n",
      "[Epoch 4/20] [Batch 160/832] [D loss: 0.026226, acc:  99%] [G loss: 5.493739] time: 0:14:44.815175\n",
      "[Epoch 4/20] [Batch 180/832] [D loss: 0.010142, acc:  99%] [G loss: 2.855925] time: 0:14:49.535112\n",
      "[Epoch 4/20] [Batch 200/832] [D loss: 0.034478, acc:  99%] [G loss: 5.345281] time: 0:14:54.242289\n",
      "[Epoch 4/20] [Batch 220/832] [D loss: 0.013057, acc: 100%] [G loss: 4.487681] time: 0:14:59.569862\n",
      "[Epoch 4/20] [Batch 240/832] [D loss: 0.010335, acc:  99%] [G loss: 3.503338] time: 0:15:04.290151\n",
      "[Epoch 4/20] [Batch 260/832] [D loss: 2.651792, acc:  38%] [G loss: 5.994125] time: 0:15:09.011760\n",
      "[Epoch 4/20] [Batch 280/832] [D loss: 0.342755, acc:  60%] [G loss: 4.247722] time: 0:15:13.737199\n",
      "[Epoch 4/20] [Batch 300/832] [D loss: 0.115849, acc:  89%] [G loss: 4.351979] time: 0:15:18.511383\n",
      "[Epoch 4/20] [Batch 320/832] [D loss: 0.065744, acc:  95%] [G loss: 3.385498] time: 0:15:23.926566\n",
      "[Epoch 4/20] [Batch 340/832] [D loss: 0.021081, acc:  99%] [G loss: 4.025651] time: 0:15:28.689031\n",
      "[Epoch 4/20] [Batch 360/832] [D loss: 0.025240, acc:  99%] [G loss: 4.283138] time: 0:15:33.401774\n",
      "[Epoch 4/20] [Batch 380/832] [D loss: 0.036619, acc:  98%] [G loss: 3.785210] time: 0:15:38.136123\n",
      "[Epoch 4/20] [Batch 400/832] [D loss: 0.010334, acc:  99%] [G loss: 3.098944] time: 0:15:42.853924\n",
      "[Epoch 4/20] [Batch 420/832] [D loss: 0.024103, acc:  99%] [G loss: 5.724022] time: 0:15:48.297979\n",
      "[Epoch 4/20] [Batch 440/832] [D loss: 0.024364, acc:  99%] [G loss: 3.528371] time: 0:15:53.023563\n",
      "[Epoch 4/20] [Batch 460/832] [D loss: 0.016480, acc:  99%] [G loss: 5.036184] time: 0:15:57.750222\n",
      "[Epoch 4/20] [Batch 480/832] [D loss: 0.010341, acc:  99%] [G loss: 3.896986] time: 0:16:02.474304\n",
      "[Epoch 4/20] [Batch 500/832] [D loss: 0.043581, acc:  97%] [G loss: 4.206522] time: 0:16:07.188786\n",
      "[Epoch 4/20] [Batch 520/832] [D loss: 0.007768, acc:  99%] [G loss: 3.240636] time: 0:16:12.536080\n",
      "[Epoch 4/20] [Batch 540/832] [D loss: 0.017682, acc:  99%] [G loss: 5.764624] time: 0:16:17.246526\n",
      "[Epoch 4/20] [Batch 560/832] [D loss: 0.016067, acc:  99%] [G loss: 3.848562] time: 0:16:21.975243\n",
      "[Epoch 4/20] [Batch 580/832] [D loss: 0.006828, acc:  99%] [G loss: 5.253685] time: 0:16:26.686727\n",
      "[Epoch 4/20] [Batch 600/832] [D loss: 0.010352, acc:  99%] [G loss: 5.324557] time: 0:16:31.432197\n",
      "[Epoch 4/20] [Batch 620/832] [D loss: 0.012169, acc:  99%] [G loss: 4.388067] time: 0:16:36.796842\n",
      "[Epoch 4/20] [Batch 640/832] [D loss: 0.016240, acc:  99%] [G loss: 3.374048] time: 0:16:41.524983\n",
      "[Epoch 4/20] [Batch 660/832] [D loss: 0.019466, acc:  99%] [G loss: 4.950723] time: 0:16:46.253817\n",
      "[Epoch 4/20] [Batch 680/832] [D loss: 0.003649, acc: 100%] [G loss: 4.335145] time: 0:16:50.969238\n",
      "[Epoch 4/20] [Batch 700/832] [D loss: 0.006415, acc:  99%] [G loss: 5.298944] time: 0:16:55.684321\n",
      "[Epoch 4/20] [Batch 720/832] [D loss: 0.013774, acc:  99%] [G loss: 4.229835] time: 0:17:01.037556\n",
      "[Epoch 4/20] [Batch 740/832] [D loss: 0.007773, acc:  99%] [G loss: 3.842594] time: 0:17:05.753199\n",
      "[Epoch 4/20] [Batch 760/832] [D loss: 0.007937, acc:  99%] [G loss: 4.148264] time: 0:17:10.485775\n",
      "[Epoch 4/20] [Batch 780/832] [D loss: 0.015159, acc:  98%] [G loss: 4.889296] time: 0:17:15.218474\n",
      "[Epoch 4/20] [Batch 800/832] [D loss: 0.005512, acc: 100%] [G loss: 5.090042] time: 0:17:19.948350\n",
      "[Epoch 4/20] [Batch 820/832] [D loss: 0.010025, acc:  99%] [G loss: 4.795527] time: 0:17:25.308849\n",
      "[Epoch 5/20] [Batch 0/832] [D loss: 0.020993, acc:  99%] [G loss: 3.494979] time: 0:17:27.892051\n",
      "[Epoch 5/20] [Batch 20/832] [D loss: 0.004900, acc:  99%] [G loss: 3.723369] time: 0:17:33.198071\n",
      "[Epoch 5/20] [Batch 40/832] [D loss: 0.008622, acc:  99%] [G loss: 3.970817] time: 0:17:37.869971\n",
      "[Epoch 5/20] [Batch 60/832] [D loss: 0.013435, acc:  99%] [G loss: 3.928910] time: 0:17:42.570100\n",
      "[Epoch 5/20] [Batch 80/832] [D loss: 0.009408, acc:  99%] [G loss: 4.076132] time: 0:17:47.276023\n",
      "[Epoch 5/20] [Batch 100/832] [D loss: 0.003574, acc:  99%] [G loss: 3.392418] time: 0:17:51.999922\n",
      "[Epoch 5/20] [Batch 120/832] [D loss: 0.011022, acc: 100%] [G loss: 4.731455] time: 0:17:57.349831\n",
      "[Epoch 5/20] [Batch 140/832] [D loss: 0.006643, acc: 100%] [G loss: 4.421666] time: 0:18:02.065793\n",
      "[Epoch 5/20] [Batch 160/832] [D loss: 0.018029, acc:  99%] [G loss: 3.331578] time: 0:18:06.797279\n",
      "[Epoch 5/20] [Batch 180/832] [D loss: 0.009456, acc:  99%] [G loss: 4.168691] time: 0:18:11.514294\n",
      "[Epoch 5/20] [Batch 200/832] [D loss: 0.060755, acc:  96%] [G loss: 3.374532] time: 0:18:16.232284\n",
      "[Epoch 5/20] [Batch 220/832] [D loss: 0.068789, acc:  99%] [G loss: 3.306034] time: 0:18:21.568190\n",
      "[Epoch 5/20] [Batch 240/832] [D loss: 0.119769, acc:  84%] [G loss: 3.348822] time: 0:18:26.294412\n",
      "[Epoch 5/20] [Batch 260/832] [D loss: 0.007188, acc:  99%] [G loss: 3.904611] time: 0:18:31.019584\n",
      "[Epoch 5/20] [Batch 280/832] [D loss: 0.017924, acc:  99%] [G loss: 3.109348] time: 0:18:35.708385\n",
      "[Epoch 5/20] [Batch 300/832] [D loss: 0.016207, acc:  99%] [G loss: 4.512932] time: 0:18:40.433425\n",
      "[Epoch 5/20] [Batch 320/832] [D loss: 0.023430, acc:  98%] [G loss: 4.909114] time: 0:18:45.817519\n",
      "[Epoch 5/20] [Batch 340/832] [D loss: 0.014119, acc:  98%] [G loss: 5.140207] time: 0:18:50.545869\n",
      "[Epoch 5/20] [Batch 360/832] [D loss: 0.014136, acc:  99%] [G loss: 3.610462] time: 0:18:55.284778\n",
      "[Epoch 5/20] [Batch 380/832] [D loss: 0.006814, acc: 100%] [G loss: 4.438616] time: 0:18:59.996147\n",
      "[Epoch 5/20] [Batch 400/832] [D loss: 0.092873, acc:  94%] [G loss: 5.956880] time: 0:19:04.681531\n",
      "[Epoch 5/20] [Batch 420/832] [D loss: 0.012287, acc:  99%] [G loss: 4.821972] time: 0:19:10.052042\n",
      "[Epoch 5/20] [Batch 440/832] [D loss: 0.019274, acc:  99%] [G loss: 3.868963] time: 0:19:14.774395\n",
      "[Epoch 5/20] [Batch 460/832] [D loss: 0.006321, acc:  99%] [G loss: 3.439804] time: 0:19:19.498827\n",
      "[Epoch 5/20] [Batch 480/832] [D loss: 0.009954, acc:  99%] [G loss: 4.163100] time: 0:19:24.220269\n",
      "[Epoch 5/20] [Batch 500/832] [D loss: 0.004785, acc: 100%] [G loss: 4.198189] time: 0:19:28.945374\n",
      "[Epoch 5/20] [Batch 520/832] [D loss: 0.016189, acc: 100%] [G loss: 4.729495] time: 0:19:34.297325\n",
      "[Epoch 5/20] [Batch 540/832] [D loss: 0.123755, acc:  89%] [G loss: 4.034823] time: 0:19:39.019569\n",
      "[Epoch 5/20] [Batch 560/832] [D loss: 0.010371, acc:  99%] [G loss: 4.557556] time: 0:19:43.761814\n",
      "[Epoch 5/20] [Batch 580/832] [D loss: 0.008568, acc:  99%] [G loss: 3.905001] time: 0:19:48.478129\n",
      "[Epoch 5/20] [Batch 600/832] [D loss: 0.005991, acc:  99%] [G loss: 4.635865] time: 0:19:53.181655\n",
      "[Epoch 5/20] [Batch 620/832] [D loss: 0.018446, acc:  99%] [G loss: 3.641999] time: 0:19:58.463169\n",
      "[Epoch 5/20] [Batch 640/832] [D loss: 0.006409, acc:  99%] [G loss: 3.208570] time: 0:20:03.192731\n",
      "[Epoch 5/20] [Batch 660/832] [D loss: 0.003532, acc: 100%] [G loss: 4.039513] time: 0:20:07.910868\n",
      "[Epoch 5/20] [Batch 680/832] [D loss: 0.012008, acc:  99%] [G loss: 4.440090] time: 0:20:12.613505\n",
      "[Epoch 5/20] [Batch 700/832] [D loss: 0.006780, acc: 100%] [G loss: 5.112566] time: 0:20:17.338950\n",
      "[Epoch 5/20] [Batch 720/832] [D loss: 0.011957, acc:  99%] [G loss: 3.840440] time: 0:20:22.644983\n",
      "[Epoch 5/20] [Batch 740/832] [D loss: 0.053651, acc:  94%] [G loss: 3.156446] time: 0:20:27.354128\n",
      "[Epoch 5/20] [Batch 760/832] [D loss: 0.003290, acc: 100%] [G loss: 3.140161] time: 0:20:32.065973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/20] [Batch 780/832] [D loss: 0.006110, acc:  99%] [G loss: 3.052248] time: 0:20:36.763090\n",
      "[Epoch 5/20] [Batch 800/832] [D loss: 0.006901, acc:  99%] [G loss: 4.422963] time: 0:20:41.473704\n",
      "[Epoch 5/20] [Batch 820/832] [D loss: 0.005314, acc:  99%] [G loss: 3.959533] time: 0:20:46.788913\n",
      "[Epoch 6/20] [Batch 0/832] [D loss: 0.006431, acc:  99%] [G loss: 3.503030] time: 0:20:49.380042\n",
      "[Epoch 6/20] [Batch 20/832] [D loss: 0.010254, acc:  99%] [G loss: 3.949857] time: 0:20:54.784208\n",
      "[Epoch 6/20] [Batch 40/832] [D loss: 0.006324, acc: 100%] [G loss: 4.316365] time: 0:20:59.520778\n",
      "[Epoch 6/20] [Batch 60/832] [D loss: 0.004712, acc:  99%] [G loss: 3.392971] time: 0:21:04.259961\n",
      "[Epoch 6/20] [Batch 80/832] [D loss: 0.015568, acc:  99%] [G loss: 5.106857] time: 0:21:08.995144\n",
      "[Epoch 6/20] [Batch 100/832] [D loss: 0.014506, acc:  99%] [G loss: 4.222776] time: 0:21:13.725835\n",
      "[Epoch 6/20] [Batch 120/832] [D loss: 0.009827, acc:  99%] [G loss: 4.503709] time: 0:21:19.043881\n",
      "[Epoch 6/20] [Batch 140/832] [D loss: 0.009147, acc:  99%] [G loss: 3.762628] time: 0:21:23.771483\n",
      "[Epoch 6/20] [Batch 160/832] [D loss: 0.003780, acc: 100%] [G loss: 4.539232] time: 0:21:28.482573\n",
      "[Epoch 6/20] [Batch 180/832] [D loss: 0.011792, acc:  99%] [G loss: 4.846499] time: 0:21:33.177727\n",
      "[Epoch 6/20] [Batch 200/832] [D loss: 0.014694, acc:  99%] [G loss: 5.981318] time: 0:21:37.903081\n",
      "[Epoch 6/20] [Batch 220/832] [D loss: 0.013694, acc:  99%] [G loss: 3.413637] time: 0:21:43.286226\n",
      "[Epoch 6/20] [Batch 240/832] [D loss: 0.298208, acc:  44%] [G loss: 3.574749] time: 0:21:48.035406\n",
      "[Epoch 6/20] [Batch 260/832] [D loss: 0.258152, acc:  51%] [G loss: 3.913157] time: 0:21:52.776828\n",
      "[Epoch 6/20] [Batch 280/832] [D loss: 0.194322, acc:  71%] [G loss: 3.243610] time: 0:21:57.508437\n",
      "[Epoch 6/20] [Batch 300/832] [D loss: 0.086817, acc:  95%] [G loss: 3.151798] time: 0:22:02.238750\n",
      "[Epoch 6/20] [Batch 320/832] [D loss: 0.019125, acc:  99%] [G loss: 3.607795] time: 0:22:07.566513\n",
      "[Epoch 6/20] [Batch 340/832] [D loss: 0.027990, acc:  98%] [G loss: 3.591100] time: 0:22:12.293477\n",
      "[Epoch 6/20] [Batch 360/832] [D loss: 0.016949, acc: 100%] [G loss: 4.636737] time: 0:22:17.036201\n",
      "[Epoch 6/20] [Batch 380/832] [D loss: 0.015851, acc:  99%] [G loss: 4.136679] time: 0:22:21.784039\n",
      "[Epoch 6/20] [Batch 400/832] [D loss: 0.010432, acc:  99%] [G loss: 4.067939] time: 0:22:26.528740\n",
      "[Epoch 6/20] [Batch 420/832] [D loss: 0.010685, acc: 100%] [G loss: 3.810689] time: 0:22:31.894229\n",
      "[Epoch 6/20] [Batch 440/832] [D loss: 0.009582, acc:  99%] [G loss: 2.862556] time: 0:22:36.675913\n",
      "[Epoch 6/20] [Batch 460/832] [D loss: 0.080107, acc:  94%] [G loss: 4.495940] time: 0:22:41.454358\n",
      "[Epoch 6/20] [Batch 480/832] [D loss: 0.015900, acc:  99%] [G loss: 3.669261] time: 0:22:46.183788\n",
      "[Epoch 6/20] [Batch 500/832] [D loss: 0.006886, acc:  99%] [G loss: 3.989928] time: 0:22:50.907504\n",
      "[Epoch 6/20] [Batch 520/832] [D loss: 0.012666, acc:  99%] [G loss: 3.313148] time: 0:22:56.234595\n",
      "[Epoch 6/20] [Batch 540/832] [D loss: 0.064422, acc:  98%] [G loss: 3.491710] time: 0:23:00.959849\n",
      "[Epoch 6/20] [Batch 560/832] [D loss: 0.027245, acc:  99%] [G loss: 2.750972] time: 0:23:05.704706\n",
      "[Epoch 6/20] [Batch 580/832] [D loss: 0.007694, acc:  99%] [G loss: 3.738853] time: 0:23:10.424115\n",
      "[Epoch 6/20] [Batch 600/832] [D loss: 0.009179, acc:  99%] [G loss: 3.051130] time: 0:23:15.138160\n",
      "[Epoch 6/20] [Batch 620/832] [D loss: 0.098991, acc:  88%] [G loss: 3.121213] time: 0:23:20.548222\n",
      "[Epoch 6/20] [Batch 640/832] [D loss: 0.014224, acc:  99%] [G loss: 4.228073] time: 0:23:25.286333\n",
      "[Epoch 6/20] [Batch 660/832] [D loss: 0.008730, acc:  99%] [G loss: 4.677152] time: 0:23:30.003605\n",
      "[Epoch 6/20] [Batch 680/832] [D loss: 0.003712, acc:  99%] [G loss: 3.614432] time: 0:23:34.742549\n",
      "[Epoch 6/20] [Batch 700/832] [D loss: 0.003456, acc: 100%] [G loss: 4.023454] time: 0:23:39.451490\n",
      "[Epoch 6/20] [Batch 720/832] [D loss: 0.005232, acc: 100%] [G loss: 4.627746] time: 0:23:44.744921\n",
      "[Epoch 6/20] [Batch 740/832] [D loss: 0.007326, acc:  99%] [G loss: 2.856744] time: 0:23:49.440463\n",
      "[Epoch 6/20] [Batch 760/832] [D loss: 0.005750, acc:  99%] [G loss: 3.786205] time: 0:23:54.166379\n",
      "[Epoch 6/20] [Batch 780/832] [D loss: 0.021022, acc:  99%] [G loss: 2.580508] time: 0:23:58.901779\n",
      "[Epoch 6/20] [Batch 800/832] [D loss: 0.034152, acc:  98%] [G loss: 4.061272] time: 0:24:03.598106\n",
      "[Epoch 6/20] [Batch 820/832] [D loss: 0.012423, acc:  99%] [G loss: 3.183713] time: 0:24:08.945591\n",
      "[Epoch 7/20] [Batch 0/832] [D loss: 0.005549, acc:  99%] [G loss: 3.563987] time: 0:24:11.527334\n",
      "[Epoch 7/20] [Batch 20/832] [D loss: 0.020580, acc:  99%] [G loss: 3.346387] time: 0:24:16.841966\n",
      "[Epoch 7/20] [Batch 40/832] [D loss: 0.006580, acc:  99%] [G loss: 4.331347] time: 0:24:21.534098\n",
      "[Epoch 7/20] [Batch 60/832] [D loss: 0.005008, acc: 100%] [G loss: 3.941559] time: 0:24:26.225546\n",
      "[Epoch 7/20] [Batch 80/832] [D loss: 0.015482, acc:  99%] [G loss: 4.451249] time: 0:24:30.943415\n",
      "[Epoch 7/20] [Batch 100/832] [D loss: 0.006640, acc: 100%] [G loss: 3.803018] time: 0:24:35.636936\n",
      "[Epoch 7/20] [Batch 120/832] [D loss: 0.023518, acc:  99%] [G loss: 4.044734] time: 0:24:40.997966\n",
      "[Epoch 7/20] [Batch 140/832] [D loss: 0.023085, acc:  99%] [G loss: 3.822780] time: 0:24:45.706562\n",
      "[Epoch 7/20] [Batch 160/832] [D loss: 0.012943, acc:  99%] [G loss: 3.495480] time: 0:24:50.421786\n",
      "[Epoch 7/20] [Batch 180/832] [D loss: 0.005789, acc:  99%] [G loss: 4.062762] time: 0:24:55.130947\n",
      "[Epoch 7/20] [Batch 200/832] [D loss: 0.009097, acc:  99%] [G loss: 3.876209] time: 0:24:59.837732\n",
      "[Epoch 7/20] [Batch 220/832] [D loss: 0.013287, acc:  99%] [G loss: 3.173253] time: 0:25:05.247492\n",
      "[Epoch 7/20] [Batch 240/832] [D loss: 0.016420, acc:  99%] [G loss: 2.405045] time: 0:25:09.963067\n",
      "[Epoch 7/20] [Batch 260/832] [D loss: 0.006114, acc:  99%] [G loss: 2.386371] time: 0:25:14.698653\n",
      "[Epoch 7/20] [Batch 280/832] [D loss: 0.010711, acc:  99%] [G loss: 3.026252] time: 0:25:19.407131\n",
      "[Epoch 7/20] [Batch 300/832] [D loss: 0.004261, acc: 100%] [G loss: 4.462424] time: 0:25:24.136783\n",
      "[Epoch 7/20] [Batch 320/832] [D loss: 0.002678, acc:  99%] [G loss: 3.696574] time: 0:25:29.463592\n",
      "[Epoch 7/20] [Batch 340/832] [D loss: 0.024937, acc:  99%] [G loss: 3.765246] time: 0:25:34.176252\n",
      "[Epoch 7/20] [Batch 360/832] [D loss: 0.009319, acc:  99%] [G loss: 3.463586] time: 0:25:38.880144\n",
      "[Epoch 7/20] [Batch 380/832] [D loss: 0.009172, acc:  99%] [G loss: 4.127749] time: 0:25:43.588000\n",
      "[Epoch 7/20] [Batch 400/832] [D loss: 0.009312, acc:  99%] [G loss: 4.183158] time: 0:25:48.287774\n",
      "[Epoch 7/20] [Batch 420/832] [D loss: 0.159414, acc:  83%] [G loss: 3.247535] time: 0:25:53.605756\n",
      "[Epoch 7/20] [Batch 440/832] [D loss: 0.019982, acc:  98%] [G loss: 3.885747] time: 0:25:58.340393\n",
      "[Epoch 7/20] [Batch 460/832] [D loss: 0.040612, acc:  97%] [G loss: 3.655198] time: 0:26:03.066227\n",
      "[Epoch 7/20] [Batch 480/832] [D loss: 0.357901, acc:  76%] [G loss: 3.502438] time: 0:26:07.776313\n",
      "[Epoch 7/20] [Batch 500/832] [D loss: 0.016270, acc:  99%] [G loss: 4.487218] time: 0:26:12.486995\n",
      "[Epoch 7/20] [Batch 520/832] [D loss: 0.004885, acc:  99%] [G loss: 3.644159] time: 0:26:17.827654\n",
      "[Epoch 7/20] [Batch 540/832] [D loss: 0.005304, acc: 100%] [G loss: 4.314993] time: 0:26:22.556399\n",
      "[Epoch 7/20] [Batch 560/832] [D loss: 0.007206, acc:  99%] [G loss: 4.378665] time: 0:26:27.278710\n",
      "[Epoch 7/20] [Batch 580/832] [D loss: 0.012887, acc:  99%] [G loss: 4.221036] time: 0:26:31.974070\n",
      "[Epoch 7/20] [Batch 600/832] [D loss: 0.005766, acc:  99%] [G loss: 4.021244] time: 0:26:36.686694\n",
      "[Epoch 7/20] [Batch 620/832] [D loss: 0.005522, acc:  99%] [G loss: 4.742691] time: 0:26:42.024695\n",
      "[Epoch 7/20] [Batch 640/832] [D loss: 0.027033, acc:  98%] [G loss: 3.662561] time: 0:26:46.765737\n",
      "[Epoch 7/20] [Batch 660/832] [D loss: 0.005163, acc: 100%] [G loss: 5.066402] time: 0:26:51.464571\n",
      "[Epoch 7/20] [Batch 680/832] [D loss: 0.005548, acc: 100%] [G loss: 5.070177] time: 0:26:56.156759\n",
      "[Epoch 7/20] [Batch 700/832] [D loss: 0.002713, acc:  99%] [G loss: 3.101129] time: 0:27:00.897018\n",
      "[Epoch 7/20] [Batch 720/832] [D loss: 0.005732, acc:  99%] [G loss: 3.700289] time: 0:27:06.242738\n",
      "[Epoch 7/20] [Batch 740/832] [D loss: 0.003084, acc: 100%] [G loss: 3.798887] time: 0:27:10.939077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/20] [Batch 760/832] [D loss: 0.003381, acc: 100%] [G loss: 3.933330] time: 0:27:15.651760\n",
      "[Epoch 7/20] [Batch 780/832] [D loss: 0.002698, acc: 100%] [G loss: 4.430490] time: 0:27:20.373575\n",
      "[Epoch 7/20] [Batch 800/832] [D loss: 0.002439, acc: 100%] [G loss: 3.982880] time: 0:27:25.091327\n",
      "[Epoch 7/20] [Batch 820/832] [D loss: 0.002587, acc: 100%] [G loss: 3.881551] time: 0:27:30.443008\n",
      "[Epoch 8/20] [Batch 0/832] [D loss: 0.002563, acc: 100%] [G loss: 3.508863] time: 0:27:33.027604\n",
      "[Epoch 8/20] [Batch 20/832] [D loss: 0.002016, acc: 100%] [G loss: 3.671824] time: 0:27:38.340401\n",
      "[Epoch 8/20] [Batch 40/832] [D loss: 0.002461, acc: 100%] [G loss: 3.341746] time: 0:27:43.069480\n",
      "[Epoch 8/20] [Batch 60/832] [D loss: 0.031147, acc:  97%] [G loss: 3.989508] time: 0:27:47.787833\n",
      "[Epoch 8/20] [Batch 80/832] [D loss: 0.009036, acc:  99%] [G loss: 3.970236] time: 0:27:52.518633\n",
      "[Epoch 8/20] [Batch 100/832] [D loss: 0.013775, acc:  99%] [G loss: 3.636239] time: 0:27:57.228543\n",
      "[Epoch 8/20] [Batch 120/832] [D loss: 0.002636, acc: 100%] [G loss: 3.437220] time: 0:28:02.568803\n",
      "[Epoch 8/20] [Batch 140/832] [D loss: 0.003271, acc: 100%] [G loss: 3.218114] time: 0:28:07.287368\n",
      "[Epoch 8/20] [Batch 160/832] [D loss: 0.004157, acc: 100%] [G loss: 2.974129] time: 0:28:11.980797\n",
      "[Epoch 8/20] [Batch 180/832] [D loss: 0.010913, acc:  99%] [G loss: 3.434263] time: 0:28:16.696390\n",
      "[Epoch 8/20] [Batch 200/832] [D loss: 0.002603, acc: 100%] [G loss: 3.568527] time: 0:28:21.397873\n",
      "[Epoch 8/20] [Batch 220/832] [D loss: 0.003673, acc: 100%] [G loss: 3.643345] time: 0:28:26.686479\n",
      "[Epoch 8/20] [Batch 240/832] [D loss: 0.005182, acc:  99%] [G loss: 3.686168] time: 0:28:31.427373\n",
      "[Epoch 8/20] [Batch 260/832] [D loss: 0.008113, acc:  99%] [G loss: 2.559384] time: 0:28:36.159864\n",
      "[Epoch 8/20] [Batch 280/832] [D loss: 0.021366, acc:  99%] [G loss: 3.400533] time: 0:28:40.869001\n",
      "[Epoch 8/20] [Batch 300/832] [D loss: 0.028161, acc:  99%] [G loss: 3.879824] time: 0:28:45.600128\n",
      "[Epoch 8/20] [Batch 320/832] [D loss: 0.007867, acc: 100%] [G loss: 3.874646] time: 0:28:50.952931\n",
      "[Epoch 8/20] [Batch 340/832] [D loss: 0.004350, acc: 100%] [G loss: 3.779556] time: 0:28:55.684045\n",
      "[Epoch 8/20] [Batch 360/832] [D loss: 0.003350, acc:  99%] [G loss: 2.974739] time: 0:29:00.381563\n",
      "[Epoch 8/20] [Batch 380/832] [D loss: 0.010299, acc:  99%] [G loss: 3.574723] time: 0:29:05.101880\n",
      "[Epoch 8/20] [Batch 400/832] [D loss: 0.004706, acc: 100%] [G loss: 3.413343] time: 0:29:09.810178\n",
      "[Epoch 8/20] [Batch 420/832] [D loss: 0.003222, acc: 100%] [G loss: 3.383776] time: 0:29:15.154133\n",
      "[Epoch 8/20] [Batch 440/832] [D loss: 0.003410, acc:  99%] [G loss: 4.045269] time: 0:29:19.865999\n",
      "[Epoch 8/20] [Batch 460/832] [D loss: 0.022512, acc:  99%] [G loss: 3.117112] time: 0:29:24.561865\n",
      "[Epoch 8/20] [Batch 480/832] [D loss: 0.002675, acc: 100%] [G loss: 3.687141] time: 0:29:29.272630\n",
      "[Epoch 8/20] [Batch 500/832] [D loss: 0.002455, acc: 100%] [G loss: 4.409489] time: 0:29:33.965650\n",
      "[Epoch 8/20] [Batch 520/832] [D loss: 0.001903, acc: 100%] [G loss: 4.019134] time: 0:29:39.317623\n",
      "[Epoch 8/20] [Batch 540/832] [D loss: 0.002295, acc: 100%] [G loss: 3.602555] time: 0:29:44.017947\n",
      "[Epoch 8/20] [Batch 560/832] [D loss: 0.004433, acc:  99%] [G loss: 3.654323] time: 0:29:48.731291\n",
      "[Epoch 8/20] [Batch 580/832] [D loss: 0.001734, acc: 100%] [G loss: 4.307789] time: 0:29:53.456297\n",
      "[Epoch 8/20] [Batch 600/832] [D loss: 0.002520, acc: 100%] [G loss: 6.265686] time: 0:29:58.169641\n",
      "[Epoch 8/20] [Batch 620/832] [D loss: 0.002566, acc: 100%] [G loss: 3.425990] time: 0:30:03.524060\n",
      "[Epoch 8/20] [Batch 640/832] [D loss: 0.001964, acc: 100%] [G loss: 4.424452] time: 0:30:08.245138\n",
      "[Epoch 8/20] [Batch 660/832] [D loss: 0.001448, acc: 100%] [G loss: 4.222980] time: 0:30:12.973811\n",
      "[Epoch 8/20] [Batch 680/832] [D loss: 0.001819, acc: 100%] [G loss: 3.999876] time: 0:30:17.684056\n",
      "[Epoch 8/20] [Batch 700/832] [D loss: 0.006436, acc:  99%] [G loss: 2.759317] time: 0:30:22.427694\n",
      "[Epoch 8/20] [Batch 720/832] [D loss: 0.003338, acc: 100%] [G loss: 4.355865] time: 0:30:27.809844\n",
      "[Epoch 8/20] [Batch 740/832] [D loss: 0.001837, acc: 100%] [G loss: 4.250165] time: 0:30:32.554084\n",
      "[Epoch 8/20] [Batch 760/832] [D loss: 0.003079, acc: 100%] [G loss: 2.724921] time: 0:30:37.260764\n",
      "[Epoch 8/20] [Batch 780/832] [D loss: 0.003188, acc: 100%] [G loss: 3.220134] time: 0:30:41.965811\n",
      "[Epoch 8/20] [Batch 800/832] [D loss: 0.002631, acc: 100%] [G loss: 2.975585] time: 0:30:46.676145\n",
      "[Epoch 8/20] [Batch 820/832] [D loss: 0.002747, acc: 100%] [G loss: 3.727226] time: 0:30:52.042282\n",
      "[Epoch 9/20] [Batch 0/832] [D loss: 0.002663, acc: 100%] [G loss: 4.184489] time: 0:30:54.638915\n",
      "[Epoch 9/20] [Batch 20/832] [D loss: 0.003052, acc: 100%] [G loss: 3.438704] time: 0:30:59.912573\n",
      "[Epoch 9/20] [Batch 40/832] [D loss: 0.001733, acc: 100%] [G loss: 3.232372] time: 0:31:04.639301\n",
      "[Epoch 9/20] [Batch 60/832] [D loss: 0.004797, acc:  99%] [G loss: 4.269139] time: 0:31:09.371742\n",
      "[Epoch 9/20] [Batch 80/832] [D loss: 0.002366, acc: 100%] [G loss: 3.678911] time: 0:31:14.078026\n",
      "[Epoch 9/20] [Batch 100/832] [D loss: 0.002787, acc: 100%] [G loss: 3.933903] time: 0:31:18.766682\n",
      "[Epoch 9/20] [Batch 120/832] [D loss: 0.005506, acc: 100%] [G loss: 4.055591] time: 0:31:24.106628\n",
      "[Epoch 9/20] [Batch 140/832] [D loss: 0.001646, acc: 100%] [G loss: 3.669415] time: 0:31:28.824311\n",
      "[Epoch 9/20] [Batch 160/832] [D loss: 0.005399, acc:  99%] [G loss: 4.071620] time: 0:31:33.536274\n",
      "[Epoch 9/20] [Batch 180/832] [D loss: 0.003991, acc:  99%] [G loss: 2.898384] time: 0:31:38.260167\n",
      "[Epoch 9/20] [Batch 200/832] [D loss: 0.003902, acc:  99%] [G loss: 3.824247] time: 0:31:42.990360\n",
      "[Epoch 9/20] [Batch 220/832] [D loss: 0.001894, acc: 100%] [G loss: 4.172324] time: 0:31:48.323627\n",
      "[Epoch 9/20] [Batch 240/832] [D loss: 0.004909, acc:  99%] [G loss: 3.984307] time: 0:31:53.036305\n",
      "[Epoch 9/20] [Batch 260/832] [D loss: 0.013708, acc:  99%] [G loss: 3.230855] time: 0:31:57.753322\n",
      "[Epoch 9/20] [Batch 280/832] [D loss: 0.005145, acc: 100%] [G loss: 3.580885] time: 0:32:02.466119\n",
      "[Epoch 9/20] [Batch 300/832] [D loss: 0.057506, acc:  96%] [G loss: 3.571386] time: 0:32:07.182097\n",
      "[Epoch 9/20] [Batch 320/832] [D loss: 0.020954, acc:  98%] [G loss: 3.345360] time: 0:32:12.565583\n",
      "[Epoch 9/20] [Batch 340/832] [D loss: 0.006760, acc:  99%] [G loss: 4.212794] time: 0:32:17.265942\n",
      "[Epoch 9/20] [Batch 360/832] [D loss: 0.005466, acc:  99%] [G loss: 2.977407] time: 0:32:21.959833\n",
      "[Epoch 9/20] [Batch 380/832] [D loss: 0.011653, acc:  99%] [G loss: 3.728471] time: 0:32:26.677007\n",
      "[Epoch 9/20] [Batch 400/832] [D loss: 0.001967, acc: 100%] [G loss: 3.486752] time: 0:32:31.402898\n",
      "[Epoch 9/20] [Batch 420/832] [D loss: 0.004930, acc: 100%] [G loss: 3.716921] time: 0:32:36.674642\n",
      "[Epoch 9/20] [Batch 440/832] [D loss: 0.002710, acc: 100%] [G loss: 3.241709] time: 0:32:41.377515\n",
      "[Epoch 9/20] [Batch 460/832] [D loss: 0.002376, acc: 100%] [G loss: 3.705735] time: 0:32:46.093569\n",
      "[Epoch 9/20] [Batch 480/832] [D loss: 0.002187, acc: 100%] [G loss: 3.651222] time: 0:32:50.803882\n",
      "[Epoch 9/20] [Batch 500/832] [D loss: 0.005055, acc: 100%] [G loss: 4.694876] time: 0:32:55.529251\n",
      "[Epoch 9/20] [Batch 520/832] [D loss: 0.003197, acc:  99%] [G loss: 3.299180] time: 0:33:00.908324\n",
      "[Epoch 9/20] [Batch 540/832] [D loss: 0.011418, acc:  99%] [G loss: 3.959335] time: 0:33:05.630785\n",
      "[Epoch 9/20] [Batch 560/832] [D loss: 0.008978, acc:  99%] [G loss: 3.418271] time: 0:33:10.369589\n",
      "[Epoch 9/20] [Batch 580/832] [D loss: 0.268575, acc:  74%] [G loss: 3.209397] time: 0:33:15.090152\n",
      "[Epoch 9/20] [Batch 600/832] [D loss: 0.015432, acc:  99%] [G loss: 3.472407] time: 0:33:19.791730\n",
      "[Epoch 9/20] [Batch 620/832] [D loss: 0.010732, acc:  99%] [G loss: 2.587940] time: 0:33:25.035975\n",
      "[Epoch 9/20] [Batch 640/832] [D loss: 0.014768, acc:  99%] [G loss: 2.975257] time: 0:33:29.744446\n",
      "[Epoch 9/20] [Batch 660/832] [D loss: 0.004537, acc: 100%] [G loss: 3.278154] time: 0:33:34.452310\n",
      "[Epoch 9/20] [Batch 680/832] [D loss: 0.003517, acc: 100%] [G loss: 3.285887] time: 0:33:39.166386\n",
      "[Epoch 9/20] [Batch 700/832] [D loss: 0.003458, acc: 100%] [G loss: 3.770720] time: 0:33:43.890674\n",
      "[Epoch 9/20] [Batch 720/832] [D loss: 0.009679, acc:  99%] [G loss: 3.315818] time: 0:33:49.240248\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/20] [Batch 740/832] [D loss: 0.013383, acc:  99%] [G loss: 4.639285] time: 0:33:53.945324\n",
      "[Epoch 9/20] [Batch 760/832] [D loss: 0.028380, acc:  98%] [G loss: 3.668562] time: 0:33:58.665336\n",
      "[Epoch 9/20] [Batch 780/832] [D loss: 0.005740, acc:  99%] [G loss: 3.075785] time: 0:34:03.376485\n",
      "[Epoch 9/20] [Batch 800/832] [D loss: 0.006434, acc:  99%] [G loss: 3.677903] time: 0:34:08.083058\n",
      "[Epoch 9/20] [Batch 820/832] [D loss: 0.002321, acc:  99%] [G loss: 3.977887] time: 0:34:13.458061\n",
      "[Epoch 10/20] [Batch 0/832] [D loss: 0.005673, acc:  99%] [G loss: 4.196154] time: 0:34:16.046916\n",
      "[Epoch 10/20] [Batch 20/832] [D loss: 0.004793, acc:  99%] [G loss: 3.909562] time: 0:34:21.366630\n",
      "[Epoch 10/20] [Batch 40/832] [D loss: 0.005211, acc:  99%] [G loss: 2.455539] time: 0:34:26.085068\n",
      "[Epoch 10/20] [Batch 60/832] [D loss: 0.017130, acc:  98%] [G loss: 3.237546] time: 0:34:30.798730\n",
      "[Epoch 10/20] [Batch 80/832] [D loss: 0.015598, acc:  99%] [G loss: 3.674239] time: 0:34:35.515805\n",
      "[Epoch 10/20] [Batch 100/832] [D loss: 0.004662, acc:  99%] [G loss: 3.094209] time: 0:34:40.234848\n",
      "[Epoch 10/20] [Batch 120/832] [D loss: 0.006744, acc:  99%] [G loss: 2.887161] time: 0:34:45.599238\n",
      "[Epoch 10/20] [Batch 140/832] [D loss: 0.003970, acc: 100%] [G loss: 3.617373] time: 0:34:50.302792\n",
      "[Epoch 10/20] [Batch 160/832] [D loss: 0.009424, acc:  99%] [G loss: 3.287676] time: 0:34:54.993135\n",
      "[Epoch 10/20] [Batch 180/832] [D loss: 0.002364, acc: 100%] [G loss: 3.598911] time: 0:34:59.763329\n",
      "[Epoch 10/20] [Batch 200/832] [D loss: 0.002404, acc:  99%] [G loss: 3.038969] time: 0:35:04.503429\n",
      "[Epoch 10/20] [Batch 220/832] [D loss: 0.003339, acc:  99%] [G loss: 3.771952] time: 0:35:09.838244\n",
      "[Epoch 10/20] [Batch 240/832] [D loss: 0.019257, acc:  99%] [G loss: 3.511810] time: 0:35:14.601214\n",
      "[Epoch 10/20] [Batch 260/832] [D loss: 0.012410, acc: 100%] [G loss: 3.197469] time: 0:35:19.297217\n",
      "[Epoch 10/20] [Batch 280/832] [D loss: 0.002930, acc: 100%] [G loss: 3.363040] time: 0:35:24.029349\n",
      "[Epoch 10/20] [Batch 300/832] [D loss: 0.002553, acc: 100%] [G loss: 3.638612] time: 0:35:28.742457\n",
      "[Epoch 10/20] [Batch 320/832] [D loss: 0.050974, acc:  95%] [G loss: 4.186337] time: 0:35:34.112204\n",
      "[Epoch 10/20] [Batch 340/832] [D loss: 0.004029, acc: 100%] [G loss: 3.545873] time: 0:35:38.808422\n",
      "[Epoch 10/20] [Batch 360/832] [D loss: 0.003439, acc:  99%] [G loss: 5.417032] time: 0:35:43.526052\n",
      "[Epoch 10/20] [Batch 380/832] [D loss: 0.002640, acc: 100%] [G loss: 3.421982] time: 0:35:48.226093\n",
      "[Epoch 10/20] [Batch 400/832] [D loss: 0.002790, acc: 100%] [G loss: 3.602681] time: 0:35:52.938398\n",
      "[Epoch 10/20] [Batch 420/832] [D loss: 0.005174, acc: 100%] [G loss: 3.670030] time: 0:35:58.270232\n",
      "[Epoch 10/20] [Batch 440/832] [D loss: 0.002687, acc: 100%] [G loss: 2.590286] time: 0:36:02.982075\n",
      "[Epoch 10/20] [Batch 460/832] [D loss: 0.003874, acc:  99%] [G loss: 4.157374] time: 0:36:07.697714\n",
      "[Epoch 10/20] [Batch 480/832] [D loss: 0.005336, acc:  99%] [G loss: 3.181685] time: 0:36:12.419711\n",
      "[Epoch 10/20] [Batch 500/832] [D loss: 0.003829, acc:  99%] [G loss: 3.273938] time: 0:36:17.121297\n",
      "[Epoch 10/20] [Batch 520/832] [D loss: 0.002115, acc: 100%] [G loss: 3.742646] time: 0:36:22.449582\n",
      "[Epoch 10/20] [Batch 540/832] [D loss: 0.004344, acc: 100%] [G loss: 3.346764] time: 0:36:27.181724\n",
      "[Epoch 10/20] [Batch 560/832] [D loss: 0.001673, acc: 100%] [G loss: 3.789457] time: 0:36:31.861227\n",
      "[Epoch 10/20] [Batch 580/832] [D loss: 0.003588, acc:  99%] [G loss: 3.004740] time: 0:36:36.555754\n",
      "[Epoch 10/20] [Batch 600/832] [D loss: 0.007383, acc:  99%] [G loss: 2.881523] time: 0:36:41.278935\n",
      "[Epoch 10/20] [Batch 620/832] [D loss: 0.013778, acc:  99%] [G loss: 4.321517] time: 0:36:46.637864\n",
      "[Epoch 10/20] [Batch 640/832] [D loss: 0.001789, acc: 100%] [G loss: 3.013681] time: 0:36:51.354343\n",
      "[Epoch 10/20] [Batch 660/832] [D loss: 0.004046, acc:  99%] [G loss: 4.179073] time: 0:36:56.089391\n",
      "[Epoch 10/20] [Batch 680/832] [D loss: 0.002534, acc: 100%] [G loss: 3.994358] time: 0:37:00.803733\n",
      "[Epoch 10/20] [Batch 700/832] [D loss: 0.004083, acc:  99%] [G loss: 3.683341] time: 0:37:05.508316\n",
      "[Epoch 10/20] [Batch 720/832] [D loss: 0.003186, acc: 100%] [G loss: 3.676934] time: 0:37:10.881707\n",
      "[Epoch 10/20] [Batch 740/832] [D loss: 0.003577, acc: 100%] [G loss: 3.397983] time: 0:37:15.600780\n",
      "[Epoch 10/20] [Batch 760/832] [D loss: 0.007950, acc:  99%] [G loss: 3.341393] time: 0:37:20.318181\n",
      "[Epoch 10/20] [Batch 780/832] [D loss: 0.007696, acc:  99%] [G loss: 3.675782] time: 0:37:25.034117\n",
      "[Epoch 10/20] [Batch 800/832] [D loss: 0.002915, acc:  99%] [G loss: 4.143389] time: 0:37:29.764804\n",
      "[Epoch 10/20] [Batch 820/832] [D loss: 0.002040, acc: 100%] [G loss: 3.314813] time: 0:37:35.095717\n",
      "[Epoch 11/20] [Batch 0/832] [D loss: 0.002568, acc:  99%] [G loss: 3.178495] time: 0:37:37.687824\n",
      "[Epoch 11/20] [Batch 20/832] [D loss: 0.004276, acc:  99%] [G loss: 4.069217] time: 0:37:43.026906\n",
      "[Epoch 11/20] [Batch 40/832] [D loss: 0.004695, acc: 100%] [G loss: 3.934830] time: 0:37:47.736049\n",
      "[Epoch 11/20] [Batch 60/832] [D loss: 0.008783, acc:  99%] [G loss: 3.609320] time: 0:37:52.475844\n",
      "[Epoch 11/20] [Batch 80/832] [D loss: 0.003345, acc: 100%] [G loss: 3.322929] time: 0:37:57.174642\n",
      "[Epoch 11/20] [Batch 100/832] [D loss: 0.002674, acc: 100%] [G loss: 3.057069] time: 0:38:01.888516\n",
      "[Epoch 11/20] [Batch 120/832] [D loss: 0.014018, acc:  99%] [G loss: 4.891844] time: 0:38:07.245203\n",
      "[Epoch 11/20] [Batch 140/832] [D loss: 0.005016, acc: 100%] [G loss: 3.576478] time: 0:38:11.963590\n",
      "[Epoch 11/20] [Batch 160/832] [D loss: 0.002812, acc: 100%] [G loss: 4.086013] time: 0:38:16.716338\n",
      "[Epoch 11/20] [Batch 180/832] [D loss: 0.003084, acc: 100%] [G loss: 4.422091] time: 0:38:21.472323\n",
      "[Epoch 11/20] [Batch 200/832] [D loss: 0.232904, acc:  69%] [G loss: 4.625460] time: 0:38:26.189519\n",
      "[Epoch 11/20] [Batch 220/832] [D loss: 0.019500, acc:  99%] [G loss: 4.768369] time: 0:38:31.578917\n",
      "[Epoch 11/20] [Batch 240/832] [D loss: 0.013491, acc:  99%] [G loss: 3.643610] time: 0:38:36.278464\n",
      "[Epoch 11/20] [Batch 260/832] [D loss: 0.010157, acc:  99%] [G loss: 3.485891] time: 0:38:41.002183\n",
      "[Epoch 11/20] [Batch 280/832] [D loss: 0.010880, acc:  99%] [G loss: 3.073660] time: 0:38:45.731086\n",
      "[Epoch 11/20] [Batch 300/832] [D loss: 0.009044, acc:  99%] [G loss: 3.473648] time: 0:38:50.455266\n",
      "[Epoch 11/20] [Batch 320/832] [D loss: 0.009162, acc:  99%] [G loss: 3.142630] time: 0:38:55.878577\n",
      "[Epoch 11/20] [Batch 340/832] [D loss: 0.004056, acc: 100%] [G loss: 3.654529] time: 0:39:00.603053\n",
      "[Epoch 11/20] [Batch 360/832] [D loss: 0.003393, acc: 100%] [G loss: 3.778453] time: 0:39:05.283436\n",
      "[Epoch 11/20] [Batch 380/832] [D loss: 0.008834, acc:  99%] [G loss: 3.296481] time: 0:39:09.986548\n",
      "[Epoch 11/20] [Batch 400/832] [D loss: 0.004916, acc:  99%] [G loss: 4.446198] time: 0:39:14.715552\n",
      "[Epoch 11/20] [Batch 420/832] [D loss: 0.013468, acc: 100%] [G loss: 3.064016] time: 0:39:22.846116\n",
      "[Epoch 11/20] [Batch 440/832] [D loss: 0.004764, acc:  99%] [G loss: 3.743365] time: 0:39:27.595883\n",
      "[Epoch 11/20] [Batch 460/832] [D loss: 0.003874, acc: 100%] [G loss: 3.898205] time: 0:39:32.313941\n",
      "[Epoch 11/20] [Batch 480/832] [D loss: 0.010216, acc: 100%] [G loss: 2.887028] time: 0:39:37.020535\n",
      "[Epoch 11/20] [Batch 500/832] [D loss: 0.002709, acc: 100%] [G loss: 3.833471] time: 0:39:41.746028\n",
      "[Epoch 11/20] [Batch 520/832] [D loss: 0.004012, acc: 100%] [G loss: 4.441960] time: 0:39:47.129896\n",
      "[Epoch 11/20] [Batch 540/832] [D loss: 0.005012, acc:  99%] [G loss: 3.426683] time: 0:39:51.852412\n",
      "[Epoch 11/20] [Batch 560/832] [D loss: 0.003831, acc:  99%] [G loss: 2.693646] time: 0:39:56.568895\n",
      "[Epoch 11/20] [Batch 580/832] [D loss: 0.014210, acc:  99%] [G loss: 3.527388] time: 0:40:01.336012\n",
      "[Epoch 11/20] [Batch 600/832] [D loss: 0.012224, acc: 100%] [G loss: 5.708258] time: 0:40:06.073647\n",
      "[Epoch 11/20] [Batch 620/832] [D loss: 0.002882, acc: 100%] [G loss: 3.199933] time: 0:40:11.485623\n",
      "[Epoch 11/20] [Batch 640/832] [D loss: 0.002263, acc: 100%] [G loss: 3.315279] time: 0:40:16.186383\n",
      "[Epoch 11/20] [Batch 660/832] [D loss: 0.001956, acc: 100%] [G loss: 3.352453] time: 0:40:20.937309\n",
      "[Epoch 11/20] [Batch 680/832] [D loss: 0.002360, acc: 100%] [G loss: 4.478145] time: 0:40:25.668017\n",
      "[Epoch 11/20] [Batch 700/832] [D loss: 0.001692, acc: 100%] [G loss: 3.622699] time: 0:40:30.424548\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 11/20] [Batch 720/832] [D loss: 0.004404, acc:  99%] [G loss: 3.780509] time: 0:40:35.809456\n",
      "[Epoch 11/20] [Batch 740/832] [D loss: 0.002198, acc: 100%] [G loss: 3.492057] time: 0:40:40.536445\n",
      "[Epoch 11/20] [Batch 760/832] [D loss: 0.002117, acc: 100%] [G loss: 4.118449] time: 0:40:45.253326\n",
      "[Epoch 11/20] [Batch 780/832] [D loss: 0.005650, acc:  99%] [G loss: 3.027246] time: 0:40:49.961894\n",
      "[Epoch 11/20] [Batch 800/832] [D loss: 0.001469, acc: 100%] [G loss: 3.187466] time: 0:40:54.694862\n",
      "[Epoch 11/20] [Batch 820/832] [D loss: 0.004519, acc: 100%] [G loss: 2.992311] time: 0:41:00.076636\n",
      "[Epoch 12/20] [Batch 0/832] [D loss: 0.004970, acc: 100%] [G loss: 2.459819] time: 0:41:02.666633\n",
      "[Epoch 12/20] [Batch 20/832] [D loss: 0.003015, acc: 100%] [G loss: 4.533289] time: 0:41:08.048214\n",
      "[Epoch 12/20] [Batch 40/832] [D loss: 0.003176, acc:  99%] [G loss: 3.576066] time: 0:41:12.785325\n",
      "[Epoch 12/20] [Batch 60/832] [D loss: 0.008419, acc:  99%] [G loss: 3.375981] time: 0:41:17.498174\n",
      "[Epoch 12/20] [Batch 80/832] [D loss: 0.001869, acc: 100%] [G loss: 3.975534] time: 0:41:22.205652\n",
      "[Epoch 12/20] [Batch 100/832] [D loss: 0.021820, acc:  98%] [G loss: 3.653881] time: 0:41:26.919226\n",
      "[Epoch 12/20] [Batch 120/832] [D loss: 0.012068, acc:  99%] [G loss: 5.612460] time: 0:41:32.245493\n",
      "[Epoch 12/20] [Batch 140/832] [D loss: 0.016616, acc:  98%] [G loss: 3.708824] time: 0:41:36.968530\n",
      "[Epoch 12/20] [Batch 160/832] [D loss: 0.004420, acc: 100%] [G loss: 3.859306] time: 0:41:41.682963\n",
      "[Epoch 12/20] [Batch 180/832] [D loss: 0.009657, acc:  99%] [G loss: 5.214089] time: 0:41:46.393918\n",
      "[Epoch 12/20] [Batch 200/832] [D loss: 0.003951, acc: 100%] [G loss: 3.535534] time: 0:41:51.144031\n",
      "[Epoch 12/20] [Batch 220/832] [D loss: 0.005406, acc:  99%] [G loss: 5.697762] time: 0:41:56.481043\n",
      "[Epoch 12/20] [Batch 240/832] [D loss: 0.008241, acc:  99%] [G loss: 3.831195] time: 0:42:01.196124\n",
      "[Epoch 12/20] [Batch 260/832] [D loss: 0.278129, acc:  50%] [G loss: 3.780147] time: 0:42:05.951219\n",
      "[Epoch 12/20] [Batch 280/832] [D loss: 0.003654, acc:  99%] [G loss: 2.865520] time: 0:42:10.666760\n",
      "[Epoch 12/20] [Batch 300/832] [D loss: 0.002826, acc: 100%] [G loss: 3.786322] time: 0:42:15.417025\n",
      "[Epoch 12/20] [Batch 320/832] [D loss: 0.002539, acc:  99%] [G loss: 3.608995] time: 0:42:20.774867\n",
      "[Epoch 12/20] [Batch 340/832] [D loss: 0.001343, acc: 100%] [G loss: 2.982164] time: 0:42:25.500893\n",
      "[Epoch 12/20] [Batch 360/832] [D loss: 0.008939, acc:  99%] [G loss: 4.358514] time: 0:42:30.223924\n",
      "[Epoch 12/20] [Batch 380/832] [D loss: 0.002412, acc: 100%] [G loss: 3.729250] time: 0:42:34.946132\n",
      "[Epoch 12/20] [Batch 400/832] [D loss: 0.004090, acc:  99%] [G loss: 3.046633] time: 0:42:39.661754\n",
      "[Epoch 12/20] [Batch 420/832] [D loss: 0.004185, acc:  99%] [G loss: 2.649087] time: 0:42:45.039682\n",
      "[Epoch 12/20] [Batch 440/832] [D loss: 0.004256, acc: 100%] [G loss: 3.761201] time: 0:42:49.752648\n",
      "[Epoch 12/20] [Batch 460/832] [D loss: 0.006650, acc:  99%] [G loss: 4.306394] time: 0:42:54.457127\n",
      "[Epoch 12/20] [Batch 480/832] [D loss: 0.007198, acc:  99%] [G loss: 4.665941] time: 0:42:59.170025\n",
      "[Epoch 12/20] [Batch 500/832] [D loss: 0.001601, acc: 100%] [G loss: 2.927668] time: 0:43:03.883322\n",
      "[Epoch 12/20] [Batch 520/832] [D loss: 0.001497, acc: 100%] [G loss: 3.433617] time: 0:43:09.258771\n",
      "[Epoch 12/20] [Batch 540/832] [D loss: 0.026393, acc:  98%] [G loss: 3.774422] time: 0:43:13.953057\n",
      "[Epoch 12/20] [Batch 560/832] [D loss: 0.001762, acc: 100%] [G loss: 3.493254] time: 0:43:18.683925\n",
      "[Epoch 12/20] [Batch 580/832] [D loss: 0.003535, acc:  99%] [G loss: 3.297639] time: 0:43:23.392985\n",
      "[Epoch 12/20] [Batch 600/832] [D loss: 0.001705, acc: 100%] [G loss: 3.890026] time: 0:43:28.095834\n",
      "[Epoch 12/20] [Batch 620/832] [D loss: 0.002865, acc: 100%] [G loss: 4.793671] time: 0:43:33.424923\n",
      "[Epoch 12/20] [Batch 640/832] [D loss: 0.016557, acc:  98%] [G loss: 4.777541] time: 0:43:38.147674\n",
      "[Epoch 12/20] [Batch 660/832] [D loss: 0.002530, acc: 100%] [G loss: 2.831751] time: 0:43:42.851172\n",
      "[Epoch 12/20] [Batch 680/832] [D loss: 0.003927, acc:  99%] [G loss: 2.615818] time: 0:43:47.573961\n",
      "[Epoch 12/20] [Batch 700/832] [D loss: 0.001954, acc: 100%] [G loss: 3.258372] time: 0:43:52.308252\n",
      "[Epoch 12/20] [Batch 720/832] [D loss: 0.002025, acc: 100%] [G loss: 3.580238] time: 0:43:57.617445\n",
      "[Epoch 12/20] [Batch 740/832] [D loss: 0.004735, acc: 100%] [G loss: 3.339870] time: 0:44:02.344786\n",
      "[Epoch 12/20] [Batch 760/832] [D loss: 0.012713, acc:  99%] [G loss: 2.721356] time: 0:44:07.063102\n",
      "[Epoch 12/20] [Batch 780/832] [D loss: 0.002672, acc: 100%] [G loss: 3.431453] time: 0:44:11.779672\n",
      "[Epoch 12/20] [Batch 800/832] [D loss: 0.003020, acc: 100%] [G loss: 3.819561] time: 0:44:16.497391\n",
      "[Epoch 12/20] [Batch 820/832] [D loss: 0.008390, acc:  99%] [G loss: 2.845569] time: 0:44:21.838257\n",
      "[Epoch 13/20] [Batch 0/832] [D loss: 0.004158, acc:  99%] [G loss: 4.656629] time: 0:44:24.427379\n",
      "[Epoch 13/20] [Batch 20/832] [D loss: 0.003152, acc: 100%] [G loss: 3.876019] time: 0:44:29.827564\n",
      "[Epoch 13/20] [Batch 40/832] [D loss: 0.018467, acc:  98%] [G loss: 3.443338] time: 0:44:34.531827\n",
      "[Epoch 13/20] [Batch 60/832] [D loss: 0.003812, acc:  99%] [G loss: 4.175329] time: 0:44:39.261944\n",
      "[Epoch 13/20] [Batch 80/832] [D loss: 0.002839, acc:  99%] [G loss: 3.407496] time: 0:44:44.010046\n",
      "[Epoch 13/20] [Batch 100/832] [D loss: 0.114972, acc:  93%] [G loss: 3.799799] time: 0:44:48.749595\n",
      "[Epoch 13/20] [Batch 120/832] [D loss: 0.011014, acc:  99%] [G loss: 4.076543] time: 0:44:54.061393\n",
      "[Epoch 13/20] [Batch 140/832] [D loss: 0.006040, acc:  99%] [G loss: 3.410838] time: 0:44:58.785461\n",
      "[Epoch 13/20] [Batch 160/832] [D loss: 0.002192, acc: 100%] [G loss: 3.574980] time: 0:45:03.514123\n",
      "[Epoch 13/20] [Batch 180/832] [D loss: 0.001901, acc: 100%] [G loss: 2.778420] time: 0:45:08.226991\n",
      "[Epoch 13/20] [Batch 200/832] [D loss: 0.001949, acc: 100%] [G loss: 4.513579] time: 0:45:12.944688\n",
      "[Epoch 13/20] [Batch 220/832] [D loss: 0.002514, acc: 100%] [G loss: 3.491246] time: 0:45:18.297758\n",
      "[Epoch 13/20] [Batch 240/832] [D loss: 0.003124, acc: 100%] [G loss: 3.357407] time: 0:45:22.986455\n",
      "[Epoch 13/20] [Batch 260/832] [D loss: 0.002861, acc: 100%] [G loss: 2.797009] time: 0:45:27.673625\n",
      "[Epoch 13/20] [Batch 280/832] [D loss: 0.014865, acc:  99%] [G loss: 3.965409] time: 0:45:32.381605\n",
      "[Epoch 13/20] [Batch 300/832] [D loss: 0.004834, acc:  99%] [G loss: 2.922235] time: 0:45:37.099875\n",
      "[Epoch 13/20] [Batch 320/832] [D loss: 0.003799, acc: 100%] [G loss: 3.432223] time: 0:45:42.444576\n",
      "[Epoch 13/20] [Batch 340/832] [D loss: 0.004315, acc:  99%] [G loss: 3.634181] time: 0:45:47.156374\n",
      "[Epoch 13/20] [Batch 360/832] [D loss: 0.003308, acc:  99%] [G loss: 3.656407] time: 0:45:51.895521\n",
      "[Epoch 13/20] [Batch 380/832] [D loss: 0.006643, acc:  99%] [G loss: 3.709843] time: 0:45:56.580211\n",
      "[Epoch 13/20] [Batch 400/832] [D loss: 0.001455, acc: 100%] [G loss: 3.482177] time: 0:46:01.313642\n",
      "[Epoch 13/20] [Batch 420/832] [D loss: 0.004880, acc: 100%] [G loss: 3.384595] time: 0:46:06.669408\n",
      "[Epoch 13/20] [Batch 440/832] [D loss: 0.016690, acc:  99%] [G loss: 3.210404] time: 0:46:11.385433\n",
      "[Epoch 13/20] [Batch 460/832] [D loss: 0.005042, acc:  99%] [G loss: 3.165747] time: 0:46:16.109360\n",
      "[Epoch 13/20] [Batch 480/832] [D loss: 0.003848, acc:  99%] [G loss: 3.025050] time: 0:46:20.819627\n",
      "[Epoch 13/20] [Batch 500/832] [D loss: 0.001920, acc: 100%] [G loss: 5.049292] time: 0:46:25.539066\n",
      "[Epoch 13/20] [Batch 520/832] [D loss: 0.004876, acc:  99%] [G loss: 3.645658] time: 0:46:30.865312\n",
      "[Epoch 13/20] [Batch 540/832] [D loss: 0.005534, acc:  99%] [G loss: 3.544392] time: 0:46:35.578492\n",
      "[Epoch 13/20] [Batch 560/832] [D loss: 0.003939, acc: 100%] [G loss: 4.315077] time: 0:46:40.282749\n",
      "[Epoch 13/20] [Batch 580/832] [D loss: 0.002639, acc:  99%] [G loss: 4.093058] time: 0:46:44.997302\n",
      "[Epoch 13/20] [Batch 600/832] [D loss: 0.005117, acc:  99%] [G loss: 3.797581] time: 0:46:49.722653\n",
      "[Epoch 13/20] [Batch 620/832] [D loss: 0.004623, acc:  99%] [G loss: 3.012560] time: 0:46:55.096092\n",
      "[Epoch 13/20] [Batch 640/832] [D loss: 0.011750, acc:  99%] [G loss: 2.688144] time: 0:46:59.796312\n",
      "[Epoch 13/20] [Batch 660/832] [D loss: 0.006963, acc:  99%] [G loss: 3.736488] time: 0:47:04.529630\n",
      "[Epoch 13/20] [Batch 680/832] [D loss: 0.010562, acc:  99%] [G loss: 3.436007] time: 0:47:09.247447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 13/20] [Batch 700/832] [D loss: 0.007497, acc:  99%] [G loss: 3.638604] time: 0:47:13.967507\n",
      "[Epoch 13/20] [Batch 720/832] [D loss: 0.005573, acc:  99%] [G loss: 3.278771] time: 0:47:19.313875\n",
      "[Epoch 13/20] [Batch 740/832] [D loss: 0.004512, acc: 100%] [G loss: 4.873507] time: 0:47:24.023750\n",
      "[Epoch 13/20] [Batch 760/832] [D loss: 0.007524, acc:  99%] [G loss: 2.838946] time: 0:47:28.751310\n",
      "[Epoch 13/20] [Batch 780/832] [D loss: 0.005547, acc:  99%] [G loss: 4.185836] time: 0:47:33.458706\n",
      "[Epoch 13/20] [Batch 800/832] [D loss: 0.005071, acc: 100%] [G loss: 2.608414] time: 0:47:38.144537\n",
      "[Epoch 13/20] [Batch 820/832] [D loss: 0.002905, acc:  99%] [G loss: 2.903435] time: 0:47:43.493455\n",
      "[Epoch 14/20] [Batch 0/832] [D loss: 0.001027, acc: 100%] [G loss: 3.469868] time: 0:47:46.080823\n",
      "[Epoch 14/20] [Batch 20/832] [D loss: 0.007155, acc:  99%] [G loss: 3.256444] time: 0:47:51.464074\n",
      "[Epoch 14/20] [Batch 40/832] [D loss: 0.002720, acc:  99%] [G loss: 2.781024] time: 0:47:56.179000\n",
      "[Epoch 14/20] [Batch 60/832] [D loss: 0.003428, acc: 100%] [G loss: 3.998916] time: 0:48:00.888867\n",
      "[Epoch 14/20] [Batch 80/832] [D loss: 0.028623, acc:  98%] [G loss: 3.276539] time: 0:48:05.576763\n",
      "[Epoch 14/20] [Batch 100/832] [D loss: 0.007195, acc:  99%] [G loss: 3.214992] time: 0:48:10.292451\n",
      "[Epoch 14/20] [Batch 120/832] [D loss: 0.004013, acc:  99%] [G loss: 3.348739] time: 0:48:15.601701\n",
      "[Epoch 14/20] [Batch 140/832] [D loss: 0.001608, acc: 100%] [G loss: 4.478623] time: 0:48:20.297669\n",
      "[Epoch 14/20] [Batch 160/832] [D loss: 0.001525, acc: 100%] [G loss: 3.141384] time: 0:48:24.999347\n",
      "[Epoch 14/20] [Batch 180/832] [D loss: 0.007751, acc:  99%] [G loss: 3.569406] time: 0:48:29.731285\n",
      "[Epoch 14/20] [Batch 200/832] [D loss: 0.002805, acc:  99%] [G loss: 2.602330] time: 0:48:34.454358\n",
      "[Epoch 14/20] [Batch 220/832] [D loss: 0.002116, acc: 100%] [G loss: 3.073351] time: 0:48:39.859773\n",
      "[Epoch 14/20] [Batch 240/832] [D loss: 0.004894, acc: 100%] [G loss: 3.606881] time: 0:48:44.591187\n",
      "[Epoch 14/20] [Batch 260/832] [D loss: 0.005341, acc:  99%] [G loss: 3.171292] time: 0:48:49.315938\n",
      "[Epoch 14/20] [Batch 280/832] [D loss: 0.013555, acc:  99%] [G loss: 3.804129] time: 0:48:54.031635\n",
      "[Epoch 14/20] [Batch 300/832] [D loss: 0.001434, acc: 100%] [G loss: 3.242739] time: 0:48:58.766761\n",
      "[Epoch 14/20] [Batch 320/832] [D loss: 0.004473, acc:  99%] [G loss: 3.155271] time: 0:49:04.134092\n",
      "[Epoch 14/20] [Batch 340/832] [D loss: 0.001434, acc: 100%] [G loss: 4.692999] time: 0:49:08.833995\n",
      "[Epoch 14/20] [Batch 360/832] [D loss: 0.001592, acc: 100%] [G loss: 4.077899] time: 0:49:13.555694\n",
      "[Epoch 14/20] [Batch 380/832] [D loss: 0.025490, acc:  98%] [G loss: 2.982698] time: 0:49:18.296988\n",
      "[Epoch 14/20] [Batch 400/832] [D loss: 0.002052, acc: 100%] [G loss: 3.801063] time: 0:49:23.077931\n",
      "[Epoch 14/20] [Batch 420/832] [D loss: 0.003668, acc: 100%] [G loss: 3.421999] time: 0:49:28.513362\n",
      "[Epoch 14/20] [Batch 440/832] [D loss: 0.002162, acc: 100%] [G loss: 4.714500] time: 0:49:33.227486\n",
      "[Epoch 14/20] [Batch 460/832] [D loss: 0.002084, acc: 100%] [G loss: 3.359874] time: 0:49:37.942174\n",
      "[Epoch 14/20] [Batch 480/832] [D loss: 0.001654, acc: 100%] [G loss: 3.093918] time: 0:49:42.653117\n",
      "[Epoch 14/20] [Batch 500/832] [D loss: 0.002835, acc: 100%] [G loss: 2.933356] time: 0:49:47.356702\n",
      "[Epoch 14/20] [Batch 520/832] [D loss: 0.004583, acc: 100%] [G loss: 3.597079] time: 0:49:52.765244\n",
      "[Epoch 14/20] [Batch 540/832] [D loss: 0.004375, acc: 100%] [G loss: 2.220638] time: 0:49:57.490757\n",
      "[Epoch 14/20] [Batch 560/832] [D loss: 0.002430, acc: 100%] [G loss: 3.441660] time: 0:50:02.213879\n",
      "[Epoch 14/20] [Batch 580/832] [D loss: 0.004492, acc: 100%] [G loss: 2.753624] time: 0:50:06.944869\n",
      "[Epoch 14/20] [Batch 600/832] [D loss: 0.006370, acc:  99%] [G loss: 2.955395] time: 0:50:11.672109\n",
      "[Epoch 14/20] [Batch 620/832] [D loss: 0.026540, acc:  98%] [G loss: 3.850287] time: 0:50:17.063503\n",
      "[Epoch 14/20] [Batch 640/832] [D loss: 0.007406, acc:  99%] [G loss: 2.234027] time: 0:50:21.789511\n",
      "[Epoch 14/20] [Batch 660/832] [D loss: 0.001613, acc: 100%] [G loss: 3.324462] time: 0:50:26.507227\n",
      "[Epoch 14/20] [Batch 680/832] [D loss: 0.002780, acc: 100%] [G loss: 2.780224] time: 0:50:31.218106\n",
      "[Epoch 14/20] [Batch 700/832] [D loss: 0.002129, acc: 100%] [G loss: 3.404490] time: 0:50:35.931807\n",
      "[Epoch 14/20] [Batch 720/832] [D loss: 0.001926, acc: 100%] [G loss: 2.963602] time: 0:50:41.362747\n",
      "[Epoch 14/20] [Batch 740/832] [D loss: 0.004531, acc: 100%] [G loss: 3.639230] time: 0:50:46.112803\n",
      "[Epoch 14/20] [Batch 760/832] [D loss: 0.002534, acc: 100%] [G loss: 2.995734] time: 0:50:50.848890\n",
      "[Epoch 14/20] [Batch 780/832] [D loss: 0.004280, acc: 100%] [G loss: 4.050837] time: 0:50:55.554283\n",
      "[Epoch 14/20] [Batch 800/832] [D loss: 0.003466, acc: 100%] [G loss: 3.592340] time: 0:51:00.281224\n",
      "[Epoch 14/20] [Batch 820/832] [D loss: 0.004375, acc:  99%] [G loss: 3.223421] time: 0:51:05.688080\n",
      "[Epoch 15/20] [Batch 0/832] [D loss: 0.002111, acc: 100%] [G loss: 3.780469] time: 0:51:08.292289\n",
      "[Epoch 15/20] [Batch 20/832] [D loss: 0.008298, acc:  99%] [G loss: 3.703666] time: 0:51:13.634123\n",
      "[Epoch 15/20] [Batch 40/832] [D loss: 0.003639, acc:  99%] [G loss: 2.797578] time: 0:51:18.351922\n",
      "[Epoch 15/20] [Batch 60/832] [D loss: 0.003266, acc: 100%] [G loss: 3.775327] time: 0:51:23.085408\n",
      "[Epoch 15/20] [Batch 80/832] [D loss: 0.005861, acc:  99%] [G loss: 3.628754] time: 0:51:27.809484\n",
      "[Epoch 15/20] [Batch 100/832] [D loss: 0.002934, acc: 100%] [G loss: 4.736160] time: 0:51:32.552705\n",
      "[Epoch 15/20] [Batch 120/832] [D loss: 0.008020, acc:  99%] [G loss: 3.193503] time: 0:51:37.969521\n",
      "[Epoch 15/20] [Batch 140/832] [D loss: 1.618849, acc:  46%] [G loss: 5.783792] time: 0:51:42.689579\n",
      "[Epoch 15/20] [Batch 160/832] [D loss: 0.007205, acc:  99%] [G loss: 3.625816] time: 0:51:47.434568\n",
      "[Epoch 15/20] [Batch 180/832] [D loss: 0.006242, acc: 100%] [G loss: 3.807164] time: 0:51:52.132683\n",
      "[Epoch 15/20] [Batch 200/832] [D loss: 0.002737, acc: 100%] [G loss: 2.980920] time: 0:51:56.837845\n",
      "[Epoch 15/20] [Batch 220/832] [D loss: 0.003341, acc: 100%] [G loss: 3.591198] time: 0:52:02.161200\n",
      "[Epoch 15/20] [Batch 240/832] [D loss: 0.003434, acc:  99%] [G loss: 2.415237] time: 0:52:06.877009\n",
      "[Epoch 15/20] [Batch 260/832] [D loss: 0.013802, acc: 100%] [G loss: 3.369478] time: 0:52:11.607403\n",
      "[Epoch 15/20] [Batch 280/832] [D loss: 0.001878, acc: 100%] [G loss: 3.639393] time: 0:52:16.321479\n",
      "[Epoch 15/20] [Batch 300/832] [D loss: 0.005391, acc:  99%] [G loss: 3.386495] time: 0:52:21.049454\n",
      "[Epoch 15/20] [Batch 320/832] [D loss: 0.001550, acc: 100%] [G loss: 2.912786] time: 0:52:26.416675\n",
      "[Epoch 15/20] [Batch 340/832] [D loss: 0.004861, acc:  99%] [G loss: 3.589413] time: 0:52:31.113794\n",
      "[Epoch 15/20] [Batch 360/832] [D loss: 0.001442, acc: 100%] [G loss: 3.001302] time: 0:52:35.822942\n",
      "[Epoch 15/20] [Batch 380/832] [D loss: 0.002390, acc: 100%] [G loss: 3.457170] time: 0:52:40.543497\n",
      "[Epoch 15/20] [Batch 400/832] [D loss: 0.003711, acc: 100%] [G loss: 4.150187] time: 0:52:45.247216\n",
      "[Epoch 15/20] [Batch 420/832] [D loss: 0.001229, acc: 100%] [G loss: 3.221858] time: 0:52:50.709555\n",
      "[Epoch 15/20] [Batch 440/832] [D loss: 0.001126, acc: 100%] [G loss: 3.590786] time: 0:52:55.413227\n",
      "[Epoch 15/20] [Batch 460/832] [D loss: 0.001670, acc: 100%] [G loss: 3.289111] time: 0:53:00.145370\n",
      "[Epoch 15/20] [Batch 480/832] [D loss: 0.001616, acc: 100%] [G loss: 4.119301] time: 0:53:04.854999\n",
      "[Epoch 15/20] [Batch 500/832] [D loss: 0.002363, acc:  99%] [G loss: 3.360374] time: 0:53:09.565560\n",
      "[Epoch 15/20] [Batch 520/832] [D loss: 0.002228, acc: 100%] [G loss: 3.295450] time: 0:53:15.332391\n",
      "[Epoch 15/20] [Batch 540/832] [D loss: 0.002314, acc: 100%] [G loss: 3.377609] time: 0:53:20.036527\n",
      "[Epoch 15/20] [Batch 560/832] [D loss: 0.001507, acc: 100%] [G loss: 2.891312] time: 0:53:24.758353\n",
      "[Epoch 15/20] [Batch 580/832] [D loss: 0.001880, acc: 100%] [G loss: 3.518731] time: 0:53:29.490089\n",
      "[Epoch 15/20] [Batch 600/832] [D loss: 0.001374, acc: 100%] [G loss: 3.784222] time: 0:53:34.215137\n",
      "[Epoch 15/20] [Batch 620/832] [D loss: 0.001930, acc: 100%] [G loss: 3.514552] time: 0:53:40.206449\n",
      "[Epoch 15/20] [Batch 640/832] [D loss: 0.002445, acc: 100%] [G loss: 3.601767] time: 0:53:44.933183\n",
      "[Epoch 15/20] [Batch 660/832] [D loss: 0.001454, acc: 100%] [G loss: 2.455098] time: 0:53:49.643094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 15/20] [Batch 680/832] [D loss: 0.002417, acc: 100%] [G loss: 2.989161] time: 0:53:54.359690\n",
      "[Epoch 15/20] [Batch 700/832] [D loss: 0.002936, acc:  99%] [G loss: 3.472717] time: 0:53:59.046653\n",
      "[Epoch 15/20] [Batch 720/832] [D loss: 0.003217, acc: 100%] [G loss: 3.175897] time: 0:54:04.499460\n",
      "[Epoch 15/20] [Batch 740/832] [D loss: 0.005912, acc:  99%] [G loss: 4.067271] time: 0:54:09.225936\n",
      "[Epoch 15/20] [Batch 760/832] [D loss: 0.002970, acc:  99%] [G loss: 3.165312] time: 0:54:13.935528\n",
      "[Epoch 15/20] [Batch 780/832] [D loss: 0.001643, acc:  99%] [G loss: 3.280716] time: 0:54:18.631212\n",
      "[Epoch 15/20] [Batch 800/832] [D loss: 0.002914, acc:  99%] [G loss: 2.818940] time: 0:54:23.333709\n",
      "[Epoch 15/20] [Batch 820/832] [D loss: 0.003080, acc:  99%] [G loss: 2.986535] time: 0:54:28.879311\n",
      "[Epoch 16/20] [Batch 0/832] [D loss: 0.002242, acc: 100%] [G loss: 4.703721] time: 0:54:31.466167\n",
      "[Epoch 16/20] [Batch 20/832] [D loss: 0.001225, acc: 100%] [G loss: 2.700352] time: 0:54:37.249778\n",
      "[Epoch 16/20] [Batch 40/832] [D loss: 0.003047, acc: 100%] [G loss: 3.139880] time: 0:54:41.978383\n",
      "[Epoch 16/20] [Batch 60/832] [D loss: 0.001408, acc: 100%] [G loss: 3.128736] time: 0:54:46.703059\n",
      "[Epoch 16/20] [Batch 80/832] [D loss: 0.001314, acc: 100%] [G loss: 4.686124] time: 0:54:51.436286\n",
      "[Epoch 16/20] [Batch 100/832] [D loss: 0.001945, acc:  99%] [G loss: 3.128986] time: 0:54:56.133365\n",
      "[Epoch 16/20] [Batch 120/832] [D loss: 0.004582, acc:  99%] [G loss: 3.160121] time: 0:55:01.676793\n",
      "[Epoch 16/20] [Batch 140/832] [D loss: 0.002570, acc: 100%] [G loss: 3.283644] time: 0:55:06.396985\n",
      "[Epoch 16/20] [Batch 160/832] [D loss: 0.008854, acc:  99%] [G loss: 2.935205] time: 0:55:11.135048\n",
      "[Epoch 16/20] [Batch 180/832] [D loss: 0.004486, acc:  99%] [G loss: 2.824910] time: 0:55:15.867804\n",
      "[Epoch 16/20] [Batch 200/832] [D loss: 0.002323, acc: 100%] [G loss: 3.775697] time: 0:55:20.583424\n",
      "[Epoch 16/20] [Batch 220/832] [D loss: 0.001812, acc: 100%] [G loss: 3.922736] time: 0:55:26.035410\n",
      "[Epoch 16/20] [Batch 240/832] [D loss: 0.003767, acc: 100%] [G loss: 2.721925] time: 0:55:30.765515\n",
      "[Epoch 16/20] [Batch 260/832] [D loss: 0.002976, acc: 100%] [G loss: 4.035778] time: 0:55:35.473789\n",
      "[Epoch 16/20] [Batch 280/832] [D loss: 0.011002, acc:  99%] [G loss: 4.638936] time: 0:55:40.203135\n",
      "[Epoch 16/20] [Batch 300/832] [D loss: 0.007071, acc:  99%] [G loss: 3.204546] time: 0:55:44.931828\n",
      "[Epoch 16/20] [Batch 320/832] [D loss: 0.001477, acc: 100%] [G loss: 4.581518] time: 0:55:50.788530\n",
      "[Epoch 16/20] [Batch 340/832] [D loss: 0.001139, acc: 100%] [G loss: 3.616959] time: 0:55:55.489716\n",
      "[Epoch 16/20] [Batch 360/832] [D loss: 0.004164, acc:  99%] [G loss: 3.278868] time: 0:56:00.228343\n",
      "[Epoch 16/20] [Batch 380/832] [D loss: 0.006141, acc: 100%] [G loss: 2.363790] time: 0:56:04.937337\n",
      "[Epoch 16/20] [Batch 400/832] [D loss: 0.001687, acc: 100%] [G loss: 3.923603] time: 0:56:09.671741\n",
      "[Epoch 16/20] [Batch 420/832] [D loss: 0.002526, acc:  99%] [G loss: 3.281080] time: 0:56:15.059497\n",
      "[Epoch 16/20] [Batch 440/832] [D loss: 0.001180, acc: 100%] [G loss: 2.924832] time: 0:56:19.781761\n",
      "[Epoch 16/20] [Batch 460/832] [D loss: 0.004295, acc: 100%] [G loss: 3.654674] time: 0:56:24.535788\n",
      "[Epoch 16/20] [Batch 480/832] [D loss: 0.001651, acc: 100%] [G loss: 3.601526] time: 0:56:29.287923\n",
      "[Epoch 16/20] [Batch 500/832] [D loss: 0.005373, acc:  99%] [G loss: 3.515699] time: 0:56:33.993523\n",
      "[Epoch 16/20] [Batch 520/832] [D loss: 0.010714, acc:  99%] [G loss: 4.028986] time: 0:56:39.511525\n",
      "[Epoch 16/20] [Batch 540/832] [D loss: 0.002004, acc: 100%] [G loss: 3.431103] time: 0:56:44.223949\n",
      "[Epoch 16/20] [Batch 560/832] [D loss: 0.001831, acc: 100%] [G loss: 4.550926] time: 0:56:48.949115\n",
      "[Epoch 16/20] [Batch 580/832] [D loss: 0.006621, acc:  99%] [G loss: 2.989696] time: 0:56:53.691257\n",
      "[Epoch 16/20] [Batch 600/832] [D loss: 0.001636, acc: 100%] [G loss: 2.515216] time: 0:56:58.401689\n",
      "[Epoch 16/20] [Batch 620/832] [D loss: 0.002518, acc: 100%] [G loss: 4.578230] time: 0:57:03.888155\n",
      "[Epoch 16/20] [Batch 640/832] [D loss: 0.001077, acc: 100%] [G loss: 3.197484] time: 0:57:08.606199\n",
      "[Epoch 16/20] [Batch 660/832] [D loss: 0.001381, acc: 100%] [G loss: 2.769226] time: 0:57:13.312303\n",
      "[Epoch 16/20] [Batch 680/832] [D loss: 0.001122, acc: 100%] [G loss: 3.506552] time: 0:57:18.028488\n",
      "[Epoch 16/20] [Batch 700/832] [D loss: 0.003581, acc:  99%] [G loss: 3.411893] time: 0:57:22.758318\n",
      "[Epoch 16/20] [Batch 720/832] [D loss: 0.001851, acc: 100%] [G loss: 2.800753] time: 0:57:28.596838\n",
      "[Epoch 16/20] [Batch 740/832] [D loss: 0.001672, acc: 100%] [G loss: 3.881258] time: 0:57:33.324105\n",
      "[Epoch 16/20] [Batch 760/832] [D loss: 0.001210, acc: 100%] [G loss: 3.307397] time: 0:57:38.031699\n",
      "[Epoch 16/20] [Batch 780/832] [D loss: 0.001192, acc: 100%] [G loss: 3.089336] time: 0:57:42.750463\n",
      "[Epoch 16/20] [Batch 800/832] [D loss: 0.005746, acc:  99%] [G loss: 4.235699] time: 0:57:47.472676\n",
      "[Epoch 16/20] [Batch 820/832] [D loss: 0.002538, acc:  99%] [G loss: 4.004963] time: 0:57:52.835893\n",
      "[Epoch 17/20] [Batch 0/832] [D loss: 0.001218, acc: 100%] [G loss: 3.330687] time: 0:57:55.418136\n",
      "[Epoch 17/20] [Batch 20/832] [D loss: 0.000872, acc: 100%] [G loss: 3.600361] time: 0:58:01.072498\n",
      "[Epoch 17/20] [Batch 40/832] [D loss: 0.004340, acc: 100%] [G loss: 2.625840] time: 0:58:05.782611\n",
      "[Epoch 17/20] [Batch 60/832] [D loss: 0.008991, acc:  99%] [G loss: 2.793030] time: 0:58:10.487001\n",
      "[Epoch 17/20] [Batch 80/832] [D loss: 0.002364, acc:  99%] [G loss: 3.696087] time: 0:58:15.205378\n",
      "[Epoch 17/20] [Batch 100/832] [D loss: 0.004953, acc: 100%] [G loss: 3.848980] time: 0:58:19.915733\n",
      "[Epoch 17/20] [Batch 120/832] [D loss: 0.003043, acc: 100%] [G loss: 3.012027] time: 0:58:25.629473\n",
      "[Epoch 17/20] [Batch 140/832] [D loss: 0.002110, acc: 100%] [G loss: 3.932188] time: 0:58:30.353881\n",
      "[Epoch 17/20] [Batch 160/832] [D loss: 0.004370, acc:  99%] [G loss: 3.647285] time: 0:58:35.033361\n",
      "[Epoch 17/20] [Batch 180/832] [D loss: 0.000986, acc: 100%] [G loss: 3.431763] time: 0:58:39.733963\n",
      "[Epoch 17/20] [Batch 200/832] [D loss: 0.003291, acc:  99%] [G loss: 3.849230] time: 0:58:44.453431\n",
      "[Epoch 17/20] [Batch 220/832] [D loss: 0.004380, acc: 100%] [G loss: 3.925346] time: 0:58:50.130669\n",
      "[Epoch 17/20] [Batch 240/832] [D loss: 0.004886, acc:  99%] [G loss: 3.095745] time: 0:58:54.869396\n",
      "[Epoch 17/20] [Batch 260/832] [D loss: 0.017268, acc:  98%] [G loss: 2.834948] time: 0:58:59.595626\n",
      "[Epoch 17/20] [Batch 280/832] [D loss: 0.020932, acc:  97%] [G loss: 2.829619] time: 0:59:04.289244\n",
      "[Epoch 17/20] [Batch 300/832] [D loss: 0.001579, acc: 100%] [G loss: 3.929433] time: 0:59:08.991883\n",
      "[Epoch 17/20] [Batch 320/832] [D loss: 0.004280, acc:  99%] [G loss: 3.441088] time: 0:59:14.600424\n",
      "[Epoch 17/20] [Batch 340/832] [D loss: 0.001416, acc: 100%] [G loss: 3.502752] time: 0:59:19.315522\n",
      "[Epoch 17/20] [Batch 360/832] [D loss: 0.001458, acc: 100%] [G loss: 2.742470] time: 0:59:24.045311\n",
      "[Epoch 17/20] [Batch 380/832] [D loss: 0.167199, acc:  78%] [G loss: 3.319196] time: 0:59:28.774815\n",
      "[Epoch 17/20] [Batch 400/832] [D loss: 0.005539, acc: 100%] [G loss: 2.070204] time: 0:59:33.514362\n",
      "[Epoch 17/20] [Batch 420/832] [D loss: 0.002413, acc: 100%] [G loss: 4.222652] time: 0:59:38.896003\n",
      "[Epoch 17/20] [Batch 440/832] [D loss: 0.003327, acc: 100%] [G loss: 2.867978] time: 0:59:43.581297\n",
      "[Epoch 17/20] [Batch 460/832] [D loss: 0.005256, acc:  99%] [G loss: 3.384130] time: 0:59:48.302306\n",
      "[Epoch 17/20] [Batch 480/832] [D loss: 0.006663, acc:  99%] [G loss: 3.684134] time: 0:59:53.025479\n",
      "[Epoch 17/20] [Batch 500/832] [D loss: 0.003340, acc: 100%] [G loss: 3.099715] time: 0:59:57.737998\n",
      "[Epoch 17/20] [Batch 520/832] [D loss: 0.002010, acc: 100%] [G loss: 4.678772] time: 1:00:03.288022\n",
      "[Epoch 17/20] [Batch 540/832] [D loss: 0.001658, acc: 100%] [G loss: 3.814727] time: 1:00:08.020982\n",
      "[Epoch 17/20] [Batch 560/832] [D loss: 0.001135, acc: 100%] [G loss: 2.874243] time: 1:00:12.744104\n",
      "[Epoch 17/20] [Batch 580/832] [D loss: 0.001569, acc: 100%] [G loss: 3.540399] time: 1:00:17.451057\n",
      "[Epoch 17/20] [Batch 600/832] [D loss: 0.002363, acc: 100%] [G loss: 2.800022] time: 1:00:22.192989\n",
      "[Epoch 17/20] [Batch 620/832] [D loss: 0.001617, acc: 100%] [G loss: 3.697288] time: 1:00:27.872226\n",
      "[Epoch 17/20] [Batch 640/832] [D loss: 0.001158, acc: 100%] [G loss: 3.455036] time: 1:00:32.595374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/20] [Batch 660/832] [D loss: 0.001888, acc: 100%] [G loss: 4.038249] time: 1:00:37.314821\n",
      "[Epoch 17/20] [Batch 680/832] [D loss: 0.001168, acc: 100%] [G loss: 3.376439] time: 1:00:42.035451\n",
      "[Epoch 17/20] [Batch 700/832] [D loss: 0.000838, acc: 100%] [G loss: 3.573323] time: 1:00:46.749463\n",
      "[Epoch 17/20] [Batch 720/832] [D loss: 0.001644, acc: 100%] [G loss: 4.166298] time: 1:00:52.284261\n",
      "[Epoch 17/20] [Batch 740/832] [D loss: 0.000640, acc: 100%] [G loss: 2.572508] time: 1:00:57.002091\n",
      "[Epoch 17/20] [Batch 760/832] [D loss: 0.000869, acc: 100%] [G loss: 4.059485] time: 1:01:01.703792\n",
      "[Epoch 17/20] [Batch 780/832] [D loss: 0.001187, acc: 100%] [G loss: 2.625669] time: 1:01:06.410498\n",
      "[Epoch 17/20] [Batch 800/832] [D loss: 0.000730, acc: 100%] [G loss: 3.644705] time: 1:01:11.132732\n",
      "[Epoch 17/20] [Batch 820/832] [D loss: 0.001679, acc: 100%] [G loss: 3.448162] time: 1:01:16.469384\n",
      "[Epoch 18/20] [Batch 0/832] [D loss: 0.005068, acc: 100%] [G loss: 2.253225] time: 1:01:19.076776\n",
      "[Epoch 18/20] [Batch 20/832] [D loss: 0.001815, acc: 100%] [G loss: 3.551788] time: 1:01:24.394830\n",
      "[Epoch 18/20] [Batch 40/832] [D loss: 0.000685, acc: 100%] [G loss: 3.099971] time: 1:01:29.107320\n",
      "[Epoch 18/20] [Batch 60/832] [D loss: 0.001662, acc: 100%] [G loss: 3.119902] time: 1:01:33.835967\n",
      "[Epoch 18/20] [Batch 80/832] [D loss: 0.001132, acc: 100%] [G loss: 4.165857] time: 1:01:38.566812\n",
      "[Epoch 18/20] [Batch 100/832] [D loss: 0.000924, acc: 100%] [G loss: 2.750430] time: 1:01:43.265279\n",
      "[Epoch 18/20] [Batch 120/832] [D loss: 0.000933, acc: 100%] [G loss: 2.955027] time: 1:01:48.617475\n",
      "[Epoch 18/20] [Batch 140/832] [D loss: 0.001676, acc: 100%] [G loss: 4.289042] time: 1:01:53.331394\n",
      "[Epoch 18/20] [Batch 160/832] [D loss: 0.001610, acc: 100%] [G loss: 2.589072] time: 1:01:57.995243\n",
      "[Epoch 18/20] [Batch 180/832] [D loss: 0.002677, acc: 100%] [G loss: 4.827593] time: 1:02:02.742790\n",
      "[Epoch 18/20] [Batch 200/832] [D loss: 0.029838, acc:  97%] [G loss: 3.019449] time: 1:02:07.458216\n",
      "[Epoch 18/20] [Batch 220/832] [D loss: 0.001256, acc: 100%] [G loss: 3.475598] time: 1:02:12.973418\n",
      "[Epoch 18/20] [Batch 240/832] [D loss: 0.002581, acc: 100%] [G loss: 2.649107] time: 1:02:17.675422\n",
      "[Epoch 18/20] [Batch 260/832] [D loss: 0.001063, acc: 100%] [G loss: 3.004696] time: 1:02:22.380735\n",
      "[Epoch 18/20] [Batch 280/832] [D loss: 0.001403, acc: 100%] [G loss: 4.594599] time: 1:02:27.093360\n",
      "[Epoch 18/20] [Batch 300/832] [D loss: 0.000646, acc: 100%] [G loss: 2.625909] time: 1:02:31.819069\n",
      "[Epoch 18/20] [Batch 320/832] [D loss: 0.003418, acc: 100%] [G loss: 2.704589] time: 1:02:37.407176\n",
      "[Epoch 18/20] [Batch 340/832] [D loss: 0.001257, acc: 100%] [G loss: 2.870620] time: 1:02:42.114893\n",
      "[Epoch 18/20] [Batch 360/832] [D loss: 0.001208, acc: 100%] [G loss: 3.076375] time: 1:02:46.851658\n",
      "[Epoch 18/20] [Batch 380/832] [D loss: 0.001075, acc: 100%] [G loss: 3.820658] time: 1:02:51.548280\n",
      "[Epoch 18/20] [Batch 400/832] [D loss: 0.013475, acc:  99%] [G loss: 3.800379] time: 1:02:56.231340\n",
      "[Epoch 18/20] [Batch 420/832] [D loss: 0.006880, acc: 100%] [G loss: 4.261068] time: 1:03:01.915975\n",
      "[Epoch 18/20] [Batch 440/832] [D loss: 0.002136, acc: 100%] [G loss: 3.380189] time: 1:03:06.624604\n",
      "[Epoch 18/20] [Batch 460/832] [D loss: 0.002091, acc: 100%] [G loss: 3.814291] time: 1:03:11.347815\n",
      "[Epoch 18/20] [Batch 480/832] [D loss: 0.001539, acc: 100%] [G loss: 3.663440] time: 1:03:16.069978\n",
      "[Epoch 18/20] [Batch 500/832] [D loss: 0.001885, acc: 100%] [G loss: 4.676682] time: 1:03:20.795434\n",
      "[Epoch 18/20] [Batch 520/832] [D loss: 0.000725, acc: 100%] [G loss: 2.765374] time: 1:03:26.237654\n",
      "[Epoch 18/20] [Batch 540/832] [D loss: 0.001110, acc: 100%] [G loss: 2.254457] time: 1:03:30.949965\n",
      "[Epoch 18/20] [Batch 560/832] [D loss: 0.001755, acc: 100%] [G loss: 3.749842] time: 1:03:35.675134\n",
      "[Epoch 18/20] [Batch 580/832] [D loss: 0.002088, acc: 100%] [G loss: 3.082792] time: 1:03:40.371863\n",
      "[Epoch 18/20] [Batch 600/832] [D loss: 0.001217, acc: 100%] [G loss: 3.179280] time: 1:03:45.075856\n",
      "[Epoch 18/20] [Batch 620/832] [D loss: 0.007606, acc:  99%] [G loss: 3.324460] time: 1:03:50.642643\n",
      "[Epoch 18/20] [Batch 640/832] [D loss: 0.001079, acc: 100%] [G loss: 3.237380] time: 1:03:55.347327\n",
      "[Epoch 18/20] [Batch 660/832] [D loss: 0.006907, acc:  99%] [G loss: 3.217305] time: 1:04:00.094675\n",
      "[Epoch 18/20] [Batch 680/832] [D loss: 0.005198, acc:  99%] [G loss: 2.936266] time: 1:04:04.790363\n",
      "[Epoch 18/20] [Batch 700/832] [D loss: 0.065460, acc:  93%] [G loss: 3.066467] time: 1:04:09.478575\n",
      "[Epoch 18/20] [Batch 720/832] [D loss: 0.000770, acc: 100%] [G loss: 3.704390] time: 1:04:15.074382\n",
      "[Epoch 18/20] [Batch 740/832] [D loss: 0.001700, acc: 100%] [G loss: 4.928504] time: 1:04:19.802671\n",
      "[Epoch 18/20] [Batch 760/832] [D loss: 0.000604, acc: 100%] [G loss: 3.576607] time: 1:04:24.504330\n",
      "[Epoch 18/20] [Batch 780/832] [D loss: 0.000816, acc: 100%] [G loss: 3.464227] time: 1:04:29.226154\n",
      "[Epoch 18/20] [Batch 800/832] [D loss: 0.000835, acc: 100%] [G loss: 3.271869] time: 1:04:33.949259\n",
      "[Epoch 18/20] [Batch 820/832] [D loss: 0.001839, acc: 100%] [G loss: 4.047446] time: 1:04:39.272379\n",
      "[Epoch 19/20] [Batch 0/832] [D loss: 0.000948, acc: 100%] [G loss: 3.436018] time: 1:04:41.867997\n",
      "[Epoch 19/20] [Batch 20/832] [D loss: 0.000853, acc: 100%] [G loss: 3.991075] time: 1:04:47.290721\n",
      "[Epoch 19/20] [Batch 40/832] [D loss: 0.001228, acc: 100%] [G loss: 2.522761] time: 1:04:52.009599\n",
      "[Epoch 19/20] [Batch 60/832] [D loss: 0.006542, acc:  99%] [G loss: 2.458290] time: 1:04:56.723966\n",
      "[Epoch 19/20] [Batch 80/832] [D loss: 0.000907, acc: 100%] [G loss: 3.519532] time: 1:05:01.462015\n",
      "[Epoch 19/20] [Batch 100/832] [D loss: 0.001041, acc: 100%] [G loss: 3.123967] time: 1:05:06.184924\n",
      "[Epoch 19/20] [Batch 120/832] [D loss: 0.002233, acc: 100%] [G loss: 3.209156] time: 1:05:11.643728\n",
      "[Epoch 19/20] [Batch 140/832] [D loss: 0.002427, acc:  99%] [G loss: 3.151649] time: 1:05:16.407306\n",
      "[Epoch 19/20] [Batch 160/832] [D loss: 0.001723, acc: 100%] [G loss: 3.357144] time: 1:05:21.146472\n",
      "[Epoch 19/20] [Batch 180/832] [D loss: 0.001565, acc: 100%] [G loss: 3.343990] time: 1:05:25.885700\n",
      "[Epoch 19/20] [Batch 200/832] [D loss: 0.001266, acc: 100%] [G loss: 4.277452] time: 1:05:30.616336\n",
      "[Epoch 19/20] [Batch 220/832] [D loss: 0.015098, acc:  99%] [G loss: 4.078322] time: 1:05:36.034995\n",
      "[Epoch 19/20] [Batch 240/832] [D loss: 0.001373, acc: 100%] [G loss: 3.599450] time: 1:05:40.770476\n",
      "[Epoch 19/20] [Batch 260/832] [D loss: 0.001319, acc: 100%] [G loss: 3.644858] time: 1:05:45.522722\n",
      "[Epoch 19/20] [Batch 280/832] [D loss: 0.001071, acc: 100%] [G loss: 3.200150] time: 1:05:50.249455\n",
      "[Epoch 19/20] [Batch 300/832] [D loss: 0.001498, acc: 100%] [G loss: 3.564458] time: 1:05:54.949173\n",
      "[Epoch 19/20] [Batch 320/832] [D loss: 0.000686, acc: 100%] [G loss: 2.700478] time: 1:06:00.668010\n",
      "[Epoch 19/20] [Batch 340/832] [D loss: 0.001167, acc: 100%] [G loss: 3.190257] time: 1:06:05.417097\n",
      "[Epoch 19/20] [Batch 360/832] [D loss: 0.002231, acc:  99%] [G loss: 2.914586] time: 1:06:10.142038\n",
      "[Epoch 19/20] [Batch 380/832] [D loss: 0.002657, acc: 100%] [G loss: 3.262616] time: 1:06:14.857578\n",
      "[Epoch 19/20] [Batch 400/832] [D loss: 0.000993, acc: 100%] [G loss: 2.716801] time: 1:06:19.564671\n",
      "[Epoch 19/20] [Batch 420/832] [D loss: 0.004320, acc: 100%] [G loss: 3.877247] time: 1:06:25.107690\n",
      "[Epoch 19/20] [Batch 440/832] [D loss: 0.001691, acc: 100%] [G loss: 2.894038] time: 1:06:29.832928\n",
      "[Epoch 19/20] [Batch 460/832] [D loss: 0.003251, acc: 100%] [G loss: 3.568876] time: 1:06:34.526935\n",
      "[Epoch 19/20] [Batch 480/832] [D loss: 0.001231, acc: 100%] [G loss: 3.348777] time: 1:06:39.241312\n",
      "[Epoch 19/20] [Batch 500/832] [D loss: 0.001106, acc: 100%] [G loss: 3.489530] time: 1:06:43.956934\n",
      "[Epoch 19/20] [Batch 520/832] [D loss: 0.002227, acc: 100%] [G loss: 3.411342] time: 1:06:49.656262\n",
      "[Epoch 19/20] [Batch 540/832] [D loss: 0.000998, acc: 100%] [G loss: 3.210327] time: 1:06:54.364595\n",
      "[Epoch 19/20] [Batch 560/832] [D loss: 0.001088, acc: 100%] [G loss: 3.803821] time: 1:06:59.049133\n",
      "[Epoch 19/20] [Batch 580/832] [D loss: 0.001285, acc: 100%] [G loss: 5.050580] time: 1:07:03.770028\n",
      "[Epoch 19/20] [Batch 600/832] [D loss: 0.005460, acc:  99%] [G loss: 2.833814] time: 1:07:08.474104\n",
      "[Epoch 19/20] [Batch 620/832] [D loss: 0.005194, acc:  99%] [G loss: 5.160443] time: 1:07:13.848458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 19/20] [Batch 640/832] [D loss: 0.000806, acc: 100%] [G loss: 3.278689] time: 1:07:18.558599\n",
      "[Epoch 19/20] [Batch 660/832] [D loss: 0.010385, acc:  99%] [G loss: 3.145500] time: 1:07:23.246175\n",
      "[Epoch 19/20] [Batch 680/832] [D loss: 0.001061, acc: 100%] [G loss: 3.598850] time: 1:07:27.946940\n",
      "[Epoch 19/20] [Batch 700/832] [D loss: 0.000868, acc: 100%] [G loss: 2.938743] time: 1:07:32.671395\n",
      "[Epoch 19/20] [Batch 720/832] [D loss: 0.001234, acc: 100%] [G loss: 3.445480] time: 1:07:38.022643\n",
      "[Epoch 19/20] [Batch 740/832] [D loss: 0.000806, acc: 100%] [G loss: 3.313713] time: 1:07:42.722030\n",
      "[Epoch 19/20] [Batch 760/832] [D loss: 0.005767, acc: 100%] [G loss: 2.794884] time: 1:07:47.441554\n",
      "[Epoch 19/20] [Batch 780/832] [D loss: 0.003647, acc: 100%] [G loss: 2.560962] time: 1:07:52.173570\n",
      "[Epoch 19/20] [Batch 800/832] [D loss: 0.000791, acc: 100%] [G loss: 3.443842] time: 1:07:56.879906\n",
      "[Epoch 19/20] [Batch 820/832] [D loss: 0.000839, acc: 100%] [G loss: 2.700754] time: 1:08:02.709057\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = Pix2Pix()\n",
    "    gan.train(epochs=20, batch_size=16, sample_interval=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = DiscoGAN()\n",
    "    gan.train(epochs=20, batch_size=16, sample_interval=100)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = CycleGAN()\n",
    "    #gan.train(epochs=200, batch_size=1, sample_interval=200) # Original from github\n",
    "    gan.train(epochs=20, batch_size=16, sample_interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f784a416470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if 1:\n",
    "        r, c = 3, 3\n",
    "        p_size_1 = 128\n",
    "        \n",
    "        imgs_A = y_test_sim[[0, 2, 3]]\n",
    "        imgs_B = x_test_sim[[0, 2, 3]]\n",
    "        \n",
    "        #imgs_A, imgs_B = self.data_loader.load_data(batch_size=3, is_testing=True)\n",
    "        fake_A = gan.generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Generated', 'Target']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                \n",
    "                #axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 128, 128, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, c = 2, 3 # Original at github\n",
    "imgs_A = x_test_sim[[2]]\n",
    "imgs_B = y_test_sim[[2]]\n",
    "p_size_1 = 128\n",
    "if 1:\n",
    "        # Translate images to the other domain\n",
    "        fake_B = gan.g_AB.predict(imgs_A)\n",
    "        fake_A = gan.g_BA.predict(imgs_B)\n",
    "        # Translate back to original domain\n",
    "        reconstr_A = gan.g_BA.predict(fake_B)\n",
    "        reconstr_B = gan.g_AB.predict(fake_A)\n",
    "        gen_imgs = np.concatenate([imgs_A, fake_B, reconstr_A, imgs_B, fake_A, reconstr_B])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Original', 'Translated', 'Reconstructed']\n",
    "        \n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                axs[i,j].set_title(titles[j])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(reconstr_B), np.min(reconstr_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(fake_B), np.min(fake_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
