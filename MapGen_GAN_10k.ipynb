{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "## Map Generalization for Polygons using Pix2Pix strucutures\n",
    "## \n",
    "## Author: Yu Feng, yuzz.feng@gmail.com\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "def lambda_output(input_shape):\n",
    "    return input_shape[:2]\n",
    "\n",
    "def minb_disc(x):\n",
    "    diffs = K.expand_dims(x, 3) - K.expand_dims(K.permute_dimensions(x, [1, 2, 0]), 0)\n",
    "    abs_diffs = K.sum(K.abs(diffs), 2)\n",
    "    x = K.sum(K.exp(-abs_diffs), 2)\n",
    "\n",
    "    return x\n",
    "\n",
    "def generate_patch_gan_loss(last_disc_conv_layer, patch_dim, input_layer, nb_patches):\n",
    "\n",
    "    # generate a list of inputs for the different patches to the network\n",
    "    list_input = [Input(shape=patch_dim, name=\"patch_gan_input_%s\" % i) for i in range(nb_patches)]\n",
    "\n",
    "    # get an activation\n",
    "    x_flat = Flatten()(last_disc_conv_layer)\n",
    "    x = Dense(2, activation='softmax', name=\"disc_dense\")(x_flat)\n",
    "\n",
    "    patch_gan = Model(inputs=[input_layer], outputs=[x, x_flat], name=\"patch_gan\")\n",
    "\n",
    "    # generate individual losses for each patch\n",
    "    x = [patch_gan(patch)[0] for patch in list_input]\n",
    "    x_mbd = [patch_gan(patch)[1] for patch in list_input]\n",
    "\n",
    "    # merge layers if have multiple patches (aka perceptual loss)\n",
    "    if len(x) > 1:\n",
    "        #x = merge(x, mode=\"concat\", name=\"merged_features\")\n",
    "        x = Concatenate(name=\"merged_features\")(x)\n",
    "    else:\n",
    "        x = x[0]\n",
    "\n",
    "    # merge mbd if needed\n",
    "    # mbd = mini batch discrimination\n",
    "    # https://arxiv.org/pdf/1606.03498.pdf\n",
    "    if len(x_mbd) > 1:\n",
    "        #x_mbd = merge(x_mbd, mode=\"concat\", name=\"merged_feature_mbd\")\n",
    "        x_mbd = Concatenate(name=\"merged_feature_mbd\")(x_mbd)\n",
    "    else:\n",
    "        x_mbd = x_mbd[0]\n",
    "\n",
    "    num_kernels = 100\n",
    "    dim_per_kernel = 5\n",
    "\n",
    "    M = Dense(num_kernels * dim_per_kernel, use_bias=False, activation=None)\n",
    "    MBD = Lambda(minb_disc, output_shape=lambda_output)\n",
    "\n",
    "    x_mbd = M(x_mbd)\n",
    "    x_mbd = Reshape((num_kernels, dim_per_kernel))(x_mbd)\n",
    "    x_mbd = MBD(x_mbd)\n",
    "    \n",
    "    #x = merge([x, x_mbd], mode='concat')\n",
    "    x = Concatenate()([x, x_mbd])\n",
    "\n",
    "    x_out = Dense(2, activation=\"softmax\", name=\"disc_output\")(x)\n",
    "\n",
    "    discriminator = Model(inputs=list_input, outputs=[x_out], name='discriminator_nn')\n",
    "    return discriminator\n",
    "\n",
    "def res_block(x, nb_filters, strides, increase = False):\n",
    "    # This implementation used the double 3x3 structure and followed the Identity Mappings\n",
    "    res_path = BatchNormalization()(x)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    \n",
    "    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same', strides=strides[0])(res_path)\n",
    "    res_path = BatchNormalization()(res_path)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same', strides=strides[1])(res_path)\n",
    "    \n",
    "    if increase:\n",
    "        shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1), strides=strides[0])(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    res_path = Add()([shortcut, res_path])\n",
    "    return res_path\n",
    "\n",
    "def decoder(x, from_encoder):\n",
    "    main_path = UpSampling2D(size=(2, 2))(x)\n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[2]])\n",
    "    main_path = res_block(main_path, [128, 128], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    main_path = UpSampling2D(size=(2, 2))(main_path) \n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[1]])\n",
    "    main_path = res_block(main_path, [64, 64], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    main_path = UpSampling2D(size=(2, 2))(main_path)\n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[0]])\n",
    "    main_path = res_block(main_path, [32, 32], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    return main_path\n",
    "\n",
    "def encoder(x):\n",
    "    to_decoder = []\n",
    "\n",
    "    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n",
    "    main_path = BatchNormalization()(main_path)\n",
    "    main_path = Activation(activation='relu')(main_path)\n",
    "    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n",
    "\n",
    "    shortcut = Conv2D(filters=32, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    main_path = Add()([shortcut, main_path])\n",
    "    # first branching to decoder\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    main_path = res_block(main_path, [64, 64], [(2, 2), (1, 1)], increase = True)\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    main_path = res_block(main_path, [128, 128], [(2, 2), (1, 1)], increase = True)\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    return to_decoder\n",
    "\n",
    "def build_res_unet(input_shape):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    to_decoder = encoder(inputs)\n",
    "\n",
    "    path = res_block(to_decoder[2], [256, 256], [(2, 2), (1, 1)]) # 3x\n",
    "    \n",
    "    path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Yu.add - in 2018-12-02 16-09-04_15 only once\n",
    "\n",
    "    path = decoder(path, from_encoder=to_decoder)\n",
    "    \n",
    "    path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path) \n",
    "\n",
    "    return Model(input=inputs, output=path)\n",
    "\n",
    "class EL_GAN(): # Based on pix2pix\n",
    "    def __init__(self):\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN) better version\n",
    "        self.patch_size = 32\n",
    "        self.nb_patches = int((self.img_rows / self.patch_size) * (self.img_cols / self.patch_size))\n",
    "        self.patch_gan_dim = (self.patch_size, self.patch_size, self.channels)\n",
    "        \n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5) # Original\n",
    "        #optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # An old version of Pix2pix\n",
    "\n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator() # Old generator from \n",
    "        self.generator = self.build_res_unet_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        #img_A = Input(shape=self.img_shape) # Target\n",
    "        img_B = Input(shape=self.img_shape) # Input\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        #valid = self.discriminator([fake_A, img_B])\n",
    "        valid = self.discriminator([fake_A])\n",
    "\n",
    "        #self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        self.combined = Model(inputs= img_B, outputs=[valid, fake_A])\n",
    "        \n",
    "        # Original Pix2Pix - low weight for discriminator\n",
    "        self.combined.compile(loss=['mse', 'mae'],\n",
    "                              loss_weights=[1, 100], # 20190117: original [1, 100]\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    def build_res_unet_generator(self):\n",
    "        \"\"\"Residual U-Net Generator\"\"\"\n",
    "        \n",
    "        inputs = Input(shape=self.img_shape)\n",
    "        to_decoder = encoder(inputs)\n",
    "        path = res_block(to_decoder[2], [256, 256], [(2, 2), (1, 1)], increase = True) # 3x\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Number of block of bottleneck = 1\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.14, achieved best result ever\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.15\n",
    "        #path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.18 15:00 because of 1:25k\n",
    "        #path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.18 16:00 because of 1:25k\n",
    "        path = decoder(path, from_encoder=to_decoder)\n",
    "        path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path) \n",
    "\n",
    "        return Model(input=inputs, output=path)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "    \n",
    "    def build_PatchGanDiscriminator(self):\n",
    "        \"\"\"\n",
    "        Creates the generator according to the specs in the paper below.\n",
    "        [https://arxiv.org/pdf/1611.07004v1.pdf][5. Appendix]\n",
    "\n",
    "        PatchGAN only penalizes structure at the scale of patches. This\n",
    "        discriminator tries to classify if each N x N patch in an\n",
    "        image is real or fake. We run this discriminator convolutationally\n",
    "        across the image, averaging all responses to provide\n",
    "        the ultimate output of D.\n",
    "\n",
    "        The discriminator has two parts. First part is the actual discriminator\n",
    "        seconds part we make it a PatchGAN by running each image patch through the model\n",
    "        and then we average the responses\n",
    "\n",
    "        Discriminator does the following:\n",
    "        1. Runs many pieces of the image through the network\n",
    "        2. Calculates the cost for each patch\n",
    "        3. Returns the avg of the costs as the output of the network\n",
    "\n",
    "        :param patch_dim: (channels, width, height) T\n",
    "        :param nb_patches:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # -------------------------------\n",
    "        # DISCRIMINATOR\n",
    "        # C64-C128-C256-C512-C512-C512 (for 256x256)\n",
    "        # otherwise, it scales from 64\n",
    "        # 1 layer block = Conv - BN - LeakyRelu\n",
    "        # -------------------------------\n",
    "        \n",
    "        output_img_dim = self.img_shape\n",
    "        patch_dim = self.patch_gan_dim\n",
    "        input_layer = Input(shape=patch_dim)\n",
    "        \n",
    "        # We have to build the discriminator dinamically because\n",
    "        # the size of the disc patches is dynamic\n",
    "        num_filters_start = self.gf\n",
    "        nb_conv = int(np.floor(np.log(output_img_dim[1]) / np.log(2)))\n",
    "        filters_list = [num_filters_start * min(8, (2 ** i)) for i in range(nb_conv)]\n",
    "        \n",
    "        # CONV 1\n",
    "        # Do first conv bc it is different from the rest\n",
    "        # paper skips batch norm for first layer\n",
    "        disc_out = Conv2D(filters=64, kernel_size=(4, 4), padding='same', strides=(2, 2), name='disc_conv_1')(input_layer)\n",
    "        disc_out = LeakyReLU(alpha=0.2)(disc_out)\n",
    "        \n",
    "        # CONV 2 - CONV N\n",
    "        # do the rest of the convs based on the sizes from the filters\n",
    "        for i, filter_size in enumerate(filters_list[1:]):\n",
    "            name = 'disc_conv_{}'.format(i+2)\n",
    "\n",
    "            disc_out = Conv2D(filters=filter_size, kernel_size=(4, 4), padding='same', strides=(2, 2), name=name)(disc_out)\n",
    "            disc_out = BatchNormalization(name=name + '_bn')(disc_out)\n",
    "            disc_out = LeakyReLU(alpha=0.2)(disc_out)\n",
    "        \n",
    "        # ------------------------\n",
    "        # BUILD PATCH GAN\n",
    "        # this is where we evaluate the loss over each sublayer of the input\n",
    "        # ------------------------\n",
    "        patch_gan_discriminator = generate_patch_gan_loss(last_disc_conv_layer=disc_out,\n",
    "                                                          patch_dim=patch_dim,\n",
    "                                                          input_layer=input_layer,\n",
    "                                                          nb_patches=nb_patches)\n",
    "        return patch_gan_discriminator\n",
    "    \n",
    "    def build_2head_discriminator(self):\n",
    "        \n",
    "        def d_layer(layer_input, filters, f_size=3, bn=True): # Chnaged here for the order of bn and activation\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            conv = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')\n",
    "            d = conv(layer_input)\n",
    "            e = conv(layer_input2)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Activation(activation='relu')(d)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "        \n",
    "        def d_layers(img_A):\n",
    "            d1 = d_layer(img_A, self.df, bn=False)\n",
    "            d2 = d_layer(d1, self.df*2)\n",
    "            d3 = d_layer(d2, self.df*4)\n",
    "            d4 = d_layer(d3, self.df*8)\n",
    "            d5 = Flatten()(d4)\n",
    "            d6 = Dense(128, activation='softmax')(d5)\n",
    "            \n",
    "            return Model(img_A, d6)\n",
    "        \n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        img_B = Input(shape=self.img_shape)\n",
    "        \n",
    "        encoded_a = d_layers(img_A)\n",
    "        encoded_b = d_layers(img_B)\n",
    "        \n",
    "        # We can then concatenate the two vectors:\n",
    "        #merged_vector = keras.layers.concatenate([encoded_a, encoded_b], axis=-1)\n",
    "        \n",
    "        return Model([img_A, img_B], validity)\n",
    "        \n",
    "    \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=3, bn=True): # Chnaged here for the order of bn and activation\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Activation(activation='relu')(d)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        #img_B = Input(shape=self.img_shape)\n",
    "\n",
    "        ## Concatenate image and conditioning image by channels to produce input\n",
    "        #combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "\n",
    "        #d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        \n",
    "        d1 = d_layer(img_A, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A], validity)\n",
    "    \n",
    "    def train_generator_only(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        data_gen_args = dict(rotation_range=180.)\n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        \n",
    "        seed = 1\n",
    "        BATCH_SIZE = 16\n",
    "        result_generator = zip(image_datagen.flow(x_train_sim, batch_size=BATCH_SIZE, seed=seed), \n",
    "                               mask_datagen.flow(y_train_sim, batch_size=BATCH_SIZE, seed=seed))\n",
    "        \n",
    "        History1 = History()\n",
    "        hist1 = self.generator.fit_generator( result_generator,\n",
    "                                              epochs = 100,\n",
    "                                              steps_per_epoch=2000,\n",
    "                                              verbose=1,\n",
    "                                              shuffle=True,\n",
    "                                              callbacks=[History1, \n",
    "                                                         EarlyStopping(patience=5), \n",
    "                                                         ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                                         ModelCheckpoint(outPath + \"weights.hdf5\", \n",
    "                                                                         save_best_only = True, \n",
    "                                                                         save_weights_only = False)],\n",
    "                                              validation_data=(x_test_sim, y_test_sim))\n",
    "        save_hist(History1, outPath)\n",
    "        \n",
    "    \n",
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                #d_loss_real = self.discriminator.train_on_batch([imgs_A, imgs_B], valid)\n",
    "                #d_loss_fake = self.discriminator.train_on_batch([fake_A, imgs_B], fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs_A], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fake_A], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                self.discriminator.trainable = False\n",
    "                #g_loss = self.combined.train_on_batch([imgs_A, imgs_B], [valid, imgs_A])\n",
    "                g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "                self.discriminator.trainable = True\n",
    "                \n",
    "                elapsed_time = datetime.datetime.now() - start_time\n",
    "\n",
    "                # Plot the progress\n",
    "                if batch_i % sample_interval == 0:\n",
    "                    \n",
    "                    valid_test = np.ones((len(x_test_sim),) + self.disc_patch)\n",
    "                    #t_loss = self.combined.evaluate([y_test_sim, x_test_sim], [valid_test, y_test_sim], verbose=0)\n",
    "                    t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "                    \n",
    "                    print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] [Test loss&acc: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                100*t_loss[2], 100*t_loss[3], 100*t_loss[4],\n",
    "                                                                                elapsed_time))                 \n",
    "            if epoch > 10:\n",
    "                self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')\n",
    "                        \n",
    "                ## If at save interval => save generated image samples\n",
    "                #if batch_i % sample_interval == 0:\n",
    "                #    self.sample_images(outPath, epoch, batch_i)\n",
    "\n",
    "\n",
    "    def sample_images(self, outPath, epoch, batch_i, examples = [0, 77, 34]):\n",
    "        \n",
    "        r, c = 3, 3\n",
    "        p_size_1 = 128\n",
    "        \n",
    "        imgs_A = y_test_sim[examples]\n",
    "        imgs_B = x_test_sim[examples]\n",
    "        \n",
    "        fake_A = gan.generator.predict(imgs_B)\n",
    "\n",
    "        gen_imgs = np.concatenate([imgs_B, fake_A, imgs_A])\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        titles = ['Input', 'Generated', 'Target']\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                gen = np.reshape(gen_imgs[cnt], (p_size_1,p_size_1))\n",
    "                axs[i,j].imshow(gen)\n",
    "                \n",
    "                #axs[i,j].imshow(gen_imgs[cnt])\n",
    "                axs[i, j].set_title(titles[i])\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(outPath + \"%d_%d.png\" % (epoch, batch_i),\n",
    "                   format='png', transparent=True, dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the trains (32475, 128, 128, 1)\n",
      "Input Shape of the tests (3608, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "scale = 10\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the trains', x_train_sim.shape)\n",
    "print('Input Shape of the tests', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create\n",
    "\n",
    "from time import gmtime, strftime\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "\n",
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:220: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/26-0/1014] [D loss&acc: 1.700, 25.195%] [G loss&accA&accB: 42.985, 30.957%, 72.229%] [Test loss&acc: 31.763, 0.000%, 71.166%] time: 0:00:12.583044\n",
      "[Epoch 0/26-500/1014] [D loss&acc: 0.312, 12.305%] [G loss&accA&accB: 0.614, 4.785%, 99.395%] [Test loss&acc: 0.598, 0.000%, 99.409%] time: 0:03:23.226868\n",
      "[Epoch 0/26-1000/1014] [D loss&acc: 0.299, 11.865%] [G loss&accA&accB: 0.616, 7.666%, 99.388%] [Test loss&acc: 0.591, 0.000%, 99.411%] time: 0:06:33.688033\n",
      "[Epoch 1/26-0/1014] [D loss&acc: 0.301, 16.992%] [G loss&accA&accB: 0.797, 16.406%, 99.206%] [Test loss&acc: 0.591, 0.000%, 99.411%] time: 0:06:48.750666\n",
      "[Epoch 1/26-500/1014] [D loss&acc: 0.290, 15.674%] [G loss&accA&accB: 0.951, 8.496%, 99.050%] [Test loss&acc: 0.591, 0.000%, 99.410%] time: 0:09:57.437336\n",
      "[Epoch 1/26-1000/1014] [D loss&acc: 0.281, 32.251%] [G loss&accA&accB: 0.606, 2.148%, 99.396%] [Test loss&acc: 0.584, 0.000%, 99.417%] time: 0:13:06.465863\n",
      "[Epoch 2/26-0/1014] [D loss&acc: 0.290, 16.113%] [G loss&accA&accB: 0.388, 2.930%, 99.616%] [Test loss&acc: 0.585, 0.000%, 99.416%] time: 0:13:21.434744\n",
      "[Epoch 2/26-500/1014] [D loss&acc: 0.318, 3.076%] [G loss&accA&accB: 0.569, 0.586%, 99.431%] [Test loss&acc: 0.583, 0.820%, 99.419%] time: 0:16:31.145100\n",
      "[Epoch 2/26-1000/1014] [D loss&acc: 0.294, 9.692%] [G loss&accA&accB: 0.615, 7.764%, 99.388%] [Test loss&acc: 0.578, 9.330%, 99.424%] time: 0:19:40.880523\n",
      "[Epoch 3/26-0/1014] [D loss&acc: 0.312, 27.344%] [G loss&accA&accB: 0.557, 36.475%, 99.443%] [Test loss&acc: 0.577, 7.395%, 99.424%] time: 0:19:55.896535\n",
      "[Epoch 3/26-500/1014] [D loss&acc: 0.303, 11.694%] [G loss&accA&accB: 0.421, 1.123%, 99.580%] [Test loss&acc: 0.575, 3.458%, 99.427%] time: 0:23:04.190529\n",
      "[Epoch 3/26-1000/1014] [D loss&acc: 0.294, 10.376%] [G loss&accA&accB: 0.559, 10.205%, 99.442%] [Test loss&acc: 0.571, 4.134%, 99.429%] time: 0:26:12.236082\n",
      "[Epoch 4/26-0/1014] [D loss&acc: 0.288, 9.277%] [G loss&accA&accB: 0.579, 3.711%, 99.422%] [Test loss&acc: 0.570, 4.956%, 99.431%] time: 0:26:27.199042\n",
      "[Epoch 4/26-500/1014] [D loss&acc: 0.304, 7.642%] [G loss&accA&accB: 0.879, 4.541%, 99.122%] [Test loss&acc: 0.568, 19.665%, 99.433%] time: 0:29:35.560732\n",
      "[Epoch 4/26-1000/1014] [D loss&acc: 0.292, 25.562%] [G loss&accA&accB: 0.783, 17.871%, 99.218%] [Test loss&acc: 0.564, 25.640%, 99.436%] time: 0:32:43.968357\n",
      "[Epoch 5/26-0/1014] [D loss&acc: 0.292, 18.091%] [G loss&accA&accB: 0.568, 17.432%, 99.433%] [Test loss&acc: 0.561, 24.509%, 99.440%] time: 0:32:59.000357\n",
      "[Epoch 5/26-500/1014] [D loss&acc: 0.281, 43.774%] [G loss&accA&accB: 0.478, 0.098%, 99.522%] [Test loss&acc: 0.563, 22.532%, 99.437%] time: 0:36:07.453151\n",
      "[Epoch 5/26-1000/1014] [D loss&acc: 0.303, 5.322%] [G loss&accA&accB: 0.465, 2.930%, 99.535%] [Test loss&acc: 0.554, 23.709%, 99.447%] time: 0:39:15.568461\n",
      "[Epoch 6/26-0/1014] [D loss&acc: 0.305, 5.664%] [G loss&accA&accB: 0.324, 3.760%, 99.676%] [Test loss&acc: 0.555, 19.376%, 99.445%] time: 0:39:30.520917\n",
      "[Epoch 6/26-500/1014] [D loss&acc: 0.290, 11.475%] [G loss&accA&accB: 0.513, 2.100%, 99.487%] [Test loss&acc: 0.553, 21.020%, 99.448%] time: 0:42:38.988872\n",
      "[Epoch 6/26-1000/1014] [D loss&acc: 0.277, 18.921%] [G loss&accA&accB: 0.580, 2.588%, 99.421%] [Test loss&acc: 0.545, 14.402%, 99.456%] time: 0:45:46.921375\n",
      "[Epoch 7/26-0/1014] [D loss&acc: 0.277, 40.942%] [G loss&accA&accB: 0.366, 0.928%, 99.634%] [Test loss&acc: 0.548, 16.878%, 99.453%] time: 0:46:01.916679\n",
      "[Epoch 7/26-500/1014] [D loss&acc: 0.293, 5.835%] [G loss&accA&accB: 0.536, 7.373%, 99.465%] [Test loss&acc: 0.540, 38.202%, 99.460%] time: 0:49:10.237216\n",
      "[Epoch 7/26-1000/1014] [D loss&acc: 0.284, 10.327%] [G loss&accA&accB: 0.695, 9.033%, 99.306%] [Test loss&acc: 0.537, 33.721%, 99.464%] time: 0:52:17.844603\n",
      "[Epoch 8/26-0/1014] [D loss&acc: 0.280, 14.087%] [G loss&accA&accB: 0.533, 7.227%, 99.467%] [Test loss&acc: 0.537, 33.709%, 99.464%] time: 0:52:32.795399\n",
      "[Epoch 8/26-500/1014] [D loss&acc: 0.277, 26.636%] [G loss&accA&accB: 0.531, 3.711%, 99.470%] [Test loss&acc: 0.539, 9.421%, 99.462%] time: 0:55:40.363249\n",
      "[Epoch 8/26-1000/1014] [D loss&acc: 0.282, 8.838%] [G loss&accA&accB: 0.378, 6.689%, 99.622%] [Test loss&acc: 0.536, 1.705%, 99.465%] time: 0:58:47.343271\n",
      "[Epoch 9/26-0/1014] [D loss&acc: 0.280, 14.624%] [G loss&accA&accB: 0.341, 4.102%, 99.660%] [Test loss&acc: 0.522, 2.391%, 99.478%] time: 0:59:02.237284\n",
      "[Epoch 9/26-500/1014] [D loss&acc: 0.293, 6.836%] [G loss&accA&accB: 0.529, 3.418%, 99.471%] [Test loss&acc: 0.500, 21.706%, 99.500%] time: 1:02:08.925806\n",
      "[Epoch 9/26-1000/1014] [D loss&acc: 0.278, 12.622%] [G loss&accA&accB: 0.475, 6.885%, 99.525%] [Test loss&acc: 0.475, 31.546%, 99.525%] time: 1:05:15.439584\n",
      "[Epoch 10/26-0/1014] [D loss&acc: 0.288, 20.410%] [G loss&accA&accB: 0.632, 15.771%, 99.370%] [Test loss&acc: 0.473, 17.634%, 99.528%] time: 1:05:30.364478\n",
      "[Epoch 10/26-500/1014] [D loss&acc: 0.284, 13.037%] [G loss&accA&accB: 0.454, 20.996%, 99.548%] [Test loss&acc: 0.470, 1.042%, 99.531%] time: 1:08:36.921345\n",
      "[Epoch 10/26-1000/1014] [D loss&acc: 0.296, 28.418%] [G loss&accA&accB: 0.431, 50.732%, 99.569%] [Test loss&acc: 0.466, 31.514%, 99.535%] time: 1:11:43.204441\n",
      "[Epoch 11/26-0/1014] [D loss&acc: 0.285, 12.695%] [G loss&accA&accB: 0.550, 7.178%, 99.450%] [Test loss&acc: 0.459, 31.929%, 99.542%] time: 1:11:58.081182\n",
      "[Epoch 11/26-500/1014] [D loss&acc: 0.285, 21.240%] [G loss&accA&accB: 0.417, 36.621%, 99.583%] [Test loss&acc: 0.447, 0.372%, 99.554%] time: 1:15:04.627883\n",
      "[Epoch 11/26-1000/1014] [D loss&acc: 0.288, 28.223%] [G loss&accA&accB: 0.353, 1.807%, 99.648%] [Test loss&acc: 0.445, 23.553%, 99.556%] time: 1:18:11.034419\n",
      "[Epoch 12/26-0/1014] [D loss&acc: 0.289, 11.914%] [G loss&accA&accB: 0.429, 22.559%, 99.574%] [Test loss&acc: 0.443, 23.799%, 99.558%] time: 1:18:30.489117\n",
      "[Epoch 12/26-500/1014] [D loss&acc: 0.275, 38.159%] [G loss&accA&accB: 0.232, 0.928%, 99.769%] [Test loss&acc: 0.432, 30.108%, 99.568%] time: 1:21:36.792620\n",
      "[Epoch 12/26-1000/1014] [D loss&acc: 0.271, 32.715%] [G loss&accA&accB: 0.471, 2.100%, 99.531%] [Test loss&acc: 0.435, 31.640%, 99.565%] time: 1:24:43.107047\n",
      "[Epoch 13/26-0/1014] [D loss&acc: 0.283, 8.105%] [G loss&accA&accB: 0.438, 13.867%, 99.562%] [Test loss&acc: 0.427, 32.484%, 99.573%] time: 1:24:58.436184\n",
      "[Epoch 13/26-500/1014] [D loss&acc: 0.277, 9.912%] [G loss&accA&accB: 0.691, 5.322%, 99.309%] [Test loss&acc: 0.428, 20.566%, 99.572%] time: 1:28:04.738183\n",
      "[Epoch 13/26-1000/1014] [D loss&acc: 0.306, 5.957%] [G loss&accA&accB: 0.254, 2.539%, 99.746%] [Test loss&acc: 0.424, 15.858%, 99.577%] time: 1:31:11.141995\n",
      "[Epoch 14/26-0/1014] [D loss&acc: 0.297, 13.379%] [G loss&accA&accB: 0.311, 1.416%, 99.689%] [Test loss&acc: 0.426, 16.121%, 99.575%] time: 1:31:26.533193\n",
      "[Epoch 14/26-500/1014] [D loss&acc: 0.273, 22.827%] [G loss&accA&accB: 0.471, 12.256%, 99.531%] [Test loss&acc: 0.427, 33.609%, 99.573%] time: 1:34:33.007819\n",
      "[Epoch 14/26-1000/1014] [D loss&acc: 0.265, 32.642%] [G loss&accA&accB: 0.401, 10.400%, 99.599%] [Test loss&acc: 0.422, 29.203%, 99.578%] time: 1:37:39.302498\n",
      "[Epoch 15/26-0/1014] [D loss&acc: 0.263, 27.100%] [G loss&accA&accB: 0.309, 27.588%, 99.692%] [Test loss&acc: 0.429, 29.726%, 99.572%] time: 1:37:54.725006\n",
      "[Epoch 15/26-500/1014] [D loss&acc: 0.264, 29.102%] [G loss&accA&accB: 0.425, 38.135%, 99.575%] [Test loss&acc: 0.423, 32.486%, 99.577%] time: 1:41:01.143644\n",
      "[Epoch 15/26-1000/1014] [D loss&acc: 0.262, 24.390%] [G loss&accA&accB: 0.255, 18.115%, 99.746%] [Test loss&acc: 0.426, 31.200%, 99.574%] time: 1:44:07.349781\n",
      "[Epoch 16/26-0/1014] [D loss&acc: 0.275, 20.044%] [G loss&accA&accB: 0.460, 39.551%, 99.540%] [Test loss&acc: 0.423, 31.282%, 99.577%] time: 1:44:22.752691\n",
      "[Epoch 16/26-500/1014] [D loss&acc: 0.263, 24.731%] [G loss&accA&accB: 0.263, 16.504%, 99.736%] [Test loss&acc: 0.426, 31.829%, 99.574%] time: 1:47:28.842702\n",
      "[Epoch 16/26-1000/1014] [D loss&acc: 0.266, 26.392%] [G loss&accA&accB: 0.218, 42.822%, 99.783%] [Test loss&acc: 0.415, 31.713%, 99.585%] time: 1:50:35.044652\n",
      "[Epoch 17/26-0/1014] [D loss&acc: 0.261, 38.574%] [G loss&accA&accB: 0.237, 54.834%, 99.763%] [Test loss&acc: 0.422, 31.831%, 99.578%] time: 1:50:50.402316\n",
      "[Epoch 17/26-500/1014] [D loss&acc: 0.258, 48.755%] [G loss&accA&accB: 0.341, 5.273%, 99.659%] [Test loss&acc: 0.424, 32.029%, 99.576%] time: 1:53:56.617803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 17/26-1000/1014] [D loss&acc: 0.266, 27.075%] [G loss&accA&accB: 0.278, 21.240%, 99.722%] [Test loss&acc: 0.422, 31.952%, 99.578%] time: 1:57:02.597925\n",
      "[Epoch 18/26-0/1014] [D loss&acc: 0.273, 41.235%] [G loss&accA&accB: 0.376, 13.281%, 99.624%] [Test loss&acc: 0.426, 19.279%, 99.575%] time: 1:57:18.026738\n",
      "[Epoch 18/26-500/1014] [D loss&acc: 0.262, 36.914%] [G loss&accA&accB: 0.569, 64.111%, 99.432%] [Test loss&acc: 0.413, 30.414%, 99.587%] time: 2:00:24.273843\n",
      "[Epoch 18/26-1000/1014] [D loss&acc: 0.256, 31.372%] [G loss&accA&accB: 0.365, 14.160%, 99.636%] [Test loss&acc: 0.422, 27.942%, 99.578%] time: 2:03:30.297978\n",
      "[Epoch 19/26-0/1014] [D loss&acc: 0.256, 43.848%] [G loss&accA&accB: 0.292, 5.664%, 99.708%] [Test loss&acc: 0.420, 29.334%, 99.580%] time: 2:03:45.722449\n",
      "[Epoch 19/26-500/1014] [D loss&acc: 0.258, 36.279%] [G loss&accA&accB: 0.351, 49.561%, 99.649%] [Test loss&acc: 0.416, 27.141%, 99.584%] time: 2:06:51.625581\n",
      "[Epoch 19/26-1000/1014] [D loss&acc: 0.256, 36.816%] [G loss&accA&accB: 0.312, 51.562%, 99.688%] [Test loss&acc: 0.415, 17.500%, 99.585%] time: 2:09:57.748224\n",
      "[Epoch 20/26-0/1014] [D loss&acc: 0.258, 30.542%] [G loss&accA&accB: 0.327, 14.502%, 99.673%] [Test loss&acc: 0.411, 21.731%, 99.589%] time: 2:10:13.148538\n",
      "[Epoch 20/26-500/1014] [D loss&acc: 0.263, 36.182%] [G loss&accA&accB: 0.395, 8.643%, 99.606%] [Test loss&acc: 0.423, 20.189%, 99.578%] time: 2:13:19.030627\n",
      "[Epoch 20/26-1000/1014] [D loss&acc: 0.256, 43.457%] [G loss&accA&accB: 0.267, 4.932%, 99.733%] [Test loss&acc: 0.427, 23.405%, 99.573%] time: 2:16:24.952313\n",
      "[Epoch 21/26-0/1014] [D loss&acc: 0.259, 27.881%] [G loss&accA&accB: 0.258, 27.490%, 99.743%] [Test loss&acc: 0.413, 24.284%, 99.587%] time: 2:16:40.358760\n",
      "[Epoch 21/26-500/1014] [D loss&acc: 0.257, 35.449%] [G loss&accA&accB: 0.402, 46.924%, 99.598%] [Test loss&acc: 0.415, 18.516%, 99.586%] time: 2:19:45.969136\n",
      "[Epoch 21/26-1000/1014] [D loss&acc: 0.261, 36.475%] [G loss&accA&accB: 0.246, 65.723%, 99.756%] [Test loss&acc: 0.419, 19.623%, 99.581%] time: 2:22:51.695612\n",
      "[Epoch 22/26-0/1014] [D loss&acc: 0.256, 44.556%] [G loss&accA&accB: 0.319, 68.359%, 99.682%] [Test loss&acc: 0.415, 19.569%, 99.585%] time: 2:23:07.028362\n",
      "[Epoch 22/26-500/1014] [D loss&acc: 0.254, 54.419%] [G loss&accA&accB: 0.339, 27.344%, 99.661%] [Test loss&acc: 0.418, 18.855%, 99.583%] time: 2:26:12.899967\n",
      "[Epoch 22/26-1000/1014] [D loss&acc: 0.255, 37.646%] [G loss&accA&accB: 0.369, 10.596%, 99.631%] [Test loss&acc: 0.416, 11.708%, 99.584%] time: 2:29:18.722485\n",
      "[Epoch 23/26-0/1014] [D loss&acc: 0.256, 32.764%] [G loss&accA&accB: 0.273, 31.250%, 99.728%] [Test loss&acc: 0.414, 12.607%, 99.586%] time: 2:29:34.106320\n",
      "[Epoch 23/26-500/1014] [D loss&acc: 0.253, 44.019%] [G loss&accA&accB: 0.343, 25.098%, 99.658%] [Test loss&acc: 0.413, 15.967%, 99.587%] time: 2:32:39.723414\n",
      "[Epoch 23/26-1000/1014] [D loss&acc: 0.256, 47.974%] [G loss&accA&accB: 0.189, 7.471%, 99.811%] [Test loss&acc: 0.410, 14.735%, 99.590%] time: 2:35:45.455643\n",
      "[Epoch 24/26-0/1014] [D loss&acc: 0.258, 35.596%] [G loss&accA&accB: 0.385, 48.633%, 99.615%] [Test loss&acc: 0.415, 14.845%, 99.585%] time: 2:36:00.895942\n",
      "[Epoch 24/26-500/1014] [D loss&acc: 0.252, 50.244%] [G loss&accA&accB: 0.157, 5.713%, 99.843%] [Test loss&acc: 0.409, 13.524%, 99.591%] time: 2:39:06.816876\n",
      "[Epoch 24/26-1000/1014] [D loss&acc: 0.252, 46.265%] [G loss&accA&accB: 0.202, 44.727%, 99.798%] [Test loss&acc: 0.419, 11.266%, 99.581%] time: 2:42:12.684540\n",
      "[Epoch 25/26-0/1014] [D loss&acc: 0.255, 32.520%] [G loss&accA&accB: 0.463, 37.451%, 99.536%] [Test loss&acc: 0.415, 11.358%, 99.585%] time: 2:42:28.077177\n",
      "[Epoch 25/26-500/1014] [D loss&acc: 0.255, 28.003%] [G loss&accA&accB: 1.304, 65.625%, 98.696%] [Test loss&acc: 0.417, 12.232%, 99.583%] time: 2:45:33.833508\n",
      "[Epoch 25/26-1000/1014] [D loss&acc: 0.254, 46.533%] [G loss&accA&accB: 0.351, 9.131%, 99.649%] [Test loss&acc: 0.428, 13.512%, 99.572%] time: 2:48:39.670603\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    gan = EL_GAN() # 26 Epochs, 32 \n",
    "    # size 32 - 26 Epochs\n",
    "    # size 8 - 20 Epoch\n",
    "    gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs=26, batch_size=32, sample_interval=500)\n",
    "    #gan.train_generator_only(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from sklearn.metrics import accuracy_score, classification_report\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    def IoUcheck(img_input, img_output):\n",
    "\n",
    "        logic_and = np.sum(np.logical_and(img_output, img_input))\n",
    "        logic_or = np.sum(np.logical_or(img_output, img_input))\n",
    "\n",
    "        return logic_and/logic_or\n",
    "    \n",
    "    def rescaleImg(image_arr):\n",
    "    \n",
    "        if image_arr.shape[0] % 8 != 0:\n",
    "            n = image_arr.shape[0] % 8\n",
    "            new_x = image_arr.shape[0] - n\n",
    "        else:\n",
    "            new_x = image_arr.shape[0]\n",
    "\n",
    "        if image_arr.shape[1] % 8 != 0:\n",
    "            n = image_arr.shape[1] % 8\n",
    "            new_y = image_arr.shape[1] - n\n",
    "        else:\n",
    "            new_y = image_arr.shape[1]\n",
    "\n",
    "        image_arr = image_arr[:new_x, :new_y]\n",
    "\n",
    "        return image_arr\n",
    "    \n",
    "    def update_model_to_any_size(old_model):\n",
    "    \n",
    "        old_model.layers.pop(0)\n",
    "        \n",
    "        newInput = Input(shape=(None, None, 1)) # New image input\n",
    "        newOutputs = old_model(newInput)\n",
    "        newModel = Model(newInput, newOutputs)\n",
    "        #newModel.summary()\n",
    "\n",
    "        return newModel\n",
    "    \n",
    "    def evaluate(image_arrA, image_arrB):\n",
    "        \n",
    "        y_true = image_arrB.flatten().astype(bool) \n",
    "        y_pred = image_arrA.flatten().astype(bool)\n",
    "        \n",
    "        Accuracy = accuracy_score(image_arrB.flatten().astype(bool), \n",
    "                                  image_arrA.flatten().astype(bool))\n",
    "\n",
    "        IntOverUnion = IoUcheck(image_arrB.flatten().astype(bool), \n",
    "                                image_arrA.flatten().astype(bool))\n",
    "        \n",
    "        conf = confusion_matrix(image_arrB.flatten().astype(bool), \n",
    "                                image_arrA.flatten().astype(bool))\n",
    "        print('aAcc:', Accuracy)\n",
    "        print('Error:', 1 - Accuracy)\n",
    "        print('IoU:', IntOverUnion)\n",
    "        print('Confusion:', conf)\n",
    "        \n",
    "        target_names = ['0', '1']\n",
    "        print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "        \n",
    "        return Accuracy, IntOverUnion\n",
    "        \n",
    "    def model_predict(newModel, input_image, num_runs):\n",
    "        m,n = input_image.shape\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            input_image = np.reshape(input_image, (1, m, n, 1))\n",
    "            conc2 = newModel.predict([input_image])\n",
    "            input_image = np.reshape(conc2,(m, n))\n",
    "\n",
    "        return input_image\n",
    "    \n",
    "    def save_prediction(output_image, fn_input, subfix):\n",
    "        fig = plt.figure(figsize=(output_image.shape[1] / 1000, output_image.shape[0] / 1000), dpi=100, frameon=False)\n",
    "        ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
    "        ax.set_axis_off()\n",
    "        fig.add_axes(ax)\n",
    "\n",
    "        plt.imshow(output_image, cmap='gray')\n",
    "        #plt.imshow(output_image)\n",
    "        fig.savefig(outPath + fn_input[:-4] + subfix, dpi=1000)\n",
    "\n",
    "    def predict_15k_examples(testPath, fn_input, fn_target, nr = 1):\n",
    "    \n",
    "        image_arrA = readImg(testPath + fn_input)\n",
    "        image_arrB = readImg(testPath + fn_target)\n",
    "\n",
    "        print(\"15k\", 'Example: ')\n",
    "        acc_orig, iou_orig = evaluate(image_arrA, image_arrB)\n",
    "\n",
    "        image_arr = readImg(testPath + fn_input)\n",
    "        image_arr = rescaleImg(image_arr)\n",
    "\n",
    "        image_tar = readImg(testPath + fn_target)\n",
    "        image_tar = rescaleImg(image_tar)\n",
    "\n",
    "        newModel = update_model_to_any_size(gan.generator)\n",
    "        output_image = model_predict(newModel, image_arr, num_runs = nr)\n",
    "\n",
    "        print(\"- 15k\", 'Prediction: ')\n",
    "        acc_pred, iou_pred = evaluate(output_image > 0.5, image_tar)\n",
    "\n",
    "        save_prediction(output_image, fn_input, '_' + str(nr) + '_out.png')\n",
    "\n",
    "        return [[acc_orig, iou_orig], [acc_pred, iou_pred]]\n",
    "    \n",
    "    def predict_15k_only(testPath, fn_input, nr = 1):\n",
    "    \n",
    "        print(\"15k\", 'Example: ')\n",
    "        image_arr = readImg(testPath + fn_input)\n",
    "        image_arr = rescaleImg(image_arr)\n",
    "\n",
    "        newModel = update_model_to_any_size(gan.generator)\n",
    "        output_image = model_predict(newModel, image_arr, num_runs = nr)\n",
    "\n",
    "        print(\"- 15k\", 'Prediction: ')\n",
    "        acc_pred, iou_pred = evaluate(output_image > 0.5, image_tar)\n",
    "\n",
    "        save_prediction(output_image, fn_input, '_' + str(nr) + '_out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testPath = r\"../tmp_data/Data/Testing/\"\n",
    "all_records = []\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\", nr = 1)\n",
    "all_records.extend(records)\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\", nr = 1)\n",
    "all_records.extend(records)\n",
    "\n",
    "records = predict_15k_examples(testPath, r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\", nr = 10)\n",
    "all_records.extend(records)\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\", nr = 10)\n",
    "all_records.extend(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(np.transpose(all_records))\n",
    "df.columns = [\"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 1)\", \n",
    "              \"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 1)\",\n",
    "              \"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 10)\", \n",
    "              \"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 10)\"]\n",
    "\n",
    "df = df.rename({0: \"Accuracy\", 1: 'IoU'})\n",
    "df.index.name = 'Metrics'\n",
    "\n",
    "df[[\"Input vs Target (Test1)\", \"Prediction vs Target (Test1 - 1)\", \"Prediction vs Target (Test1 - 10)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df[[\"Input vs Target (Test2)\", \"Prediction vs Target (Test2 - 1)\", \"Prediction vs Target (Test2 - 10)\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gan.generator.summary()\n",
    "gan.discriminator.summary()\n",
    "gan.combined.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testPath = r\"../tmp_data/Data/Testing_large/4270/\"\n",
    "all_records = []\n",
    "\n",
    "records = predict_15k_examples(testPath, r\"geb_clip_4270.png\", r\"geb15_clip_4270.png\", nr = 1)\n",
    "all_records.extend(records)\n",
    "    \n",
    "records = predict_15k_examples(testPath, r\"geb_clip_4270.png\", r\"geb15_clip_4270.png\", nr = 10)\n",
    "all_records.extend(records)\n",
    "\n",
    "df = pd.DataFrame(np.transpose(all_records))\n",
    "df.columns = [\"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\", \n",
    "              \"Input vs Target (4270)\", \"Prediction vs Target (4270 - 1)\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fn_input, fn_target = r\"geb_clip_4270.png\", r\"geb15_clip_4270.png\"\n",
    "image_arrA = readImg(testPath + fn_input)\n",
    "image_arrB = readImg(testPath + fn_target)\n",
    "\n",
    "print(\"15k\", 'Example: ')\n",
    "acc_orig, iou_orig = evaluate(image_arrA, image_arrB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image_arr = readImg(testPath + fn_input)\n",
    "image_arr = rescaleImg(image_arr)[:2400, :2400]\n",
    "\n",
    "image_tar = readImg(testPath + fn_target)\n",
    "image_tar = rescaleImg(image_tar)[:2400, :2400]\n",
    "print(image_tar.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newModel = update_model_to_any_size(gan.generator)\n",
    "output_image = model_predict(newModel, image_arr, num_runs = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"- 15k\", 'Prediction: ')\n",
    "acc_pred, iou_pred = evaluate(output_image > 0.5, image_tar)\n",
    "nr = 1\n",
    "save_prediction(output_image, fn_input, '_' + str(nr) + '_out.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "newModel = update_model_to_any_size(gan.generator)\n",
    "newModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
