{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def iou_coef(y_true, y_pred):\n",
    "    dice = dice_coef(y_true, y_pred)\n",
    "    iou = dice / (2 - dice)\n",
    "    return iou\n",
    "\n",
    "def iou_loss(y_true, y_pred):\n",
    "    return 1 - iou_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual u-net \n",
    "# Reference: https://github.com/DuFanXin/deep_residual_unet/blob/master/res_unet.py\n",
    "\n",
    "def res_block(x, nb_filters, strides, increase = False):\n",
    "    # This implementation used the double 3x3 structure and followed the Identity Mappings\n",
    "    res_path = BatchNormalization()(x)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    \n",
    "    res_path = Conv2D(filters=nb_filters[0], kernel_size=(3, 3), padding='same', strides=strides[0])(res_path)\n",
    "    res_path = BatchNormalization()(res_path)\n",
    "    res_path = Activation(activation='relu')(res_path)\n",
    "    res_path = Conv2D(filters=nb_filters[1], kernel_size=(3, 3), padding='same', strides=strides[1])(res_path)\n",
    "    \n",
    "    if increase:\n",
    "        shortcut = Conv2D(nb_filters[1], kernel_size=(1, 1), strides=strides[0])(x)\n",
    "        shortcut = BatchNormalization()(shortcut)\n",
    "    else:\n",
    "        shortcut = x\n",
    "\n",
    "    res_path = Add()([shortcut, res_path])\n",
    "    return res_path\n",
    "\n",
    "def decoder(x, from_encoder):\n",
    "    main_path = UpSampling2D(size=(2, 2))(x)\n",
    "    #main_path = Conv2D(filters=128, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path) # Add 2019.03.07\n",
    "    \n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[2]])\n",
    "    main_path = res_block(main_path, [128, 128], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    main_path = UpSampling2D(size=(2, 2))(main_path)\n",
    "    #main_path = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path) # Add 2019.03.07\n",
    "    \n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[1]])\n",
    "    main_path = res_block(main_path, [64, 64], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    main_path = UpSampling2D(size=(2, 2))(main_path)\n",
    "    #main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path) # Add 2019.03.07\n",
    "    \n",
    "    main_path = Concatenate(axis=3)([main_path, from_encoder[0]])\n",
    "    main_path = res_block(main_path, [32, 32], [(1, 1), (1, 1)], increase = True)\n",
    "\n",
    "    return main_path\n",
    "\n",
    "def encoder(x):\n",
    "    to_decoder = []\n",
    "\n",
    "    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(x)\n",
    "    main_path = BatchNormalization()(main_path)\n",
    "    main_path = Activation(activation='relu')(main_path)\n",
    "    main_path = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=(1, 1))(main_path)\n",
    "\n",
    "    shortcut = Conv2D(filters=32, kernel_size=(1, 1), strides=(1, 1))(x)\n",
    "    shortcut = BatchNormalization()(shortcut)\n",
    "\n",
    "    main_path = Add()([shortcut, main_path])\n",
    "    # first branching to decoder\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    main_path = res_block(main_path, [64, 64], [(2, 2), (1, 1)], increase = True)\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    main_path = res_block(main_path, [128, 128], [(2, 2), (1, 1)], increase = True)\n",
    "    to_decoder.append(main_path)\n",
    "\n",
    "    return to_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN4MapGen(): # Based on u-net, residual u-net and pix2pix\n",
    "    # Reference: https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = 128\n",
    "        self.img_cols = 128\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        self.clip_value = 0.01\n",
    "        \n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN) better version\n",
    "        self.patch_size = 32\n",
    "        self.nb_patches = int((self.img_rows / self.patch_size) * (self.img_cols / self.patch_size))\n",
    "        self.patch_gan_dim = (self.patch_size, self.patch_size, self.channels)\n",
    "        \n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        #optimizer = Adam(0.0002, 0.5) # Original\n",
    "        #optimizer = Adam(0.0001, 0.5) # Original # Latest achieved by 0.00008\n",
    "        #optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # An old version of Pix2pix\n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator() # Old generator from \n",
    "        self.generator = self.build_res_unet_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        #img_A = Input(shape=self.img_shape) # Target\n",
    "        img_B = Input(shape=self.img_shape) # Input\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator() # 1.Version\n",
    "        self.discriminator = self.build_discriminator_patchgan()\n",
    "        \n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        #valid = self.discriminator([fake_A, img_B])\n",
    "        #self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        \n",
    "        valid = self.discriminator([fake_A])\n",
    "        self.combined = Model(inputs= img_B, outputs=[valid, fake_A])\n",
    "        \n",
    "        # Original Pix2Pix - low weight for discriminator\n",
    "        self.combined.compile(loss=['mse', 'mae'], #['mse', 'mae'] original\n",
    "                              loss_weights=[1, 100], # [1, 100] original\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    def build_res_unet_generator(self):\n",
    "        \"\"\"Residual U-Net Generator\"\"\"\n",
    "        \n",
    "        inputs = Input(shape=self.img_shape)\n",
    "        to_decoder = encoder(inputs)\n",
    "        path = res_block(to_decoder[2], [256, 256], [(2, 2), (1, 1)], increase = True) # 3x\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Number of block of bottleneck = 1\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.14, achieved best result ever\n",
    "        path = decoder(path, from_encoder=to_decoder)\n",
    "        path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path) \n",
    "\n",
    "        return Model(input=inputs, output=path)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "    \n",
    "    def build_discriminator_simple(self):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=self.hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "\n",
    "        d9 = Dense(self.df*16)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "    \n",
    "    def build_discriminator_patchgan(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "        \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=3, bn=True): # Chnaged here for the order of bn and activation\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Activation(activation='relu')(d)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        \n",
    "        # !!!! This step was deleted because the condition do not help in this case\n",
    "        #img_B = Input(shape=self.img_shape)\n",
    "        ## Concatenate image and conditioning image by channels to produce input\n",
    "        #combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "        #d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        \n",
    "        d1 = d_layer(img_A, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A], validity)\n",
    "    \n",
    "    def train_generator_only(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        gen = self.build_res_unet_generator()\n",
    "        #optimizer = Adam(0.0001, 0.5) \n",
    "        #optimizer = Adam()\n",
    "        #optimizer = Adam(0.0004) # Current best\n",
    "        #optimizer = RMSprop() #\n",
    "        #optimizer = RMSprop(lr=0.0001) # Worse\n",
    "        #optimizer = Adadelta(0.0001) # default okay, 0.0001 worse\n",
    "        optimizer = Adam(0.0004, 0.5)\n",
    "        \n",
    "        # Loss function finished\n",
    "        gen.compile(loss='mse', optimizer=optimizer, metrics=['accuracy']) # mse better than mae\n",
    "        #gen.compile(loss='mae', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #gen.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #gen.compile(loss=dice_coef_loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        data_gen_args = dict(rotation_range=180.)\n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        \n",
    "        seed = 1\n",
    "        BATCH_SIZE = 16\n",
    "        result_generator = zip(image_datagen.flow(x_train_sim, batch_size=BATCH_SIZE, seed=seed), \n",
    "                               mask_datagen.flow(y_train_sim, batch_size=BATCH_SIZE, seed=seed))\n",
    "        \n",
    "        History1 = History()\n",
    "        hist1 = gen.fit_generator(result_generator,\n",
    "                                  epochs = 100,\n",
    "                                  steps_per_epoch=2000,\n",
    "                                  verbose=1,\n",
    "                                  shuffle=True,\n",
    "                                  callbacks=[History1, \n",
    "                                             EarlyStopping(patience=4), \n",
    "                                             ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                             ModelCheckpoint(outPath + \"weights.h5\", \n",
    "                                                             save_best_only = True, \n",
    "                                                             save_weights_only = False)],\n",
    "                                  validation_data=(x_test_sim, y_test_sim))\n",
    "        save_hist(History1, outPath)\n",
    "        \n",
    "    \n",
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        valid_acc = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        patience = 3\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs_A], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fake_A], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                ## Clip critic weights\n",
    "                #for l in self.discriminator.layers:\n",
    "                #    weights = l.get_weights()\n",
    "                #    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                #    l.set_weights(weights)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                #self.discriminator.trainable = False\n",
    "                g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "                #self.discriminator.trainable = True\n",
    "                \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    elapsed_time = datetime.datetime.now() - start_time\n",
    "                    print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                elapsed_time))    \n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch >= 0:\n",
    "                    \n",
    "                    elapsed_time = datetime.datetime.now() - start_time\n",
    "                    \n",
    "                    valid_test = np.ones((len(x_test_sim),) + self.disc_patch)\n",
    "                    t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "                    \n",
    "                    print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] [Test loss&acc: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                100*t_loss[2], 100*t_loss[3], 100*t_loss[4],\n",
    "                                                                                elapsed_time))    \n",
    "                    \n",
    "                    train_loss.append(g_loss[2])\n",
    "                    train_acc.append(g_loss[4])\n",
    "                    valid_loss.append(t_loss[2])\n",
    "                    valid_acc.append(t_loss[4])\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    waited = len(valid_loss) - 1 - np.argmin(valid_loss)\n",
    "                    print('waited for', waited, valid_loss)\n",
    "                    \n",
    "                    if waited == 0:\n",
    "                        self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')   \n",
    "                        \n",
    "                    if waited > patience:\n",
    "                        break\n",
    "            \n",
    "        return train_acc, train_loss, valid_acc, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the trains (32289, 128, 128, 1)\n",
      "Input Shape of the tests (3587, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "scale = 15\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the trains', x_train_sim.shape)\n",
    "print('Input Shape of the tests', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  \n",
    "\n",
    "def save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath):    \n",
    "    ### Save history\n",
    "    History1_loss = train_loss\n",
    "    History1_acc = train_acc\n",
    "    History1_val_loss = valid_loss\n",
    "    History1_val_acc = valid_acc\n",
    "\n",
    "    thefile1 = open(outPath + 'History1_loss.txt', 'w')\n",
    "    for item in History1_loss:\n",
    "        thefile1.write(\"%s\\n\" % item)\n",
    "    thefile1.close()\n",
    "\n",
    "    thefile2 = open(outPath + 'History1_acc.txt', 'w')\n",
    "    for item in History1_acc:\n",
    "        thefile2.write(\"%s\\n\" % item)\n",
    "    thefile2.close()\n",
    "\n",
    "    thefile3 = open(outPath + 'History1_val_loss.txt', 'w')\n",
    "    for item in History1_val_loss:\n",
    "        thefile3.write(\"%s\\n\" % item)\n",
    "    thefile3.close()\n",
    "\n",
    "    thefile4 = open(outPath + 'History1_val_acc.txt', 'w')\n",
    "    for item in History1_val_acc:\n",
    "        thefile4.write(\"%s\\n\" % item)\n",
    "    thefile4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residual unet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)\n",
    "print(outPath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen()\n",
    "gan.train_generator_only(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residual unet + PatchGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp_results/predictions/GAN_2019-03-07 20-17-46_15/\n"
     ]
    }
   ],
   "source": [
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "timestr = strftime(\"GAN_%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)\n",
    "print(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"co...)`\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50-0/2018] [D loss&acc: 20.881, 10.986%] [G loss&accA&accB: 39.387, 0.000%, 30.449%] time: 0:00:12.684488\n",
      "[Epoch 0/50-500/2018] [D loss&acc: 0.271, 50.000%] [G loss&accA&accB: 1.032, 39.062%, 98.834%] time: 0:02:18.276287\n",
      "[Epoch 0/50-1000/2018] [D loss&acc: 0.262, 49.219%] [G loss&accA&accB: 0.919, 40.625%, 98.920%] time: 0:04:23.095641\n",
      "[Epoch 0/50-1500/2018] [D loss&acc: 0.258, 50.000%] [G loss&accA&accB: 0.570, 50.000%, 99.339%] time: 0:06:28.541506\n",
      "[Epoch 0/50-2000/2018] [D loss&acc: 0.256, 50.000%] [G loss&accA&accB: 0.995, 40.625%, 98.824%] time: 0:08:33.818740\n",
      "[Epoch 0/50-2016/2018] [D loss&acc: 0.256, 50.000%] [G loss&accA&accB: 0.650, 40.625%, 99.280%] [Test loss&acc: 1.182, 0.000%, 98.568%] time: 0:08:37.798968\n",
      "waited for 0 [0.011817967645607937]\n",
      "[Epoch 1/50-0/2018] [D loss&acc: 0.256, 50.000%] [G loss&accA&accB: 0.633, 40.625%, 99.239%] time: 0:08:53.302133\n",
      "[Epoch 1/50-500/2018] [D loss&acc: 0.255, 50.000%] [G loss&accA&accB: 0.473, 40.625%, 99.441%] time: 0:10:57.478851\n",
      "[Epoch 1/50-1000/2018] [D loss&acc: 0.254, 50.000%] [G loss&accA&accB: 0.727, 42.188%, 99.118%] time: 0:13:01.508893\n",
      "[Epoch 1/50-1500/2018] [D loss&acc: 0.254, 50.000%] [G loss&accA&accB: 0.495, 42.188%, 99.388%] time: 0:15:05.682284\n",
      "[Epoch 1/50-2000/2018] [D loss&acc: 0.254, 49.219%] [G loss&accA&accB: 0.405, 41.699%, 99.532%] time: 0:17:09.521198\n",
      "[Epoch 1/50-2016/2018] [D loss&acc: 0.254, 50.000%] [G loss&accA&accB: 0.788, 41.699%, 99.049%] [Test loss&acc: 0.669, 0.000%, 99.187%] time: 0:17:13.515268\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587]\n",
      "[Epoch 2/50-0/2018] [D loss&acc: 0.254, 50.000%] [G loss&accA&accB: 0.668, 42.188%, 99.184%] time: 0:17:24.672707\n",
      "[Epoch 2/50-500/2018] [D loss&acc: 0.253, 49.902%] [G loss&accA&accB: 0.716, 42.188%, 99.134%] time: 0:19:29.353762\n",
      "[Epoch 2/50-1000/2018] [D loss&acc: 0.253, 46.143%] [G loss&accA&accB: 0.584, 32.324%, 99.233%] time: 0:21:34.514377\n",
      "[Epoch 2/50-1500/2018] [D loss&acc: 0.253, 38.232%] [G loss&accA&accB: 0.669, 16.211%, 99.194%] time: 0:23:40.255970\n",
      "[Epoch 2/50-2000/2018] [D loss&acc: 0.253, 42.529%] [G loss&accA&accB: 0.639, 19.434%, 99.258%] time: 0:25:45.411595\n",
      "[Epoch 2/50-2016/2018] [D loss&acc: 0.253, 36.230%] [G loss&accA&accB: 0.593, 14.355%, 99.246%] [Test loss&acc: 0.602, 0.000%, 99.265%] time: 0:25:49.479454\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423]\n",
      "[Epoch 3/50-0/2018] [D loss&acc: 0.253, 41.943%] [G loss&accA&accB: 0.573, 19.238%, 99.306%] time: 0:26:00.656800\n",
      "[Epoch 3/50-500/2018] [D loss&acc: 0.253, 40.332%] [G loss&accA&accB: 0.258, 14.062%, 99.702%] time: 0:28:06.458863\n",
      "[Epoch 3/50-1000/2018] [D loss&acc: 0.253, 44.287%] [G loss&accA&accB: 0.520, 14.062%, 99.318%] time: 0:30:11.923861\n",
      "[Epoch 3/50-1500/2018] [D loss&acc: 0.253, 47.900%] [G loss&accA&accB: 0.680, 14.062%, 99.173%] time: 0:32:17.618826\n",
      "[Epoch 3/50-2000/2018] [D loss&acc: 0.253, 47.998%] [G loss&accA&accB: 0.796, 14.062%, 98.997%] time: 0:34:23.211057\n",
      "[Epoch 3/50-2016/2018] [D loss&acc: 0.253, 48.926%] [G loss&accA&accB: 0.814, 14.062%, 99.005%] [Test loss&acc: 0.594, 0.000%, 99.277%] time: 0:34:27.226459\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509]\n",
      "[Epoch 4/50-0/2018] [D loss&acc: 0.253, 49.316%] [G loss&accA&accB: 0.596, 14.062%, 99.311%] time: 0:34:38.323065\n",
      "[Epoch 4/50-500/2018] [D loss&acc: 0.253, 49.805%] [G loss&accA&accB: 0.554, 14.062%, 99.292%] time: 0:36:44.806441\n",
      "[Epoch 4/50-1000/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.468, 14.062%, 99.416%] time: 0:38:50.782287\n",
      "[Epoch 4/50-1500/2018] [D loss&acc: 0.253, 49.805%] [G loss&accA&accB: 0.543, 14.062%, 99.326%] time: 0:40:56.928108\n",
      "[Epoch 4/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.561, 14.062%, 99.313%] time: 0:43:02.878790\n",
      "[Epoch 4/50-2016/2018] [D loss&acc: 0.253, 49.805%] [G loss&accA&accB: 0.646, 14.062%, 99.222%] [Test loss&acc: 0.583, 0.000%, 99.285%] time: 0:43:06.867443\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269]\n",
      "[Epoch 5/50-0/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.676, 14.062%, 99.142%] time: 0:43:18.008009\n",
      "[Epoch 5/50-500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.588, 14.062%, 99.305%] time: 0:45:23.583253\n",
      "[Epoch 5/50-1000/2018] [D loss&acc: 0.253, 49.756%] [G loss&accA&accB: 0.357, 14.062%, 99.594%] time: 0:47:29.042290\n",
      "[Epoch 5/50-1500/2018] [D loss&acc: 0.253, 49.463%] [G loss&accA&accB: 0.510, 14.062%, 99.380%] time: 0:49:34.753102\n",
      "[Epoch 5/50-2000/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.761, 14.062%, 99.133%] time: 0:51:40.472789\n",
      "[Epoch 5/50-2016/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.590, 14.062%, 99.256%] [Test loss&acc: 0.569, 0.000%, 99.309%] time: 0:51:44.509581\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379]\n",
      "[Epoch 6/50-0/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.497, 14.062%, 99.381%] time: 0:51:55.695581\n",
      "[Epoch 6/50-500/2018] [D loss&acc: 0.253, 49.707%] [G loss&accA&accB: 1.039, 14.062%, 98.808%] time: 0:54:01.144561\n",
      "[Epoch 6/50-1000/2018] [D loss&acc: 0.253, 49.756%] [G loss&accA&accB: 0.636, 14.062%, 99.197%] time: 0:56:06.814323\n",
      "[Epoch 6/50-1500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.417, 14.062%, 99.490%] time: 0:58:12.799340\n",
      "[Epoch 6/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.732, 14.062%, 99.109%] time: 1:00:18.338323\n",
      "[Epoch 6/50-2016/2018] [D loss&acc: 0.253, 49.854%] [G loss&accA&accB: 0.419, 14.062%, 99.475%] [Test loss&acc: 0.564, 0.000%, 99.320%] time: 1:00:22.411517\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755]\n",
      "[Epoch 7/50-0/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.724, 14.062%, 99.079%] time: 1:00:33.525513\n",
      "[Epoch 7/50-500/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.484, 14.062%, 99.397%] time: 1:02:39.671413\n",
      "[Epoch 7/50-1000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.292, 14.062%, 99.676%] time: 1:04:45.302061\n",
      "[Epoch 7/50-1500/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.735, 14.062%, 99.126%] time: 1:06:50.735248\n",
      "[Epoch 7/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.349, 14.062%, 99.572%] time: 1:08:56.246837\n",
      "[Epoch 7/50-2016/2018] [D loss&acc: 0.253, 49.902%] [G loss&accA&accB: 0.770, 14.062%, 99.101%] [Test loss&acc: 0.562, 0.000%, 99.316%] time: 1:09:00.223380\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185]\n",
      "[Epoch 8/50-0/2018] [D loss&acc: 0.253, 49.902%] [G loss&accA&accB: 0.380, 14.062%, 99.564%] time: 1:09:11.383138\n",
      "[Epoch 8/50-500/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.241, 14.062%, 99.702%] time: 1:11:16.178784\n",
      "[Epoch 8/50-1000/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.398, 14.062%, 99.522%] time: 1:13:20.799928\n",
      "[Epoch 8/50-1500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.380, 14.062%, 99.540%] time: 1:15:25.518489\n",
      "[Epoch 8/50-2000/2018] [D loss&acc: 0.253, 49.854%] [G loss&accA&accB: 0.629, 14.062%, 99.237%] time: 1:17:30.273652\n",
      "[Epoch 8/50-2016/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.534, 14.062%, 99.367%] [Test loss&acc: 0.561, 0.000%, 99.318%] time: 1:17:34.331300\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185, 0.005613656293425282]\n",
      "[Epoch 9/50-0/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.423, 14.062%, 99.498%] time: 1:17:45.473266\n",
      "[Epoch 9/50-500/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.413, 14.062%, 99.466%] time: 1:19:50.617747\n",
      "[Epoch 9/50-1000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.533, 14.062%, 99.353%] time: 1:21:55.426476\n",
      "[Epoch 9/50-1500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.241, 14.062%, 99.695%] time: 1:24:00.132895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/50-2000/2018] [D loss&acc: 0.253, 49.756%] [G loss&accA&accB: 0.268, 14.062%, 99.677%] time: 1:26:04.896436\n",
      "[Epoch 9/50-2016/2018] [D loss&acc: 0.253, 49.121%] [G loss&accA&accB: 0.480, 14.062%, 99.435%] [Test loss&acc: 0.555, 0.000%, 99.331%] time: 1:26:08.856921\n",
      "waited for 0 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185, 0.005613656293425282, 0.005552248891021362]\n",
      "[Epoch 10/50-0/2018] [D loss&acc: 0.253, 49.561%] [G loss&accA&accB: 0.553, 14.062%, 99.339%] time: 1:26:19.972951\n",
      "[Epoch 10/50-500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.640, 14.062%, 99.275%] time: 1:28:24.568451\n",
      "[Epoch 10/50-1000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.415, 14.062%, 99.506%] time: 1:30:29.242370\n",
      "[Epoch 10/50-1500/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.304, 14.062%, 99.619%] time: 1:32:33.764664\n",
      "[Epoch 10/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.544, 14.062%, 99.350%] time: 1:34:38.289037\n",
      "[Epoch 10/50-2016/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.462, 14.062%, 99.417%] [Test loss&acc: 0.566, 0.000%, 99.320%] time: 1:34:42.277746\n",
      "waited for 1 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185, 0.005613656293425282, 0.005552248891021362, 0.005659041803326029]\n",
      "[Epoch 11/50-0/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.220, 14.062%, 99.743%] time: 1:34:53.013403\n",
      "[Epoch 11/50-500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.359, 14.062%, 99.568%] time: 1:36:57.313175\n",
      "[Epoch 11/50-1000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.350, 14.062%, 99.577%] time: 1:39:02.047515\n",
      "[Epoch 11/50-1500/2018] [D loss&acc: 0.253, 49.609%] [G loss&accA&accB: 0.319, 14.062%, 99.607%] time: 1:41:06.785650\n",
      "[Epoch 11/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.256, 14.062%, 99.685%] time: 1:43:12.003392\n",
      "[Epoch 11/50-2016/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.449, 14.062%, 99.474%] [Test loss&acc: 0.563, 0.000%, 99.333%] time: 1:43:16.026406\n",
      "waited for 2 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185, 0.005613656293425282, 0.005552248891021362, 0.005659041803326029, 0.005632790658313491]\n",
      "[Epoch 12/50-0/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.439, 14.062%, 99.494%] time: 1:43:26.811103\n",
      "[Epoch 12/50-500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.427, 14.062%, 99.482%] time: 1:45:31.559616\n",
      "[Epoch 12/50-1000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.466, 14.062%, 99.434%] time: 1:47:36.544651\n",
      "[Epoch 12/50-1500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.412, 14.062%, 99.493%] time: 1:49:41.353223\n",
      "[Epoch 12/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.460, 14.062%, 99.390%] time: 1:51:46.005625\n",
      "[Epoch 12/50-2016/2018] [D loss&acc: 0.253, 49.854%] [G loss&accA&accB: 0.235, 14.062%, 99.726%] [Test loss&acc: 0.563, 0.000%, 99.326%] time: 1:51:49.958409\n",
      "waited for 3 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185, 0.005613656293425282, 0.005552248891021362, 0.005659041803326029, 0.005632790658313491, 0.005630991536140408]\n",
      "[Epoch 13/50-0/2018] [D loss&acc: 0.253, 49.951%] [G loss&accA&accB: 0.317, 14.062%, 99.607%] time: 1:52:00.737610\n",
      "[Epoch 13/50-500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.508, 14.062%, 99.376%] time: 1:54:05.050433\n",
      "[Epoch 13/50-1000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.268, 14.062%, 99.680%] time: 1:56:09.722390\n",
      "[Epoch 13/50-1500/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.232, 14.062%, 99.723%] time: 1:58:14.411355\n",
      "[Epoch 13/50-2000/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.318, 14.062%, 99.598%] time: 2:00:18.943014\n",
      "[Epoch 13/50-2016/2018] [D loss&acc: 0.253, 50.000%] [G loss&accA&accB: 0.234, 14.062%, 99.721%] [Test loss&acc: 0.564, 0.000%, 99.328%] time: 2:00:22.933146\n",
      "waited for 4 [0.011817967645607937, 0.0066871020583587, 0.006019271900973423, 0.00594416872154509, 0.005834723388657269, 0.005685853846511379, 0.0056392900225897755, 0.0056229218284194185, 0.005613656293425282, 0.005552248891021362, 0.005659041803326029, 0.005632790658313491, 0.005630991536140408, 0.005639041204198651]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN4MapGen()\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs=50, batch_size=16, sample_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
