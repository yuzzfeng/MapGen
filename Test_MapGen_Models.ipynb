{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate, Add, Lambda\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop, Adadelta\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sys\n",
    "import os\n",
    "\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "smooth = 1.\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1 - dice_coef(y_true, y_pred)\n",
    "\n",
    "def iou_coef(y_true, y_pred):\n",
    "    dice = dice_coef(y_true, y_pred)\n",
    "    iou = dice / (2 - dice)\n",
    "    return iou\n",
    "\n",
    "def iou_loss(y_true, y_pred):\n",
    "    return 1 - iou_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mapgenlib.losses_extend import dice_coef_loss, iou_loss, binary_focal_loss\n",
    "from mapgenlib.res_unit import res_block, decoder, encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN4MapGen(): # Based on u-net, residual u-net and pix2pix\n",
    "    # Reference: https://github.com/eriklindernoren/Keras-GAN/blob/master/pix2pix/pix2pix.py\n",
    "    \n",
    "    def __init__(self, img_rows, img_cols):\n",
    "\n",
    "        # Input shape\n",
    "        self.img_rows = img_rows\n",
    "        self.img_cols = img_cols\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        \n",
    "        self.clip_value = 0.01\n",
    "        \n",
    "        # Configure data loader\n",
    "        self.dataset_name = 'mapgen'\n",
    "\n",
    "        # Calculate output shape of D (PatchGAN)\n",
    "        patch = int(self.img_rows / 2**4)\n",
    "        self.disc_patch = (patch, patch, 1)\n",
    "        \n",
    "        # Calculate output shape of D (PatchGAN) better version\n",
    "        self.patch_size = 32\n",
    "        self.nb_patches = int((self.img_rows / self.patch_size) * (self.img_cols / self.patch_size))\n",
    "        self.patch_gan_dim = (self.patch_size, self.patch_size, self.channels)\n",
    "        \n",
    "        # Number of filters in the first layer of G and D\n",
    "        self.gf = 64\n",
    "        self.df = 64\n",
    "\n",
    "        #optimizer = Adam(0.0002, 0.5) # Original\n",
    "        #optimizer = Adam(0.0001, 0.5) # Original # Latest achieved by 0.00008\n",
    "        #optimizer = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08) # An old version of Pix2pix\n",
    "        optimizer = Adam(0.0004)\n",
    "        \n",
    "        #-------------------------\n",
    "        # Construct Computational\n",
    "        #   Graph of Generator\n",
    "        #-------------------------\n",
    "\n",
    "        # Build the generator\n",
    "        #self.generator = self.build_generator() # Old generator from \n",
    "        self.generator = self.build_res_unet_generator()\n",
    "\n",
    "        # Input images and their conditioning images\n",
    "        #img_A = Input(shape=self.img_shape) # Target\n",
    "        img_B = Input(shape=self.img_shape) # Input\n",
    "\n",
    "        # By conditioning on B generate a fake version of A\n",
    "        fake_A = self.generator(img_B)\n",
    "        \n",
    "        # Build and compile the discriminator\n",
    "        #self.discriminator = self.build_discriminator() # 1.Version\n",
    "        self.discriminator = self.build_discriminator_patchgan()\n",
    "        \n",
    "        self.discriminator.compile(loss='mse', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        \n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # Discriminators determines validity of translated images / condition pairs\n",
    "        #valid = self.discriminator([fake_A, img_B])\n",
    "        #self.combined = Model(inputs=[img_A, img_B], outputs=[valid, fake_A])\n",
    "        \n",
    "        valid = self.discriminator([fake_A])\n",
    "        self.combined = Model(inputs= img_B, outputs=[valid, fake_A])\n",
    "        \n",
    "        # Original Pix2Pix - low weight for discriminator\n",
    "        self.combined.compile(loss=['mse', 'mae'], #['mse', 'mae'] original\n",
    "                              loss_weights=[1, 10], # [1, 100] original\n",
    "                              optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    \n",
    "    def build_res_unet_generator(self):\n",
    "        \"\"\"Residual U-Net Generator\"\"\"\n",
    "        \n",
    "        inputs = Input(shape=self.img_shape)\n",
    "        to_decoder = encoder(inputs)\n",
    "        path = res_block(to_decoder[2], [256, 256], [(2, 2), (1, 1)], increase = True) # 3x\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Number of block of bottleneck = 1\n",
    "        path = res_block(path, [256, 256], [(1, 1), (1, 1)]) # Try to add one 2019.01.14, achieved best result ever\n",
    "        path = decoder(path, from_encoder=to_decoder)\n",
    "        path = Conv2D(filters=1, kernel_size=(1, 1), activation='sigmoid')(path) \n",
    "\n",
    "        return Model(input=inputs, output=path)\n",
    "        \n",
    "    def build_generator(self):\n",
    "        \"\"\"U-Net Generator\"\"\"\n",
    "\n",
    "        def conv2d(layer_input, filters, f_size=4, bn=True):\n",
    "            \"\"\"Layers used during downsampling\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n",
    "            \"\"\"Layers used during upsampling\"\"\"\n",
    "            u = UpSampling2D(size=2)(layer_input)\n",
    "            u = Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n",
    "            if dropout_rate:\n",
    "                u = Dropout(dropout_rate)(u)\n",
    "            u = BatchNormalization(momentum=0.8)(u)\n",
    "            u = Concatenate()([u, skip_input])\n",
    "            return u\n",
    "\n",
    "        # Image input\n",
    "        d0 = Input(shape=self.img_shape)\n",
    "\n",
    "        # Downsampling\n",
    "        d1 = conv2d(d0, self.gf, bn=False)\n",
    "        d2 = conv2d(d1, self.gf*2)\n",
    "        d3 = conv2d(d2, self.gf*4)\n",
    "        d4 = conv2d(d3, self.gf*8)\n",
    "        d5 = conv2d(d4, self.gf*8)\n",
    "        d6 = conv2d(d5, self.gf*8)\n",
    "        d7 = conv2d(d6, self.gf*8)\n",
    "\n",
    "        # Upsampling\n",
    "        u1 = deconv2d(d7, d6, self.gf*8)\n",
    "        u2 = deconv2d(u1, d5, self.gf*8)\n",
    "        u3 = deconv2d(u2, d4, self.gf*8)\n",
    "        u4 = deconv2d(u3, d3, self.gf*4)\n",
    "        u5 = deconv2d(u4, d2, self.gf*2)\n",
    "        u6 = deconv2d(u5, d1, self.gf)\n",
    "\n",
    "        u7 = UpSampling2D(size=2)(u6)\n",
    "        output_img = Conv2D(self.channels, kernel_size=4, strides=1, padding='same', activation='tanh')(u7)\n",
    "\n",
    "        return Model(d0, output_img)\n",
    "    \n",
    "    def build_discriminator_simple(self):\n",
    "\n",
    "        def d_block(layer_input, filters, strides=1, bn=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=3, strides=strides, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        # Input img\n",
    "        d0 = Input(shape=self.hr_shape)\n",
    "\n",
    "        d1 = d_block(d0, self.df, bn=False)\n",
    "        d2 = d_block(d1, self.df, strides=2)\n",
    "        d3 = d_block(d2, self.df*2)\n",
    "        d4 = d_block(d3, self.df*2, strides=2)\n",
    "        d5 = d_block(d4, self.df*4)\n",
    "        d6 = d_block(d5, self.df*4, strides=2)\n",
    "        d7 = d_block(d6, self.df*8)\n",
    "        d8 = d_block(d7, self.df*8, strides=2)\n",
    "\n",
    "        d9 = Dense(self.df*16)(d8)\n",
    "        d10 = LeakyReLU(alpha=0.2)(d9)\n",
    "        validity = Dense(1, activation='sigmoid')(d10)\n",
    "\n",
    "        return Model(d0, validity)\n",
    "    \n",
    "    def build_discriminator_patchgan(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=4, normalization=True):\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            d = LeakyReLU(alpha=0.2)(d)\n",
    "            if normalization:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            return d\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "\n",
    "        d1 = d_layer(img, self.df, normalization=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model(img, validity)\n",
    "        \n",
    "    def build_discriminator(self):\n",
    "\n",
    "        def d_layer(layer_input, filters, f_size=3, bn=True): # Chnaged here for the order of bn and activation\n",
    "            \"\"\"Discriminator layer\"\"\"\n",
    "            d = Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n",
    "            if bn:\n",
    "                d = BatchNormalization(momentum=0.8)(d)\n",
    "            d = Activation(activation='relu')(d)\n",
    "            #d = LeakyReLU(alpha=0.2)(d)\n",
    "            return d\n",
    "\n",
    "        img_A = Input(shape=self.img_shape)\n",
    "        \n",
    "        # !!!! This step was deleted because the condition do not help in this case\n",
    "        #img_B = Input(shape=self.img_shape)\n",
    "        ## Concatenate image and conditioning image by channels to produce input\n",
    "        #combined_imgs = Concatenate(axis=-1)([img_A, img_B])\n",
    "        #d1 = d_layer(combined_imgs, self.df, bn=False)\n",
    "        \n",
    "        d1 = d_layer(img_A, self.df, bn=False)\n",
    "        d2 = d_layer(d1, self.df*2)\n",
    "        d3 = d_layer(d2, self.df*4)\n",
    "        d4 = d_layer(d3, self.df*8)\n",
    "\n",
    "        validity = Conv2D(1, kernel_size=3, strides=1, padding='same')(d4)\n",
    "\n",
    "        return Model([img_A], validity)\n",
    "    \n",
    "    def train_generator_only(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath):\n",
    "        \n",
    "        start_time = datetime.datetime.now()\n",
    "        \n",
    "        gen = self.build_res_unet_generator()\n",
    "        optimizer = Adam(0.0004) # Current best\n",
    "        #optimizer = Adam()\n",
    "        #optimizer = Adam(0.0001)\n",
    "        #optimizer = RMSprop()\n",
    "        #optimizer = Adadelta()\n",
    "        \n",
    "        #optimizer = Adam(0.0001, 0.5) \n",
    "        #optimizer = RMSprop(lr=0.0001) # Worse\n",
    "        #optimizer = Adadelta(0.0001) # default okay, 0.0001 worse\n",
    "        #optimizer = Adam(0.0004, 0.5)\n",
    "        \n",
    "        # Loss function finished\n",
    "        gen.compile(loss='mse', optimizer=optimizer, metrics=['accuracy']) # mse better than mae\n",
    "        #gen.compile(loss='mae', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #gen.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        #gen.compile(loss=dice_coef_loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "        #gen.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], optimizer=optimizer, metrics=['accuracy']) # mse better than mae\n",
    "\n",
    "        data_gen_args = dict(rotation_range=180.)\n",
    "        image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "        \n",
    "        seed = 2\n",
    "        BATCH_SIZE = 16 # Proper for 128x128\n",
    "        #BATCH_SIZE = 8  # Proper for 256x256 16 takes much longer time\n",
    "        result_generator = zip(image_datagen.flow(x_train_sim, batch_size=BATCH_SIZE, seed=seed), \n",
    "                               mask_datagen.flow(y_train_sim, batch_size=BATCH_SIZE, seed=seed))\n",
    "        \n",
    "        History1 = History()\n",
    "        hist1 = gen.fit_generator(result_generator,\n",
    "                                  epochs = 100,\n",
    "                                  steps_per_epoch=2000,\n",
    "                                  verbose=1,\n",
    "                                  shuffle=True,\n",
    "                                  callbacks=[History1, \n",
    "                                             EarlyStopping(patience=4), \n",
    "                                             ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                             ModelCheckpoint(outPath + \"weights.h5\", \n",
    "                                                             save_best_only = True, \n",
    "                                                             save_weights_only = False)],\n",
    "                                  validation_data=(x_test_sim, y_test_sim))\n",
    "        save_hist(History1, outPath)\n",
    "        \n",
    "    \n",
    "    def train(self, x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs, batch_size=1, sample_interval=50):\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        # Adversarial loss ground truths\n",
    "        valid = np.ones((batch_size,) + self.disc_patch)\n",
    "        fake = np.zeros((batch_size,) + self.disc_patch)\n",
    "        \n",
    "        total_samples = len(x_train_sim)\n",
    "        ids = np.arange(total_samples)\n",
    "        np.random.shuffle(ids)\n",
    "        n_batches = int(total_samples / batch_size)\n",
    "        \n",
    "        train_acc = []\n",
    "        train_loss = []\n",
    "        valid_acc = []\n",
    "        valid_loss = []\n",
    "        \n",
    "        patience = 5\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            \n",
    "            for batch_i, (imgs_A, imgs_B) in enumerate(load_batch(x_train_sim, y_train_sim, batch_size)):\n",
    "\n",
    "                # ---------------------\n",
    "                #  Train Discriminator\n",
    "                # ---------------------\n",
    "\n",
    "                # Condition on B and generate a translated version\n",
    "                fake_A = self.generator.predict(imgs_B)\n",
    "                \n",
    "                # Train the discriminators (original images = real / generated = Fake)\n",
    "                d_loss_real = self.discriminator.train_on_batch([imgs_A], valid)\n",
    "                d_loss_fake = self.discriminator.train_on_batch([fake_A], fake)\n",
    "                d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "                \n",
    "                ## Clip critic weights\n",
    "                #for l in self.discriminator.layers:\n",
    "                #    weights = l.get_weights()\n",
    "                #    weights = [np.clip(w, -self.clip_value, self.clip_value) for w in weights]\n",
    "                #    l.set_weights(weights)\n",
    "\n",
    "                # -----------------\n",
    "                #  Train Generator\n",
    "                # -----------------\n",
    "\n",
    "                # Train the generators\n",
    "                self.discriminator.trainable = False\n",
    "                g_loss = self.combined.train_on_batch(imgs_B, [valid, imgs_A])\n",
    "                self.discriminator.trainable = True\n",
    "                \n",
    "                if batch_i % sample_interval == 0:\n",
    "                    elapsed_time = datetime.datetime.now() - start_time\n",
    "                    print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                elapsed_time))    \n",
    "                \n",
    "            # Plot the progress\n",
    "            if epoch >= 0:\n",
    "                    \n",
    "                    elapsed_time = datetime.datetime.now() - start_time\n",
    "                    \n",
    "                    valid_test = np.ones((len(x_test_sim),) + self.disc_patch)\n",
    "                    t_loss = self.combined.evaluate(x_test_sim, [valid_test, y_test_sim], verbose=0)\n",
    "                    \n",
    "                    print (\"[Epoch %d/%d-%d/%d] [D loss&acc: %.3f, %.3f%%] [G loss&accA&accB: %.3f, %.3f%%, %.3f%%] [Test loss&acc: %.3f, %.3f%%, %.3f%%] time: %s\" % (epoch, epochs,\n",
    "                                                                                batch_i, n_batches,\n",
    "                                                                                d_loss[0], 100*d_loss[1],\n",
    "                                                                                100*g_loss[2], 100*g_loss[3], 100*g_loss[4],\n",
    "                                                                                100*t_loss[2], 100*t_loss[3], 100*t_loss[4],\n",
    "                                                                                elapsed_time))    \n",
    "                    \n",
    "                    train_loss.append(g_loss[2])\n",
    "                    train_acc.append(g_loss[4])\n",
    "                    valid_loss.append(t_loss[2])\n",
    "                    valid_acc.append(t_loss[4])\n",
    "\n",
    "                    waited = len(valid_loss) - 1 - np.argmin(valid_loss)\n",
    "                    print('waited for', waited, valid_loss)\n",
    "                    \n",
    "                    if waited == 0:\n",
    "                        self.generator.save(outPath + 'model_epoch'+ str(epoch) +'.h5')   \n",
    "                        \n",
    "                    if waited > patience:\n",
    "                        break\n",
    "            \n",
    "        return train_acc, train_loss, valid_acc, valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape of the trains (31760, 128, 128, 1)\n",
      "Input Shape of the tests (3528, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "scale = 25\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng_/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "#scale = 10\n",
    "#p_size_1 = 256 # Compared with 256, which larger may generate round corners\n",
    "#trainPath = r\"../tmp_data/data_feng_256/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the trains', x_train_sim.shape)\n",
    "print('Input Shape of the tests', x_test_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_batch(x_train_sim, y_train_sim, batch_size):\n",
    "    total_samples = len(x_train_sim)\n",
    "    ids = np.arange(total_samples)\n",
    "    np.random.shuffle(ids)\n",
    "    n_batches = int(total_samples / batch_size)\n",
    "    for i in range(n_batches-1):\n",
    "        batch_idx = ids[i*batch_size:(i+1)*batch_size]\n",
    "        imgs_A = x_train_sim[batch_idx]\n",
    "        imgs_B = y_train_sim[batch_idx]\n",
    "        yield imgs_B, imgs_A     \n",
    "        \n",
    "def load_data(x_test_sim, y_test_sim, batch_size=1):\n",
    "    return x_test_sim  \n",
    "\n",
    "def save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath):    \n",
    "    ### Save history\n",
    "    History1_loss = train_loss\n",
    "    History1_acc = train_acc\n",
    "    History1_val_loss = valid_loss\n",
    "    History1_val_acc = valid_acc\n",
    "\n",
    "    thefile1 = open(outPath + 'History1_loss.txt', 'w')\n",
    "    for item in History1_loss:\n",
    "        thefile1.write(\"%s\\n\" % item)\n",
    "    thefile1.close()\n",
    "\n",
    "    thefile2 = open(outPath + 'History1_acc.txt', 'w')\n",
    "    for item in History1_acc:\n",
    "        thefile2.write(\"%s\\n\" % item)\n",
    "    thefile2.close()\n",
    "\n",
    "    thefile3 = open(outPath + 'History1_val_loss.txt', 'w')\n",
    "    for item in History1_val_loss:\n",
    "        thefile3.write(\"%s\\n\" % item)\n",
    "    thefile3.close()\n",
    "\n",
    "    thefile4 = open(outPath + 'History1_val_acc.txt', 'w')\n",
    "    for item in History1_val_acc:\n",
    "        thefile4.write(\"%s\\n\" % item)\n",
    "    thefile4.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residual unet"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "timestr = strftime(\"U\"+str(p_size_1)+\"_%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)\n",
    "print(outPath)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gan = GAN4MapGen(p_size_1, p_size_1)\n",
    "gan.train_generator_only(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train residual unet + PatchGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../tmp_results/predictions/U128GAN_2019-03-19 18-36-56_25/\n"
     ]
    }
   ],
   "source": [
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "timestr = strftime(\"U\"+str(p_size_1)+\"GAN_%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)\n",
    "print(outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:85: UserWarning: Update your `Model` call to the Keras 2 API: `Model(outputs=Tensor(\"co..., inputs=Tensor(\"in...)`\n",
      "/usr/local/lib/python3.5/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/50-0/1985] [D loss&acc: 23.543, 9.912%] [G loss&accA&accB: 32.820, 20.508%, 83.898%] time: 0:00:11.456283\n",
      "[Epoch 0/50-500/1985] [D loss&acc: 0.256, 44.775%] [G loss&accA&accB: 1.222, 70.117%, 98.798%] time: 0:01:54.798021\n",
      "[Epoch 0/50-1000/1985] [D loss&acc: 0.256, 50.098%] [G loss&accA&accB: 2.541, 50.293%, 97.459%] time: 0:03:36.615362\n",
      "[Epoch 0/50-1500/1985] [D loss&acc: 0.262, 46.240%] [G loss&accA&accB: 1.946, 45.703%, 98.056%] time: 0:05:18.370638\n",
      "[Epoch 0/50-1983/1985] [D loss&acc: 0.263, 62.158%] [G loss&accA&accB: 1.803, 51.953%, 98.198%] [Test loss&acc: 2.015, 16.400%, 97.986%] time: 0:06:56.467593\n",
      "waited for 0 [0.020149580272687536]\n",
      "[Epoch 1/50-0/1985] [D loss&acc: 0.271, 42.773%] [G loss&accA&accB: 1.283, 75.098%, 98.721%] time: 0:07:11.575685\n",
      "[Epoch 1/50-500/1985] [D loss&acc: 0.256, 60.986%] [G loss&accA&accB: 2.539, 41.504%, 97.462%] time: 0:08:53.095873\n",
      "[Epoch 1/50-1000/1985] [D loss&acc: 0.273, 60.254%] [G loss&accA&accB: 0.817, 62.012%, 99.187%] time: 0:10:34.785172\n",
      "[Epoch 1/50-1500/1985] [D loss&acc: 0.261, 40.381%] [G loss&accA&accB: 2.159, 64.160%, 97.842%] time: 0:12:16.293357\n",
      "[Epoch 1/50-1983/1985] [D loss&acc: 0.259, 47.705%] [G loss&accA&accB: 2.350, 49.023%, 97.649%] [Test loss&acc: 1.989, 15.151%, 98.012%] time: 0:13:54.416098\n",
      "waited for 0 [0.020149580272687536, 0.0198896748864867]\n",
      "[Epoch 2/50-0/1985] [D loss&acc: 0.258, 51.465%] [G loss&accA&accB: 2.203, 38.086%, 97.799%] time: 0:14:05.055699\n",
      "[Epoch 2/50-500/1985] [D loss&acc: 0.270, 42.139%] [G loss&accA&accB: 1.343, 60.254%, 98.658%] time: 0:15:46.476314\n",
      "[Epoch 2/50-1000/1985] [D loss&acc: 0.254, 53.564%] [G loss&accA&accB: 1.954, 44.922%, 98.051%] time: 0:17:27.857270\n",
      "[Epoch 2/50-1500/1985] [D loss&acc: 0.256, 50.830%] [G loss&accA&accB: 1.575, 53.809%, 98.427%] time: 0:19:09.106981\n",
      "[Epoch 2/50-1983/1985] [D loss&acc: 0.256, 51.807%] [G loss&accA&accB: 2.267, 38.672%, 97.735%] [Test loss&acc: 1.932, 16.632%, 98.069%] time: 0:20:46.618815\n",
      "waited for 0 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756]\n",
      "[Epoch 3/50-0/1985] [D loss&acc: 0.252, 49.707%] [G loss&accA&accB: 2.070, 30.176%, 97.931%] time: 0:20:57.269950\n",
      "[Epoch 3/50-500/1985] [D loss&acc: 0.260, 39.209%] [G loss&accA&accB: 1.649, 58.691%, 98.356%] time: 0:22:38.432398\n",
      "[Epoch 3/50-1000/1985] [D loss&acc: 0.261, 42.920%] [G loss&accA&accB: 1.340, 57.617%, 98.663%] time: 0:24:19.560218\n",
      "[Epoch 3/50-1500/1985] [D loss&acc: 0.253, 56.592%] [G loss&accA&accB: 1.512, 31.543%, 98.492%] time: 0:26:00.633677\n",
      "[Epoch 3/50-1983/1985] [D loss&acc: 0.258, 51.172%] [G loss&accA&accB: 1.490, 46.094%, 98.512%] [Test loss&acc: 1.780, 16.971%, 98.221%] time: 0:27:38.033704\n",
      "waited for 0 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894]\n",
      "[Epoch 4/50-0/1985] [D loss&acc: 0.260, 40.771%] [G loss&accA&accB: 1.331, 62.598%, 98.668%] time: 0:27:48.637733\n",
      "[Epoch 4/50-500/1985] [D loss&acc: 0.255, 54.443%] [G loss&accA&accB: 1.204, 36.328%, 98.799%] time: 0:29:29.355059\n",
      "[Epoch 4/50-1000/1985] [D loss&acc: 0.256, 45.703%] [G loss&accA&accB: 2.021, 54.590%, 97.982%] time: 0:31:09.862254\n",
      "[Epoch 4/50-1500/1985] [D loss&acc: 0.262, 36.914%] [G loss&accA&accB: 1.711, 62.793%, 98.291%] time: 0:32:50.228769\n",
      "[Epoch 4/50-1983/1985] [D loss&acc: 0.255, 40.234%] [G loss&accA&accB: 2.260, 51.270%, 97.743%] [Test loss&acc: 1.657, 5.505%, 98.345%] time: 0:34:27.050778\n",
      "waited for 0 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355]\n",
      "[Epoch 5/50-0/1985] [D loss&acc: 0.253, 49.316%] [G loss&accA&accB: 1.610, 33.203%, 98.391%] time: 0:34:37.698430\n",
      "[Epoch 5/50-500/1985] [D loss&acc: 0.252, 44.092%] [G loss&accA&accB: 1.102, 40.039%, 98.898%] time: 0:36:17.947834\n",
      "[Epoch 5/50-1000/1985] [D loss&acc: 0.254, 53.564%] [G loss&accA&accB: 1.647, 35.254%, 98.356%] time: 0:37:58.153451\n",
      "[Epoch 5/50-1500/1985] [D loss&acc: 0.244, 55.859%] [G loss&accA&accB: 2.423, 18.457%, 97.575%] time: 0:39:38.136931\n",
      "[Epoch 5/50-1983/1985] [D loss&acc: 0.200, 68.945%] [G loss&accA&accB: 1.660, 11.816%, 98.344%] [Test loss&acc: 2.209, 76.803%, 97.793%] time: 0:41:14.533019\n",
      "waited for 1 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355, 0.02209110863827794]\n",
      "[Epoch 6/50-0/1985] [D loss&acc: 0.169, 79.395%] [G loss&accA&accB: 1.232, 7.617%, 98.775%] time: 0:41:24.792873\n",
      "[Epoch 6/50-500/1985] [D loss&acc: 0.123, 85.498%] [G loss&accA&accB: 2.418, 0.684%, 97.594%] time: 0:43:04.618735\n",
      "[Epoch 6/50-1000/1985] [D loss&acc: 0.215, 56.348%] [G loss&accA&accB: 1.625, 38.086%, 98.385%] time: 0:44:44.550467\n",
      "[Epoch 6/50-1500/1985] [D loss&acc: 0.298, 51.709%] [G loss&accA&accB: 1.364, 84.473%, 98.642%] time: 0:46:24.724118\n",
      "[Epoch 6/50-1983/1985] [D loss&acc: 0.273, 51.025%] [G loss&accA&accB: 1.849, 53.125%, 98.325%] [Test loss&acc: 2.265, 25.318%, 97.750%] time: 0:48:01.382072\n",
      "waited for 2 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355, 0.02209110863827794, 0.022649606406823848]\n",
      "[Epoch 7/50-0/1985] [D loss&acc: 0.224, 55.957%] [G loss&accA&accB: 3.441, 62.207%, 96.603%] time: 0:48:11.674989\n",
      "[Epoch 7/50-500/1985] [D loss&acc: 0.151, 87.549%] [G loss&accA&accB: 1.861, 0.391%, 98.149%] time: 0:49:52.268065\n",
      "[Epoch 7/50-1000/1985] [D loss&acc: 0.151, 87.109%] [G loss&accA&accB: 1.683, 5.762%, 98.323%] time: 0:51:32.302411\n",
      "[Epoch 7/50-1500/1985] [D loss&acc: 0.238, 59.229%] [G loss&accA&accB: 2.943, 87.500%, 97.062%] time: 0:53:12.296511\n",
      "[Epoch 7/50-1983/1985] [D loss&acc: 0.210, 59.033%] [G loss&accA&accB: 1.979, 4.883%, 98.026%] [Test loss&acc: 2.476, 0.810%, 97.530%] time: 0:54:49.189767\n",
      "waited for 3 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355, 0.02209110863827794, 0.022649606406823848, 0.024755643406152186]\n",
      "[Epoch 8/50-0/1985] [D loss&acc: 0.188, 71.875%] [G loss&accA&accB: 1.688, 34.570%, 98.316%] time: 0:54:59.511463\n",
      "[Epoch 8/50-500/1985] [D loss&acc: 0.150, 72.119%] [G loss&accA&accB: 2.884, 0.000%, 97.120%] time: 0:56:39.743484\n",
      "[Epoch 8/50-1000/1985] [D loss&acc: 0.304, 50.439%] [G loss&accA&accB: 1.908, 2.344%, 98.092%] time: 0:58:19.682574\n",
      "[Epoch 8/50-1500/1985] [D loss&acc: 0.137, 83.057%] [G loss&accA&accB: 2.265, 7.812%, 97.737%] time: 0:59:59.650069\n",
      "[Epoch 8/50-1983/1985] [D loss&acc: 0.206, 59.863%] [G loss&accA&accB: 2.517, 1.953%, 97.489%] [Test loss&acc: 2.543, 17.092%, 97.459%] time: 1:01:36.122509\n",
      "waited for 4 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355, 0.02209110863827794, 0.022649606406823848, 0.024755643406152186, 0.025430874818779713]\n",
      "[Epoch 9/50-0/1985] [D loss&acc: 0.188, 61.426%] [G loss&accA&accB: 2.343, 2.637%, 97.660%] time: 1:01:46.389459\n",
      "[Epoch 9/50-500/1985] [D loss&acc: 0.060, 98.242%] [G loss&accA&accB: 2.526, 0.000%, 97.474%] time: 1:03:26.132429\n",
      "[Epoch 9/50-1000/1985] [D loss&acc: 0.065, 99.072%] [G loss&accA&accB: 5.171, 1.562%, 94.835%] time: 1:05:06.195604\n",
      "[Epoch 9/50-1500/1985] [D loss&acc: 0.023, 99.951%] [G loss&accA&accB: 5.261, 7.324%, 94.741%] time: 1:06:46.045684\n",
      "[Epoch 9/50-1983/1985] [D loss&acc: 0.151, 82.227%] [G loss&accA&accB: 2.915, 0.000%, 97.086%] [Test loss&acc: 3.327, 1.583%, 96.674%] time: 1:08:22.736829\n",
      "waited for 5 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355, 0.02209110863827794, 0.022649606406823848, 0.024755643406152186, 0.025430874818779713, 0.03327005626866066]\n",
      "[Epoch 10/50-0/1985] [D loss&acc: 0.227, 53.955%] [G loss&accA&accB: 3.554, 1.367%, 96.445%] time: 1:08:33.053175\n",
      "[Epoch 10/50-500/1985] [D loss&acc: 0.106, 89.111%] [G loss&accA&accB: 2.147, 12.305%, 97.855%] time: 1:10:13.010440\n",
      "[Epoch 10/50-1000/1985] [D loss&acc: 0.167, 82.129%] [G loss&accA&accB: 3.448, 5.176%, 96.552%] time: 1:11:52.753085\n",
      "[Epoch 10/50-1500/1985] [D loss&acc: 0.075, 97.363%] [G loss&accA&accB: 2.945, 0.391%, 97.055%] time: 1:13:32.419662\n",
      "[Epoch 10/50-1983/1985] [D loss&acc: 0.099, 91.064%] [G loss&accA&accB: 3.716, 0.586%, 96.286%] [Test loss&acc: 5.179, 0.060%, 94.822%] time: 1:15:09.343670\n",
      "waited for 6 [0.020149580272687536, 0.0198896748864867, 0.019319876606096756, 0.017797965425448894, 0.016573537373387355, 0.02209110863827794, 0.022649606406823848, 0.024755643406152186, 0.025430874818779713, 0.03327005626866066, 0.051790956635864416]\n"
     ]
    }
   ],
   "source": [
    "gan = GAN4MapGen(128,128)\n",
    "train_acc, train_loss, valid_acc, valid_loss = gan.train(x_train_sim, y_train_sim, x_test_sim, y_test_sim, outPath, epochs=50, batch_size=16, sample_interval=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hist_local(train_acc, train_loss, valid_acc, valid_loss, outPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
