{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "## Map Generalization for Polygons using Autoencode-like strucutures\n",
    "## \n",
    "## Author: Yu Feng, yuzz.feng@gmail.com\n",
    "## 1. Version Author: SERCAN CAKIR - Adatped based on Master Thesis of SERCAN CAKIR \"ROAD NETWORK EXTRACTION USING CNN\"\n",
    "\n",
    "## Changes:\n",
    "## 1. Two conv layers were added before the first down convlusional layer\n",
    "## 2. Output can be any size during the evaluation\n",
    "## 3. Adapt the code to support more images as training examples\n",
    "## 4. Dropouot may make the sharpe corners vanishing, we delete half of them, but we should used some\n",
    "## 5. Splilt step for generate training and validation data\n",
    "## 6. Prepare to update in github\n",
    "\n",
    "import matplotlib\n",
    "#matplotlib.use('Agg') # necessary for linux kernal\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "np.random.seed(7)\n",
    "import itertools\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "#from keras.layers.core import Dropout\n",
    "from keras.layers import Input, Conv2D, Dropout, UpSampling2D, Activation, Concatenate, Add\n",
    "from keras.layers import MaxPooling2D, Conv2DTranspose, BatchNormalization, Activation\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, History\n",
    "\n",
    "from osgeo import gdal\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.util.shape import view_as_windows\n",
    "\n",
    "from data_helper import readImg, readImgInv, imagePatches, removeBlackImg, removeCorrespondence, check_and_create\n",
    "\n",
    "from time import gmtime, strftime\n",
    "timestr = strftime(\"%Y-%m-%d %H-%M-%S\", gmtime())\n",
    "\n",
    "scale = 15 #25\n",
    "\n",
    "############ Path Setting ##############\n",
    "outPath = r\"../tmp_results/predictions/\"\n",
    "outPath = outPath + timestr + '_' + str(scale)+ \"/\"\n",
    "check_and_create(outPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order the image dimension acc. to TensorFlow (batc_hsize, rows, cols, channels)\n",
    "K.set_image_dim_ordering('tf')\n",
    "\n",
    "p_size_1 = 128 # Compared with 256, which larger may generate round corners\n",
    "trainPath = r\"../tmp_data/data_feng/geb\" + str(scale) +  \"/\"\n",
    "\n",
    "# save image patch arrays\n",
    "x_train_sim = np.load(trainPath + \"x_train_sim.npy\")\n",
    "y_train_sim = np.load(trainPath + \"y_train_sim.npy\")\n",
    "x_test_sim = np.load(trainPath + \"x_test_sim.npy\")\n",
    "y_test_sim = np.load(trainPath + \"y_test_sim.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(np.reshape(x_test_sim[2], (p_size_1,p_size_1)))\n",
    "plt.figure()\n",
    "plt.imshow(np.reshape(y_test_sim[2], (p_size_1,p_size_1)))\n",
    "\n",
    "input_shape1 = (None, None, 1) #x_train_sim[0].shape\n",
    "print('Input Shape of the models', x_train_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from pix2pix.utils.facades_generator import facades_generator\n",
    "from pix2pix.networks.generator import UNETGenerator\n",
    "from pix2pix.networks.discriminator import PatchGanDiscriminator\n",
    "from pix2pix.networks.DCGAN import DCGAN\n",
    "from pix2pix.utils import patch_utils\n",
    "from pix2pix.utils import logger\n",
    "import time\n",
    "\n",
    "from keras.utils import generic_utils as keras_generic_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:97: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=64, kernel_size=(4, 4))`\n",
      "  en_1 = Conv2D(nb_filter=64, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(input_layer)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:101: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=128, kernel_size=(4, 4))`\n",
      "  en_2 = Conv2D(nb_filter=128, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_1)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:106: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=256, kernel_size=(4, 4))`\n",
      "  en_3 = Conv2D(nb_filter=256, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_2)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:111: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=512, kernel_size=(4, 4))`\n",
      "  en_4 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_3)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:116: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=512, kernel_size=(4, 4))`\n",
      "  en_5 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_4)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:121: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=512, kernel_size=(4, 4))`\n",
      "  en_6 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_5)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:126: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=512, kernel_size=(4, 4))`\n",
      "  en_7 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_6)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:131: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(strides=(2, 2), padding=\"same\", filters=512, kernel_size=(4, 4))`\n",
      "  en_8 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same', subsample=(stride, stride))(en_7)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:143: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=512)`\n",
      "  de_1 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same')(de_1)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:151: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=1024)`\n",
      "  de_2 = Conv2D(nb_filter=1024, nb_row=4, nb_col=4, border_mode='same')(de_2)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:159: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=1024)`\n",
      "  de_3 = Conv2D(nb_filter=1024, nb_row=4, nb_col=4, border_mode='same')(de_3)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:167: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=1024)`\n",
      "  de_4 = Conv2D(nb_filter=1024, nb_row=4, nb_col=4, border_mode='same')(de_4)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:175: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=1024)`\n",
      "  de_5 = Conv2D(nb_filter=1024, nb_row=4, nb_col=4, border_mode='same')(de_5)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:183: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=512)`\n",
      "  de_6 = Conv2D(nb_filter=512, nb_row=4, nb_col=4, border_mode='same')(de_6)\n",
      "/notebooks/tmp/MapGen/pix2pix/networks/generator.py:191: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(padding=\"same\", kernel_size=(4, 4), filters=256)`\n",
      "  de_7 = Conv2D(nb_filter=256, nb_row=4, nb_col=4, border_mode='same')(de_7)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 192, 128, 256), (None, 1, 128, 64)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3a60921bc2eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Our generator is an AutoEncoder with U-NET skip connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m# ----------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mgenerator_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUNETGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_img_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_img_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_output_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mgenerator_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/notebooks/tmp/MapGen/pix2pix/networks/generator.py\u001b[0m in \u001b[0;36mUNETGenerator\u001b[0;34m(input_img_dim, num_output_channels)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0mde_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gen_de_bn_7'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mde_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0mde_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mde_7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m     \u001b[0mde_7\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde_7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    429\u001b[0m                                          \u001b[0;34m'You can build it manually via: '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m                                          '`layer.build(batch_input_shape)`')\n\u001b[0;32m--> 431\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/layers/merge.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    360\u001b[0m                              \u001b[0;34m'inputs with matching shapes '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                              \u001b[0;34m'except for the concat axis. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                              'Got inputs shapes: %s' % (input_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_merge_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concat axis. Got inputs shapes: [(None, 192, 128, 256), (None, 1, 128, 64)]"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# HYPER PARAMS\n",
    "# ---------------------------------------------\n",
    "# width, height of images to work with. Assumes images are square\n",
    "im_width = im_height = 256\n",
    "\n",
    "# inpu/oputputt channels in image\n",
    "input_channels = 1\n",
    "output_channels = 1\n",
    "\n",
    "# image dims\n",
    "input_img_dim = (input_channels, im_width, im_height)\n",
    "output_img_dim = (output_channels, im_width, im_height)\n",
    "\n",
    "# We're using PatchGAN setup, so we need the num of non-overlaping patches\n",
    "# this is how big we'll make the patches for the discriminator\n",
    "# for example. We can break up a 256x256 image in 16 patches of 64x64 each\n",
    "sub_patch_dim = (256, 256)\n",
    "nb_patch_patches, patch_gan_dim = patch_utils.num_patches(output_img_dim=output_img_dim, sub_patch_dim=sub_patch_dim)\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# TRAINING ROUTINE\n",
    "# ---------------------------------------------\n",
    "\n",
    "# ----------------------\n",
    "# GENERATOR\n",
    "# Our generator is an AutoEncoder with U-NET skip connections\n",
    "# ----------------------\n",
    "generator_nn = UNETGenerator(input_img_dim=input_img_dim, num_output_channels=output_channels)\n",
    "generator_nn.summary()\n",
    "\n",
    "# ----------------------\n",
    "# PATCH GAN DISCRIMINATOR\n",
    "# the patch gan averages loss across sub patches of the image\n",
    "# it's fancier than the standard gan but produces sharper results\n",
    "# ----------------------\n",
    "discriminator_nn = PatchGanDiscriminator(output_img_dim=output_img_dim,\n",
    "        patch_dim=patch_gan_dim, nb_patches=nb_patch_patches)\n",
    "discriminator_nn.summary()\n",
    "\n",
    "# disable training while we put it through the GAN\n",
    "discriminator_nn.trainable = False\n",
    "\n",
    "# ------------------------\n",
    "# Define Optimizers\n",
    "opt_discriminator = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "opt_dcgan = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# -------------------------\n",
    "# compile generator\n",
    "generator_nn.compile(loss='mae', optimizer=opt_discriminator)\n",
    "\n",
    "# ----------------------\n",
    "# MAKE FULL DCGAN\n",
    "# ----------------------\n",
    "dc_gan_nn = DCGAN(generator_model=generator_nn,\n",
    "                  discriminator_model=discriminator_nn,\n",
    "                  input_img_dim=input_img_dim,\n",
    "                  patch_dim=sub_patch_dim)\n",
    "\n",
    "dc_gan_nn.summary()\n",
    "\n",
    "# ---------------------\n",
    "# Compile DCGAN\n",
    "# we use a combination of mae and bin_crossentropy\n",
    "loss = ['mae', 'binary_crossentropy']\n",
    "loss_weights = [1E2, 1]\n",
    "dc_gan_nn.compile(loss=loss, loss_weights=loss_weights, optimizer=opt_dcgan)\n",
    "\n",
    "# ---------------------\n",
    "# ENABLE DISCRIMINATOR AND COMPILE\n",
    "discriminator_nn.trainable = True\n",
    "discriminator_nn.compile(loss='binary_crossentropy', optimizer=opt_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------\n",
    "# RUN ACTUAL TRAINING\n",
    "batch_size = 1\n",
    "data_path = WORKING_DIR + '/data/' + DATASET\n",
    "nb_epoch = 100\n",
    "n_images_per_epoch = 400\n",
    "\n",
    "print('Training starting...')\n",
    "for epoch in range(0, nb_epoch):\n",
    "\n",
    "    print('Epoch {}'.format(epoch))\n",
    "    batch_counter = 1\n",
    "    start = time.time()\n",
    "    progbar = keras_generic_utils.Progbar(n_images_per_epoch)\n",
    "\n",
    "    # init the datasources again for each epoch\n",
    "    tng_gen = facades_generator(data_dir_name=data_path, data_type='training', im_width=im_width, batch_size=batch_size)\n",
    "    val_gen = facades_generator(data_dir_name=data_path, data_type='validation', im_width=im_width, batch_size=batch_size)\n",
    "\n",
    "    # go through 1... n_images_per_epoch (which will go through all buckets as well\n",
    "    for mini_batch_i in range(0, n_images_per_epoch, batch_size):\n",
    "\n",
    "        # load a batch of decoded and original images\n",
    "        # both for training and validation\n",
    "        X_train_decoded_imgs, X_train_original_imgs = next(tng_gen)\n",
    "        X_val_decoded_imgs, X_val_original_imgs = next(val_gen)\n",
    "\n",
    "        # generate a batch of data and feed to the discriminator\n",
    "        # some images that come out of here are real and some are fake\n",
    "        # X is image patches for each image in the batch\n",
    "        # Y is a 1x2 vector for each image. (means fake or not)\n",
    "        X_discriminator, y_discriminator = patch_utils.get_disc_batch(X_train_original_imgs,\n",
    "                                                          X_train_decoded_imgs,\n",
    "                                                          generator_nn,\n",
    "                                                          batch_counter,\n",
    "                                                          patch_dim=sub_patch_dim)\n",
    "\n",
    "        # Update the discriminator\n",
    "        # print('calculating discriminator loss')\n",
    "        disc_loss = discriminator_nn.train_on_batch(X_discriminator, y_discriminator)\n",
    "\n",
    "        # create a batch to feed the generator\n",
    "        X_gen_target, X_gen = next(patch_utils.gen_batch(X_train_original_imgs, X_train_decoded_imgs, batch_size))\n",
    "        y_gen = np.zeros((X_gen.shape[0], 2), dtype=np.uint8)\n",
    "        y_gen[:, 1] = 1\n",
    "\n",
    "        # Freeze the discriminator\n",
    "        discriminator_nn.trainable = False\n",
    "\n",
    "        # trainining GAN\n",
    "        # print('calculating GAN loss...')\n",
    "        gen_loss = dc_gan_nn.train_on_batch(X_gen, [X_gen_target, y_gen])\n",
    "\n",
    "        # Unfreeze the discriminator\n",
    "        discriminator_nn.trainable = True\n",
    "\n",
    "        # counts batches we've ran through for generating fake vs real images\n",
    "        batch_counter += 1\n",
    "\n",
    "        # print losses\n",
    "        D_log_loss = disc_loss\n",
    "        gen_total_loss = gen_loss[0].tolist()\n",
    "        gen_total_loss = min(gen_total_loss, 1000000)\n",
    "        gen_mae = gen_loss[1].tolist()\n",
    "        gen_mae = min(gen_mae, 1000000)\n",
    "        gen_log_loss = gen_loss[2].tolist()\n",
    "        gen_log_loss = min(gen_log_loss, 1000000)\n",
    "\n",
    "        progbar.add(batch_size, values=[(\"Dis logloss\", D_log_loss),\n",
    "                                        (\"Gen total\", gen_total_loss),\n",
    "                                        (\"Gen L1 (mae)\", gen_mae),\n",
    "                                        (\"Gen logloss\", gen_log_loss)])\n",
    "\n",
    "        # ---------------------------\n",
    "        # Save images for visualization every 2nd batch\n",
    "        if batch_counter % 2 == 0:\n",
    "\n",
    "            # print images for training data progress\n",
    "            logger.plot_generated_batch(X_train_original_imgs, X_train_decoded_imgs, generator_nn, epoch, 'tng', mini_batch_i)\n",
    "\n",
    "            # print images for validation data\n",
    "            X_full_val_batch, X_sketch_val_batch = next(patch_utils.gen_batch(X_val_original_imgs, X_val_decoded_imgs, batch_size))\n",
    "            logger.plot_generated_batch(X_full_val_batch, X_sketch_val_batch, generator_nn, epoch, 'val', mini_batch_i)\n",
    "\n",
    "    # -----------------------\n",
    "    # log epoch\n",
    "    print(\"\")\n",
    "    print('Epoch %s/%s, Time: %s' % (epoch + 1, nb_epoch, time.time() - start))\n",
    "\n",
    "    # ------------------------------\n",
    "    # save weights on every 2nd epoch\n",
    "    if epoch % 2 == 0:\n",
    "        gen_weights_path = os.path.join('./pix2pix_out/weights/gen_weights_epoch_%s.h5' % (epoch))\n",
    "        generator_nn.save_weights(gen_weights_path, overwrite=True)\n",
    "\n",
    "        disc_weights_path = os.path.join('./pix2pix_out/weights/disc_weights_epoch_%s.h5' % (epoch))\n",
    "        discriminator_nn.save_weights(disc_weights_path, overwrite=True)\n",
    "\n",
    "        DCGAN_weights_path = os.path.join('./pix2pix_out/weights/DCGAN_weights_epoch_%s.h5' % (epoch))\n",
    "        dc_gan_nn.save_weights(DCGAN_weights_path, overwrite=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "from keras.models import Model\n",
    "\n",
    "from keras import models\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "tmpPath = \"../tmp_results/predictions/2018-11-30 11-10-35_15_residual_unet/\"\n",
    "generator_model = models.load_model(tmpPath + \"weights.hdf5\")\n",
    "                \n",
    "def DCGAN(generator_model, discriminator_model, input_img_dim, patch_dim):\n",
    "    \"\"\"\n",
    "    Here we do the following:\n",
    "    1. Generate an image with the generator\n",
    "    2. break up the generated image into patches\n",
    "    3. feed the patches to a discriminator to get the avg loss across all patches\n",
    "        (i.e is it fake or not)\n",
    "    4. the DCGAN outputs the generated image and the loss\n",
    "    This differs from standard GAN training in that we use patches of the image\n",
    "    instead of the full image (although a patch size = img_size is basically the whole image)\n",
    "    :param generator_model:\n",
    "    :param discriminator_model:\n",
    "    :param img_dim:\n",
    "    :param patch_dim:\n",
    "    :return: DCGAN model\n",
    "    \"\"\"\n",
    "\n",
    "    generator_input = Input(shape=input_img_dim, name=\"DCGAN_input\")\n",
    "\n",
    "    # generated image model from the generator\n",
    "    generated_image = generator_model(generator_input)\n",
    "\n",
    "    h, w = input_img_dim[1:]\n",
    "    ph, pw = patch_dim\n",
    "\n",
    "    # chop the generated image into patches\n",
    "    list_row_idx = [(i * ph, (i + 1) * ph) for i in range(int(h / ph))]\n",
    "    list_col_idx = [(i * pw, (i + 1) * pw) for i in range(int(w / pw))]\n",
    "\n",
    "    list_gen_patch = []\n",
    "    for row_idx in list_row_idx:\n",
    "        for col_idx in list_col_idx:\n",
    "            x_patch = Lambda(lambda z: z[:, :, row_idx[0]:row_idx[1],\n",
    "                col_idx[0]:col_idx[1]], output_shape=input_img_dim)(generated_image)\n",
    "            list_gen_patch.append(x_patch)\n",
    "\n",
    "    # measure loss from patches of the image (not the actual image)\n",
    "    dcgan_output = discriminator_model(list_gen_patch)\n",
    "\n",
    "    # actually turn into keras model\n",
    "    dc_gan = Model(input=[generator_input], output=[generated_image, dcgan_output], name=\"DCGAN\")\n",
    "    return dc_gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import patch_utils\n",
    "\n",
    "# ---------------------------------------------\n",
    "# HYPER PARAMS\n",
    "# ---------------------------------------------\n",
    "# width, height of images to work with. Assumes images are square\n",
    "im_width = im_height = 256\n",
    "\n",
    "# inpu/oputputt channels in image\n",
    "input_channels = 1\n",
    "output_channels = 1\n",
    "\n",
    "# image dims\n",
    "input_img_dim = (input_channels, im_width, im_height)\n",
    "output_img_dim = (output_channels, im_width, im_height)\n",
    "\n",
    "# We're using PatchGAN setup, so we need the num of non-overlaping patches\n",
    "# this is how big we'll make the patches for the discriminator\n",
    "# for example. We can break up a 256x256 image in 16 patches of 64x64 each\n",
    "sub_patch_dim = (256, 256)\n",
    "nb_patch_patches, patch_gan_dim = patch_utils.num_patches(output_img_dim=output_img_dim, sub_patch_dim=sub_patch_dim)\n",
    "\n",
    "# ---------------------------------------------\n",
    "# TRAINING ROUTINE\n",
    "# ---------------------------------------------\n",
    "\n",
    "# ----------------------\n",
    "# GENERATOR\n",
    "# Our generator is an AutoEncoder with U-NET skip connections\n",
    "# ----------------------\n",
    "generator_nn = UNETGenerator(input_img_dim=input_img_dim, num_output_channels=output_channels)\n",
    "generator_nn.summary()\n",
    "\n",
    "# ----------------------\n",
    "# PATCH GAN DISCRIMINATOR\n",
    "# the patch gan averages loss across sub patches of the image\n",
    "# it's fancier than the standard gan but produces sharper results\n",
    "# ----------------------\n",
    "discriminator_nn = PatchGanDiscriminator(output_img_dim=output_img_dim,\n",
    "        patch_dim=patch_gan_dim, nb_patches=nb_patch_patches)\n",
    "discriminator_nn.summary()\n",
    "\n",
    "# disable training while we put it through the GAN\n",
    "discriminator_nn.trainable = False\n",
    "\n",
    "# ------------------------\n",
    "# Define Optimizers\n",
    "opt_discriminator = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "opt_dcgan = Adam(lr=1E-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "\n",
    "# -------------------------\n",
    "# compile generator\n",
    "generator_nn.compile(loss='mae', optimizer=opt_discriminator)\n",
    "\n",
    "# ----------------------\n",
    "# MAKE FULL DCGAN\n",
    "# ----------------------\n",
    "dc_gan_nn = DCGAN(generator_model=generator_nn,\n",
    "                  discriminator_model=discriminator_nn,\n",
    "                  input_img_dim=input_img_dim,\n",
    "                  patch_dim=sub_patch_dim)\n",
    "\n",
    "dc_gan_nn.summary()\n",
    "\n",
    "# ---------------------\n",
    "# Compile DCGAN\n",
    "# we use a combination of mae and bin_crossentropy\n",
    "loss = ['mae', 'binary_crossentropy']\n",
    "loss_weights = [1E2, 1]\n",
    "dc_gan_nn.compile(loss=loss, loss_weights=loss_weights, optimizer=opt_dcgan)\n",
    "\n",
    "# ---------------------\n",
    "# ENABLE DISCRIMINATOR AND COMPILE\n",
    "discriminator_nn.trainable = True\n",
    "discriminator_nn.compile(loss='binary_crossentropy', optimizer=opt_discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt1 = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-08, decay=0.0)\n",
    "\n",
    "from mymodels.unet_residual_small_stride4 import create_model_residual\n",
    "model_ex1 = create_model_residual(opt1, input_shape1) # Residual\n",
    "\n",
    "model_ex1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args = dict(rotation_range=180.)\n",
    "\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "BATCH_SIZE = 16\n",
    "result_generator = zip(image_datagen.flow(x_train_sim, batch_size=BATCH_SIZE, seed=seed), \n",
    "                       mask_datagen.flow(y_train_sim, batch_size=BATCH_SIZE, seed=seed))\n",
    "\n",
    "History1 = History()\n",
    "hist1 = model_ex1.fit_generator(  result_generator,\n",
    "                                  epochs = 100,\n",
    "                                  steps_per_epoch=2000,\n",
    "                                  verbose=1,\n",
    "                                  shuffle=True,\n",
    "                                  callbacks=[History1, \n",
    "                                             EarlyStopping(patience=5), \n",
    "                                             ReduceLROnPlateau(patience = 3, verbose = 0),\n",
    "                                             ModelCheckpoint(outPath + \"weights.hdf5\", \n",
    "                                                             save_best_only = True, \n",
    "                                                             save_weights_only = False)],\n",
    "                                  validation_data=(x_test_sim, y_test_sim))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from data_helper import predict_15k, save_hist, save_model\n",
    "\n",
    "### Save history\n",
    "save_hist(History1, outPath)\n",
    "# Retain best model\n",
    "#from keras import models\n",
    "#model_ex1 = models.load_model(outPath + \"weights.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent Test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keras.layers.Round = Round\n",
    "keras.regularizers.linf_reg = linf_reg\n",
    "keras.losses.linf_loss = linf_loss\n",
    "keras.losses.dice_coef_loss = dice_coef_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPath = r\"../tmp_data/Data/Testing/\"\n",
    "\n",
    "all_records = []\n",
    "\n",
    "records = predict_15k(outPath, testPath, outPath, \n",
    "                      r\"FTest1_input_inv.png\", r\"FTest1_output_inv.png\")\n",
    "all_records.extend(records)\n",
    "\n",
    "records = predict_15k(outPath, testPath, outPath, \n",
    "                      r\"FTest2_input_inv.png\", r\"FTest2_output_inv.png\")\n",
    "all_records.extend(records)\n",
    "\n",
    "df = pd.DataFrame(np.transpose(all_records))\n",
    "df.columns = [\"Input vs Target (Test1)\", \"Prediction vs Target (Test1)\", \n",
    "              \"Input vs Target (Test2)\", \"Prediction vs Target (Test2)\"]\n",
    "\n",
    "df = df.rename({0: \"Accuracy\", 1: 'IoU'})\n",
    "df.index.name = 'Metrics'\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
